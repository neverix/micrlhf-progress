{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\",\n",
    "                                         from_type=\"gemma\",\n",
    "                                         load_eager=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "import pandas as pd\n",
    "\n",
    "df_adv = pd.read_csv(\"data/adv.csv\")\n",
    "\n",
    "format_prompt = \"\"\"<start_of_turn>user\\n\n",
    "{}\\n\n",
    "<start_of_turn>model\\n\n",
    "{}\"\"\"\n",
    "# offset = 1\n",
    "# df_do = df.apply(lambda x: format_response.format(x['goal'], x['target']), axis=1)\n",
    "# prompts_harmful = df.apply(lambda x: format_prompt.format(x['goal'], \"\")[:-offset], axis=1).to_list()[:100]\n",
    "prompts_harmful = df_adv.apply(lambda x: format_prompt.format(x['goal'], \"\"), axis=1).to_list()[:100]\n",
    "dataset_jail = pd.read_csv(\"data/jail.csv\").apply(lambda x: x[\"Goal\"], axis=1).to_list()\n",
    "prompts_jail = [format_prompt.format(x, \"\") for x in dataset_jail]\n",
    "import datasets\n",
    "# https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw\n",
    "hf_path = 'tatsu-lab/alpaca'\n",
    "dataset = datasets.load_dataset(hf_path)\n",
    "# filter for instructions that do not have inputs\n",
    "prompts_harmless = []\n",
    "for i in range(len(dataset['train'])):\n",
    "    if len(prompts_harmless) >= len(prompts_harmful):\n",
    "        break\n",
    "    if dataset['train'][i]['input'].strip() == '':\n",
    "        # prompts_harmless.append(format_prompt.format(dataset['train'][i]['instruction'], \"\")[:-offset])\n",
    "        prompts_harmless.append(format_prompt.format(dataset['train'][i]['instruction'], \"\"))\n",
    "\n",
    "# ds = datasets.load_dataset(\"MBZUAI/LaMini-instruction\", split=\"train\", streaming=True)\n",
    "# prompts_harmless = []\n",
    "# for _, text in zip(range(100), ds):\n",
    "#     prompts_harmless.append(format_prompt.format(text[\"instruction\"], \"\"))\n",
    "\n",
    "# ds = datasets.load_dataset(\"nev/openhermes-2.5-phi-format-text\", split=\"train\", streaming=True)\n",
    "# prompts_harmless = []\n",
    "# for _, text in zip(range(100), ds):\n",
    "#     text = text[\"text\"]\n",
    "#     text = \"\".join(text.partition(\"<|assistant|>\\n\")[:2])\n",
    "#     prompts_harmless.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca042685da549cea13440c91973faae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from micrlhf.sampling import sample\n",
    "msl = 128\n",
    "completions_harmless = sample(llama, tokenizer, prompts_harmless, max_seq_len=msl, do_sample=True, verbose=True, return_only_completion=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_harmless = [(prompt, completion.partition(\"<eos>\")[0].strip()) for prompt, completion in zip(prompts_harmless, completions_harmless)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "import jax\n",
    "\n",
    "tokens = tokenizer.batch_encode_plus(prompts_harmful + prompts_harmless,\n",
    "                                     return_tensors=\"np\",\n",
    "                                     padding=\"max_length\",\n",
    "                                     truncation=True,\n",
    "                                     max_length=128,\n",
    "                                     return_attention_mask=True)\n",
    "token_array = jnp.asarray(tokens[\"input_ids\"])\n",
    "token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "token_array = pz.nx.wrap(token_array, \"batch\", \"seq\").untag(\"batch\").tag(\"batch\")\n",
    "inputs = llama.inputs.from_basic_segments(token_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.activation_manipulation import ablate_direction\n",
    "from penzai.toolshed import basic_training\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"normalize\", \"batch_axis\"))\n",
    "def get_loss(model, rng, state, input_ids, loss_mask, normalize=True, batch_axis=\"direction\"):\n",
    "    del rng, state\n",
    "    inputs = model.inputs.from_basic_segments(input_ids)\n",
    "    logits = model(inputs)\n",
    "    loss = pz.nx.nmap(lambda l, i, m: -jnp.take_along_axis(jax.nn.log_softmax(l[:-1].astype(jnp.float32), -1), i[1:, None], -1)[:, 0] * m[1:] / jnp.maximum(1, m[1:].sum()))(\n",
    "        logits.untag(\"seq\", \"vocabulary\"), input_ids.untag(\"seq\"), loss_mask.untag(\"seq\")).sum().unwrap(\"batch\").mean()\n",
    "    return loss, None, {\"loss\": loss}\n",
    "train_step = basic_training.build_train_step_fn(get_loss, donate_params_and_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c0d3d6f1a3435b9c9086f8df12c3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0628254\n"
     ]
    }
   ],
   "source": [
    "f   rom penzai.toolshed.lora import loraify_linears_in_selection\n",
    "from micrlhf.utils.vector_storage import download_vector\n",
    "import optax\n",
    "import random\n",
    "n_iterations = 50\n",
    "batch_size = 50\n",
    "max_length = 128\n",
    "harmless_weight = 1\n",
    "# direction = download_vector(\"gemma-refusal-l12\", overwrite=True)\n",
    "frozen_llama = llama.select().at_instances_of(pz.nn.Parameter).apply(lambda param: pz.nn.FrozenParameter(param.value, param.name))\n",
    "llama_w_lora_uninit = loraify_linears_in_selection(frozen_llama.select().at_instances_of(pz.nn.Linear), rank=16)\n",
    "llama_w_lora = pz.nn.initialize_parameters(llama_w_lora_uninit, jax.random.key(0))\n",
    "optimizer = optax.chain(optax.zero_nans(), optax.clip_by_global_norm(1.0), optax.adam(1e-6))\n",
    "train_state = basic_training.TrainState.initial_state(\n",
    "    llama_w_lora, optimizer, root_rng=jax.random.PRNGKey(0)\n",
    ")\n",
    "for i in (bar := trange(n_iterations)):\n",
    "    data_harmful = df_adv.sample(batch_size).apply(lambda x: (\n",
    "        tokenizer.encode(format_prompt.format(x[\"goal\"], \"\")),\n",
    "        tokenizer.encode(x[\"target\"])[1:],), axis=1).to_list()\n",
    "    data_harmless = [(tokenizer.encode(format_prompt.format(prompt, \"\")), tokenizer.encode(completion)[1:]) for prompt, completion in [(prompts_harmless[i], completions_harmless[i]) for i in random.sample(list(range(len(prompts_harmless))), k=batch_size)]]\n",
    "    # data = [x + (1,) for x in data_harmful] + [x + (harmless_weight,) for x in data_harmless]\n",
    "    # data = [x + (1,) for x in data_harmful]\n",
    "    data = data_harmless + data_harmful\n",
    "    # data = [x + (harmless_weight,) for x in data_harmless]\n",
    "    # input_ids = [x + y for x, y, _ in data]\n",
    "    input_ids = [x + y for x, y in data]\n",
    "    input_ids = [(x + [tokenizer.pad_token_id] * max(0, max_length - len(x)))[:max_length] for x in input_ids]\n",
    "    # loss_mask = [[0] * len(x) + [z] * len(y) for x, y, z in data]\n",
    "    loss_mask = [[0] * len(x) + [1] * len(y) for x, y in data]\n",
    "    loss_mask = [(x + [0] * (max_length - len(x)))[:max_length] for x in loss_mask]\n",
    "\n",
    "    input_ids = pz.nx.wrap(jnp.asarray(input_ids), \"batch\", \"seq\")\n",
    "    loss_mask = pz.nx.wrap(jnp.asarray(loss_mask), \"batch\", \"seq\")\n",
    "\n",
    "    train_state, out = train_step(train_state, input_ids=input_ids, loss_mask=loss_mask)\n",
    "    loss = out[\"loss\"]\n",
    "    print(loss)\n",
    "    bar.set_postfix(loss=float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
