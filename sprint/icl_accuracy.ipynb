{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "import jax_smi\n",
    "jax_smi.initialise_tracking()\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\", from_type=\"gemma\", load_eager=True, device_map=\"tpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.icl_sfc_utils import Circuitizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'data/itv' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "from sprint.task_vector_utils import load_tasks, ICLRunner\n",
    "tasks = load_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = [\"en_es\", \"antonyms\", \"person_profession\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_continent 0.5729166666666666\n",
      "football_player_position 0.22916666666666666\n",
      "location_religion 0.71875\n",
      "location_language 0.53125\n",
      "person_profession 0.2604166666666667\n",
      "location_country 0.22916666666666666\n",
      "country_capital 0.6875\n",
      "person_language 0.625\n",
      "singular_plural 0.8645833333333334\n",
      "present_simple_past_simple 0.8854166666666666\n",
      "antonyms 0.71875\n",
      "plural_singular 0.8958333333333334\n",
      "present_simple_past_perfect 0.7708333333333334\n",
      "present_simple_gerund 0.9375\n",
      "en_it 0.5208333333333334\n",
      "it_en 0.4583333333333333\n",
      "en_fr 0.5104166666666666\n",
      "en_es 0.6145833333333334\n",
      "fr_en 0.5\n",
      "es_en 0.5104166666666666\n",
      "algo_last 0.84375\n",
      "algo_first 0.953125\n",
      "algo_second 0.40625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for task_name in tasks:\n",
    "\n",
    "    task = tasks[task_name]\n",
    "\n",
    "    pairs = list(task.items())\n",
    "\n",
    "    batch_size = 8\n",
    "    n_shot=12\n",
    "    if task_name.startswith(\"algo\"):\n",
    "        n_shot = 8\n",
    "    max_seq_len = 128\n",
    "    seed = 10\n",
    "\n",
    "    prompt = \"Follow the pattern:\\n{}\"\n",
    "\n",
    "    runner = ICLRunner(task_name, pairs, batch_size=batch_size, n_shot=n_shot, max_seq_len=max_seq_len, seed=seed, prompt=prompt, use_same_examples=False, use_same_target=False)\n",
    "\n",
    "\n",
    "    train_tokens = runner.get_tokens(\n",
    "        runner.train_pairs, tokenizer\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    tokens_wrapped = pz.nx.wrap(train_tokens, \"batch\", \"seq\")\n",
    "    llama_inputs = llama.inputs.from_basic_segments(tokens_wrapped)\n",
    "\n",
    "    logits = llama(llama_inputs)\n",
    "\n",
    "    logits = logits.unwrap(\n",
    "        \"batch\", \"seq\", \"vocabulary\"\n",
    "    )\n",
    "\n",
    "    decoded = [tokenizer.decode([x]) for x in logits[train_tokens == 3978].argmax(axis=-1)]\n",
    "\n",
    "    sep_mask = train_tokens == 3978\n",
    "\n",
    "    sep_mask = np.roll(sep_mask, 1, axis=1)\n",
    "\n",
    "    tgt_tokens = train_tokens[sep_mask]\n",
    "\n",
    "    hits = 0\n",
    "\n",
    "    for p, t in zip(decoded, tgt_tokens):\n",
    "        d = tokenizer.decode([t])\n",
    "\n",
    "        if p.strip()[:3] == d.strip()[:3]:\n",
    "            hits += 1\n",
    "\n",
    "        \n",
    "    print(task_name, hits / len(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_continent 0.6458333333333334\n",
      "football_player_position 0.328125\n",
      "location_religion 0.7291666666666666\n",
      "location_language 0.6822916666666666\n",
      "person_profession 0.3385416666666667\n",
      "location_country 0.3020833333333333\n",
      "country_capital 0.65625\n",
      "person_language 0.6197916666666666\n",
      "singular_plural 0.8802083333333334\n",
      "present_simple_past_simple 0.9479166666666666\n",
      "antonyms 0.7239583333333334\n",
      "plural_singular 0.8645833333333334\n",
      "present_simple_past_perfect 0.7760416666666666\n",
      "present_simple_gerund 0.9270833333333334\n",
      "en_it 0.6354166666666666\n",
      "it_en 0.5364583333333334\n",
      "en_fr 0.5677083333333334\n",
      "en_es 0.71875\n",
      "fr_en 0.5885416666666666\n",
      "es_en 0.5833333333333334\n",
      "algo_last 0.84375\n",
      "algo_first 0.953125\n",
      "algo_second 0.40625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for task_name in tasks:\n",
    "\n",
    "    task = tasks[task_name]\n",
    "\n",
    "    pairs = list(task.items())\n",
    "\n",
    "    batch_size = 8\n",
    "    n_shot=24\n",
    "    if task_name.startswith(\"algo\"):\n",
    "        n_shot = 8\n",
    "    max_seq_len = 256\n",
    "    seed = 10\n",
    "\n",
    "    prompt = \"Follow the pattern:\\n{}\"\n",
    "\n",
    "    runner = ICLRunner(task_name, pairs, batch_size=batch_size, n_shot=n_shot, max_seq_len=max_seq_len, seed=seed, prompt=prompt, use_same_examples=False, use_same_target=False)\n",
    "\n",
    "\n",
    "    train_tokens = runner.get_tokens(\n",
    "        runner.train_pairs, tokenizer\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    tokens_wrapped = pz.nx.wrap(train_tokens, \"batch\", \"seq\")\n",
    "    llama_inputs = llama.inputs.from_basic_segments(tokens_wrapped)\n",
    "\n",
    "    logits = llama(llama_inputs)\n",
    "\n",
    "    logits = logits.unwrap(\n",
    "        \"batch\", \"seq\", \"vocabulary\"\n",
    "    )\n",
    "\n",
    "    decoded = [tokenizer.decode([x]) for x in logits[train_tokens == 3978].argmax(axis=-1)]\n",
    "\n",
    "    sep_mask = train_tokens == 3978\n",
    "\n",
    "    sep_mask = np.roll(sep_mask, 1, axis=1)\n",
    "\n",
    "    tgt_tokens = train_tokens[sep_mask]\n",
    "\n",
    "    hits = 0\n",
    "\n",
    "    for p, t in zip(decoded, tgt_tokens):\n",
    "        d = tokenizer.decode([t])\n",
    "\n",
    "        if p.strip()[:3] == d.strip()[:3]:\n",
    "            hits += 1\n",
    "\n",
    "        \n",
    "    print(task_name, hits / len(decoded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-_SD4q1c9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
