{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "import jax_smi\n",
    "jax_smi.initialise_tracking()\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\", from_type=\"gemma\", load_eager=True, device_map=\"tpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import load_tasks, ICLRunner\n",
    "tasks = load_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763\n",
      "346\n"
     ]
    }
   ],
   "source": [
    "def check_if_single_token(token):\n",
    "    return len(tokenizer.tokenize(token)) == 1\n",
    "\n",
    "task_name = \"es_en\"\n",
    "\n",
    "task = tasks[task_name]\n",
    "\n",
    "print(len(task))\n",
    "\n",
    "task = {\n",
    "    k:v for k,v in task.items() if check_if_single_token(k) and check_if_single_token(v)\n",
    "}\n",
    "\n",
    "print(len(task))\n",
    "\n",
    "pairs = list(task.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import logprob_loss\n",
    "from functools import partial\n",
    "\n",
    "sep = 3978\n",
    "pad = 0\n",
    "\n",
    "def metric_fn(logits, resids, tokens):\n",
    "    return logprob_loss(logits, tokens, sep=sep, pad_token=pad, n_first=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock, LlamaAttention\n",
    "from micrlhf.utils.activation_manipulation import ActivationAddition, wrap_vector\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "from penzai import pz\n",
    "import jax\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"metric\", \"batched\"))\n",
    "def run_with_add(additions_pre, additions_mid, tokens, metric, batched=False, llama=None):\n",
    "    get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "        pz.nn.Sequential([\n",
    "            pz.de.TellIntermediate.from_config(tag=f\"resid_pre_{i}\"),\n",
    "            x\n",
    "        ])\n",
    "    )\n",
    "    get_resids = get_resids.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda l, b: b.select().at_instances_of(pz.nn.Residual).apply_with_selected_index(lambda i, x: x if i == 0 else pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_mid_{l}\"),\n",
    "        x,\n",
    "    ])))\n",
    "\n",
    "\n",
    "    get_resids = get_resids.select().at_instances_of(LlamaAttention).apply_with_selected_index(lambda i, x: x.select().at_instances_of(pz.nn.Softmax).apply(lambda b: pz.nn.Sequential([\n",
    "        b,\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"attn_{i}\"),\n",
    "    ])))\n",
    "\n",
    "    get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: True)\n",
    "    make_additions = get_resids.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "        pz.nn.Sequential([\n",
    "            ActivationAddition(pz.nx.wrap(additions_pre[i], *((\"batch\",) if batched else ()), \"seq\", \"embedding\"), \"all\"),\n",
    "            x\n",
    "        ])\n",
    "    )\n",
    "    make_additions = make_additions.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda l, b: b.select().at_instances_of(pz.nn.Residual).apply_with_selected_index(lambda i, x: x if i == 0 else pz.nn.Sequential([\n",
    "        ActivationAddition(pz.nx.wrap(additions_mid[l], *((\"batch\",) if batched else ()), \"seq\", \"embedding\"), \"all\"),\n",
    "        x,\n",
    "    ])))\n",
    "    tokens_wrapped = pz.nx.wrap(tokens, \"batch\", \"seq\")\n",
    "    logits, resids = make_additions(llama.inputs.from_basic_segments(tokens_wrapped))\n",
    "    return metric(logits.unwrap(\"batch\", \"seq\", \"vocabulary\"), resids, tokens), (logits, resids[::3], resids[1::3], resids[2::3])\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"metric\",))\n",
    "def get_metric_resid_grad(tokens, llama=llama, metric=metric_fn):\n",
    "    additions = [jnp.zeros(tokens.shape + (llama.config.hidden_size,)) for _ in range(llama.config.num_layers)]\n",
    "    batched = tokens.ndim > 1\n",
    "    (metric, (logits, resids_pre, qk, resids_mid)), (grad_pre, grad_mid) = jax.value_and_grad(run_with_add, argnums=(0, 1), has_aux=True)(additions, additions, tokens, metric, batched=batched, llama=llama)\n",
    "    return (\n",
    "        metric,\n",
    "        [r.value.unwrap(\"batch\", \"seq\", \"embedding\") for r in resids_pre],\n",
    "        [r.value.unwrap(\"batch\", \"seq\", \"embedding\") for r in resids_mid],\n",
    "        [r.value.unwrap(\"batch\", \"kv_heads\", \"q_rep\", \"seq\", \"kv_seq\") for r in qk],\n",
    "        grad_pre,\n",
    "        grad_mid\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 \n",
    "n_shot=20\n",
    "max_seq_len = 128\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Follow the pattern:\\n{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ICLRunner(task_name, pairs, batch_size=batch_size, n_shot=n_shot, max_seq_len=max_seq_len, seed=seed, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import tokenized_to_inputs\n",
    "\n",
    "train_tokens = runner.get_tokens(\n",
    "    runner.train_pairs, tokenizer\n",
    ")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value, resids_pre, resids_mid, qk, grad_pre, grad_mid = get_metric_resid_grad(train_tokens, llama=llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rms_block = lambda layer, resid_index: (\n",
    "    llama.select()\n",
    "    .at_instances_of(LlamaBlock).pick_nth_selected(layer)\n",
    "    .at_instances_of(pz.nn.Residual).pick_nth_selected(resid_index)\n",
    "    .at_instances_of(pz.nn.RMSLayerNorm).pick_nth_selected(0)\n",
    "    ).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_rms = [get_rms_block(layer, 1) for layer in range(llama.config.num_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import get_nev_it_sae_suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_to_resid(weights, sae):\n",
    "    if \"s_gate\" in sae:\n",
    "        weights = (weights > 0) * jax.nn.relu(weights * jax.nn.softplus(sae[\"s_gate\"]) * sae.get(\"scaling_factor\", 1.0) + sae[\"b_gate\"])   \n",
    "    else:\n",
    "        weights = jax.nn.relu(weights)\n",
    "\n",
    "    recon = jnp.einsum(\"fv,bsf->bsv\", sae[\"W_dec\"], weights)\n",
    "\n",
    "    if \"norm_factor\" in sae:\n",
    "        recon = recon / sae[\"norm_factor\"]\n",
    "\n",
    "    # recon = recon.astype('bfloat16')\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import sae_encode_gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_normalize(layer, resid_mid):\n",
    "    # return resid_mid / resids_mid_norms[layer] * mlp_rms_weights[layer]\n",
    "    # return resid_mid / jnp.linalg.norm(resid_mid, axis=-1, keepdims=True) * mlp_rms_weights[layer]\n",
    "    return mlp_rms[layer](pz.nx.wrap(resid_mid, \"batch\", \"seq\", \"embedding\")).unwrap(\"batch\", \"seq\", \"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c802bb8bd74de9b38df3657454e9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cab99d08aa4faf8fef7d172e1a6b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bcab6f2ab549008afa5e2449de7efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "ie_attn = {}\n",
    "sae_grads_attn = {}\n",
    "ie_resid = {}\n",
    "sae_grads_resid = {}\n",
    "ie_transcoder = {}\n",
    "sae_grads_transcoder = {}\n",
    "\n",
    "ie_error_attn = {}\n",
    "ie_error_resid = {}\n",
    "ie_error_transcoder = {}\n",
    "\n",
    "def sfc_simple(grad, resid, target, sae):\n",
    "    pre_relu, post_relu, recon = sae_encode_gated(sae, resid)\n",
    "    error = target - recon\n",
    "    f = partial(weights_to_resid, sae=sae)\n",
    "\n",
    "    sae_grad, = jax.vjp(f, post_relu)[1](grad,)\n",
    "    indirect_effects = sae_grad * post_relu\n",
    "    indirect_effects_error = grad * error\n",
    "    return indirect_effects, indirect_effects_error, sae_grad\n",
    "\n",
    "\n",
    "layers = list(range(6, 17))\n",
    "for layer in tqdm(layers):\n",
    "    r_pre, r_mid, g_mid = resids_pre[layer], resids_mid[layer], grad_mid[layer]\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"attn_out\")\n",
    "    indirect_effects, indirect_effects_error, sae_grad = sfc_simple(g_mid, r_mid - r_pre, r_mid - r_pre, sae)\n",
    "    # display((indirect_effects > 0).sum(-1))\n",
    "    ie_attn[layer] = indirect_effects\n",
    "    ie_error_attn[layer] = indirect_effects_error\n",
    "    sae_grads_attn[layer] = sae_grad\n",
    "\n",
    "# for layer, (r_pre, g_pre) in enumerate(zip(resids_pre, grad_pre)):\n",
    "for layer in tqdm(layers):\n",
    "    r_pre, g_pre = resids_pre[layer], grad_pre[layer + 1]\n",
    "    sae = get_nev_it_sae_suite(layer=layer)\n",
    "    indirect_effects, indirect_effects_error, sae_grad = sfc_simple(g_pre, r_pre, r_pre, sae)\n",
    "    # display((indirect_effects != 0).sum(-1))\n",
    "    ie_resid[layer] = indirect_effects\n",
    "    ie_error_resid[layer] = indirect_effects_error\n",
    "    sae_grads_resid[layer] = sae_grad\n",
    "\n",
    "for layer in tqdm(layers[:-1]):\n",
    "    r_mid, r_pre, g_pre = resids_mid[layer], resids_pre[layer + 1], grad_pre[layer + 1]\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"transcoder\")\n",
    "    indirect_effects, indirect_effects_error, sae_grad = sfc_simple(g_pre, mlp_normalize(layer, r_mid), r_pre - r_mid, sae)\n",
    "    # display((indirect_effects != 0).sum(-1))\n",
    "    ie_transcoder[layer] = indirect_effects\n",
    "    ie_error_transcoder[layer] = indirect_effects_error\n",
    "    sae_grads_transcoder[layer] = sae_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_wrapped = pz.nx.wrap(train_tokens, \"batch\", \"seq\")\n",
    "llama_inputs = llama.inputs.from_basic_segments(tokens_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attn_call(layer, resid_pre):\n",
    "#     subblock = llama.select().at_instances_of(LlamaBlock).pick_nth_selected(layer).at_instances_of(pz.nn.Residual).pick_nth_selected(0).get().delta\n",
    "#     subblock = subblock.select().at_instances_of(LlamaAttention).get()\n",
    "\n",
    "#     si_selection = subblock.select().at_instances_of(pz.de.HandledSideInputRef)\n",
    "#     keys = sorted(set([ref.tag for ref in si_selection.get_sequence()]))\n",
    "#     replaced = si_selection.apply(lambda ref: pz.de.SideInputRequest(tag=ref.tag))\n",
    "#     subblock = pz.de.WithSideInputsFromInputTuple.handling(replaced, keys)\n",
    "\n",
    "#     side_inputs = {\n",
    "#         'positions': llama_inputs.positions,\n",
    "#         'attn_mask': llama_inputs.attention_mask\n",
    "#     }\n",
    "    \n",
    "#     resid_pre = resid_pre / \n",
    "\n",
    "#     resid_pre = pz.nx.wrap(resid_pre, \"batch\", \"seq\", \"embedding\")\n",
    "#     attn_out = subblock((resid_pre,) + tuple(side_inputs[tag] for tag in subblock.side_input_tags))\n",
    "\n",
    "#     attn_out = attn_out.unwrap(\"batch\", \"seq\", \"embedding\") \n",
    "\n",
    "#     return attn_out.astype(resid.dtype)\n",
    "\n",
    "# attn_call(6, resids_pre[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length = len(tokenizer.tokenize(prompt))\n",
    "periods = [\"input\", \"arrow\", \"output\", \"newline\"]\n",
    "masks = {\n",
    "    \"prompt\": jnp.zeros_like(train_tokens).at[:, :prompt_length].set(1).astype(bool),\n",
    "    **{\n",
    "        period: jnp.zeros_like(train_tokens).at[:, prompt_length+i::len(periods)].set(1).astype(bool) * (train_tokens != pad) for i, period in enumerate(periods)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_node_effect(ie, top_k=64):\n",
    "    masked = {}\n",
    "    for mask_name, mask in masks.items():\n",
    "        mask = mask[..., None]\n",
    "        masked[mask_name] = ((mask * jnp.abs(ie)).sum(1) / mask.sum(1)).mean(0)\n",
    "    masked = {k: jax.lax.top_k(v, k=top_k) for k,v in masked.items()}\n",
    "    return masked\n",
    "def compute_all_avg_node_effects(ies):\n",
    "    return {k: compute_avg_node_effect(v) for k,v in ies.items()}\n",
    "# compute_all_avg_node_effects(ie_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resids_to_weights(vector, sae):\n",
    "    inputs = vector\n",
    "\n",
    "    if \"norm_factor\" in sae:\n",
    "        inputs = inputs * sae[\"norm_factor\"]\n",
    "\n",
    "    pre_relu = inputs @ sae[\"W_enc\"]\n",
    "    pre_relu = pre_relu +sae[\"b_enc\"]\n",
    "    post_relu = jax.nn.relu(pre_relu)\n",
    "    \n",
    "    post_relu = (post_relu > 0) * jax.nn.relu((inputs @ sae[\"W_enc\"]) * jax.nn.softplus(sae[\"s_gate\"]) * sae[\"scaling_factor\"] + sae[\"b_gate\"])   \n",
    "\n",
    "    return post_relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_average(vector, mask):\n",
    "    mask = masks[mask]\n",
    "    while mask.ndim < vector.ndim:\n",
    "        mask = mask[..., None]\n",
    "\n",
    "    return ((mask * vector).sum(1) / mask.sum(1)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcoder_feature_to_mid(layer, feature_idx, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"transcoder\")\n",
    "    mask = masks[mask]\n",
    "\n",
    "    resid = resids_mid[layer]\n",
    "\n",
    "    def f(resid):\n",
    "        resid = mlp_normalize(layer, resid)\n",
    "        batch_token_feat = resids_to_weights(resid, sae)[:, :, feature_idx] * sae_grads_transcoder[layer][:, :, feature_idx]\n",
    "        token_act = mask_average(batch_token_feat, mask)\n",
    "        return token_act\n",
    "\n",
    "    return jax.grad(f)(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_out_feature_to_pre(layer, feature_idx, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"attn_out\")\n",
    "    mask = masks[mask]\n",
    "\n",
    "    resid = resids_pre[layer]\n",
    "\n",
    "    subblock = llama.select().at_instances_of(LlamaBlock).pick_nth_selected(layer).at_instances_of(pz.nn.Residual).pick_nth_selected(0).delta\n",
    "\n",
    "    si_selection = subblock.select().at_instances_of(pz.de.HandledSideInputRef)\n",
    "    keys = sorted(set([ref.tag for ref in si_selection.get_sequence()]))\n",
    "    replaced = si_selection.apply(lambda ref: pz.de.SideInputRequest(tag=ref.tag))\n",
    "    subblock = pz.de.WithSideInputsFromInputTuple.handling(replaced, keys)\n",
    "\n",
    "    side_inputs = {\n",
    "        'positions': llama_inputs.positions,\n",
    "        'attn_mask': llama_inputs.attention_mask\n",
    "    }\n",
    "\n",
    "    def f(resid):\n",
    "        resid = pz.nx.wrap(resid, \"batch\", \"seq\", \"embedding\")\n",
    "        attn_out = subblock((resid,) + tuple(side_inputs[tag] for tag in subblock.side_input_tags))\n",
    "\n",
    "        attn_out = attn_out.unwrap(\"batch\", \"seq\", \"embedding\") \n",
    "\n",
    "        batch_token_feat = resids_to_weights(attn_out, sae)[:, :, feature_idx] * sae_grads_attn[layer][:, :, feature_idx]\n",
    "        token_act = mask_average(batch_token_feat, mask)\n",
    "        return token_act\n",
    "\n",
    "    return jax.grad(f)(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_feature_to_pre(layer, feature_idx, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer)\n",
    "    mask = masks[mask]\n",
    "    resid = resids_pre[layer]\n",
    "\n",
    "    def f(resid):\n",
    "        batch_token_feat = resids_to_weights(resid, sae)[:, :, feature_idx] * sae_grads_resid[layer][:, :, feature_idx]\n",
    "        token_act = mask_average(batch_token_feat, mask)\n",
    "        return token_act\n",
    "\n",
    "    return jax.grad(f)(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_pre_to_transcoder_features(layer, grad, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"transcoder\")\n",
    "    resid_mid = resids_mid[layer]\n",
    "    resid_mid = mlp_normalize(layer, resid_mid)\n",
    "    ie = sfc_simple(grad, resid_mid, resid_mid, sae)[0]\n",
    "    ie = mask_average(ie, mask)\n",
    "\n",
    "    return ie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_mid_to_attn_features(layer, grad, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"attn_out\")\n",
    "    resid_mid = resids_mid[layer]\n",
    "    resid_pre = resids_pre[layer]\n",
    "\n",
    "    ie = sfc_simple(grad, resid_mid - resid_pre, resid_mid - resid_pre, sae)[0]\n",
    "    ie = mask_average(ie, mask)\n",
    "    return ie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_pre_to_pre_features(layer, grad, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer)\n",
    "    resid = resids_pre[layer]\n",
    "    ie = sfc_simple(grad, resid, resid, sae)[0]\n",
    "    ie = mask_average(ie, mask)\n",
    "    return ie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"output_f0c8dd482b6b40ae9d0622c36e4de912\"><script> /* penzai.treescope rendering of a Python object (compressed) */ (()=>{ let observer; let lastStep = new Promise((resolve, reject) => { observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); }); window.treescope_decompress_enqueue = (encoded, destId) => { const previous = lastStep; const destElt = document.getElementById(destId); lastStep = (async () => { await previous; let blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); let reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); let parts = []; while (true) { let step = await reader.read(); if (step.done) { break; } parts.push(step.value); } let newElt = document.createElement(\"div\"); newElt.innerHTML = parts.join(\"\"); destElt.parentNode.replaceChild(newElt, destElt); for (let oldScript of newElt.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } })(); requestAnimationFrame(() => { observer.observe(destElt); }); } })(); </script><div id=\"compress_html_5ed57ba852454b49af29c768c93f5a49\"><script>window.treescope_decompress_enqueue(\"eNrFV0tv4zYQ/iuzKpCVg1ixnSZpJNtAsN2ih0UvW6CHthBoaWSxpkmVpJK4C//3Din5Le+m6KK1DzKH8/jmLY+NXQmcRlYjmkxVmGqlLHyCShluuZIxaBTM8idMoFDS9gu25GIVw1JJZSqWEf255Bb7/hBDpYkiuLF9r7pvVxVRpZJEnrFsMdeqlnk/U0LpuBFNoD3NBDGQPp7bMoaCW2KTFqVNYMn0nMu+wMLGMMpKZ0Niv0Q+L4kyjG4TWI+vG3fGJtO8slPgBYTPXObqeechTCYTIAhYkIK8R76ecsCndXJCjtIKZc7l/DFzkTHE9uvvX2T7kclcOJWyFqKDe442PYr+BMJN0FOrejCZEsjrS/ig1IKSoMGWCJ5TqhwjuLwGgRZa2T1RnxoyHjZ3byaQq6xeUkCjmcpXcHEBb9xNlAlmzAdKWuQCzrg0YXAIKuiBC9XGhhOqmCZN7wUufYbWZNnWWvpLOnb4qmv5USnpHHxWetF6RiaNJWS/EMldHZAtzxyxQk2OL5nMMJLqOextXTu5gX4jNIabkYPcVQPHqYwEyrktqTRg0FURn89p47eLAAqDO+hlLR32Lxo3JS+sc8lLuB9r+r4WQ7jh1PhnjcY+Sr5kjuMHzZYYNnHt9bozcgylqk3Z5CZ5TeA2ICZNKP5J6F4PuxM4SpKq8XssUGvMf8ZlRWWP5rSAKuGI28InMb36iAIzq/SjEFTorez+EKxlaqhSg17iGy50/UWaQBVeoXOUnnt9o3GpnvCga/Z0nOuEcIPV6S+0+gtdc3jNzeA7hBsGzWBzKp0EnfZdyzSSG21D7vPSr8jNYWJujPhTS8cX+64xtne9IyYeTtPrP7lxo5GileE7ar88JA1XjsHl6WyurJrPRTNGUj/9LQHzU44oKOwV4BNpb0Phys6fowWuXEcGOtiMHmLei3mjNwy2OtMlAQw2ONZA+6BZBOOcP4EXnBzPNbBsRoDxZRIMAlCSjBJ6ucfXjT+0JTcb5AGtnGaR5m1Bpj5IpRI5al+LfsF9w+7dt12lXiQGbpngGeGNCmJnMzLl5vqZJbyGQ7a0QUfcOTdkdLVZtseMMAXBZijieIZU07iHKvOfpNNes3H7Q7dy28U8SHa2uPR7eCaU29xnbZbUHPrUcs70wiCbU6jlqXQslQ2PSCUz4dTrnHbGwcvEWYnZgpZ7Dy57OwxOtFtow3+A0Jd/DG9/G93Osrf/J7xDobMg7/4DkC6PznCtjUtgpTgh0B12ufl6Zn0reEN932/mXI1/Has799wcPLUScZMWXBubKpm68u9orc+1UjS6dd3UmSr41/CbjB9DdF4dvEP7ht57Y66YBJ5Pgu0Ay/Dhvhg85A/DPP/2Doezh8GQDdngfnBT3ONgELRC7Vg97Ptj88TcODHmsqppl9L+mQQe9ky9BJ1KWg/psvFuOr72wvRsde3b7xq8wfRC2OQP9hI9as1WUAjF7M0ovBnd33131YMoiuBibhOKAmmaHj9oaWz/Srz6BSTsbbfO36Kcwjs=\", \"compress_html_5ed57ba852454b49af29c768c93f5a49\");</script><span style=\"color: #aaaaaa; font-family: monospace\">(Loading...)</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"compress_html_0bcad68704364dd5a91574e139bf3507\"><script>window.treescope_decompress_enqueue(\"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\", \"compress_html_0bcad68704364dd5a91574e139bf3507\");</script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"output_dest_f0c8dd482b6b40ae9d0622c36e4de912\"><script> (()=>{ const output = document.getElementById(\"output_f0c8dd482b6b40ae9d0622c36e4de912\"); const dest = document.getElementById(\"output_dest_f0c8dd482b6b40ae9d0622c36e4de912\"); dest.parentNode.replaceChild(output, dest); })(); </script></div>"
      ],
      "text/plain": [
       "# jax.Array float32(32768,) ≈1.1e-08 ±2.4e-06 [≥-0.00017, ≤0.00015] zero:32_682 nonzero:86\n",
       "  Array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie_pre_to_pre_features(6, grad_pre[6], \"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_through_transcoder(layer, grad):\n",
    "    sae = get_nev_it_sae_suite(layer, label=\"transcoder\")\n",
    "    resid_mid = resids_mid[layer]\n",
    "\n",
    "    def f(resid_mid):\n",
    "        resid_mid = mlp_normalize(layer, resid_mid)\n",
    "        # we ignore error nodes\n",
    "        weights = resids_to_weights(resid_mid, sae)\n",
    "        recon = weights_to_resid(weights, sae)\n",
    "\n",
    "        return recon\n",
    "\n",
    "    grad = jax.vjp(f, resid_mid)[1](grad,)\n",
    "\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_through_mlp(layer, grad):\n",
    "    mlp = llama.select().at_instances_of(LlamaBlock).pick_nth_selected(layer).at_instances_of(pz.nn.Residual).pick_nth_selected(1).get().delta\n",
    "    resid_mid = resids_mid[layer]\n",
    "    def f(resid_mid):\n",
    "        resids_mid = pz.nx.wrap(resid_mid, \"batch\", \"seq\", \"embedding\")\n",
    "        out = mlp(resids_mid)\n",
    "        return out.unwrap(\"batch\", \"seq\", \"embedding\")\n",
    "    return jax.vjp(f, resid_mid)[1](grad,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_through_attn(layer, grad):\n",
    "    subblock = llama.select().at_instances_of(LlamaBlock).pick_nth_selected(layer).at_instances_of(pz.nn.Residual).pick_nth_selected(0).get().delta\n",
    "\n",
    "    si_selection = subblock.select().at_instances_of(pz.de.HandledSideInputRef)\n",
    "    keys = sorted(set([ref.tag for ref in si_selection.get_sequence()]))\n",
    "    replaced = si_selection.apply(lambda ref: pz.de.SideInputRequest(tag=ref.tag))\n",
    "    subblock = pz.de.WithSideInputsFromInputTuple.handling(replaced, keys)\n",
    "\n",
    "    side_inputs = {\n",
    "        'positions': llama_inputs.positions,\n",
    "        'attn_mask': llama_inputs.attention_mask\n",
    "    }\n",
    "\n",
    "    def f(resid):\n",
    "        resid_pre = pz.nx.wrap(resid, \"batch\", \"seq\", \"embedding\")\n",
    "        attn_out = subblock((resid_pre,) + tuple(side_inputs[tag] for tag in subblock.side_input_tags))\n",
    "\n",
    "        attn_out = attn_out.unwrap(\"batch\", \"seq\", \"embedding\") \n",
    "\n",
    "        return attn_out.astype(resid.dtype)\n",
    "\n",
    "    resid = resids_pre[layer]\n",
    "    return jax.vjp(f, resid)[1](grad.astype(resid.dtype),)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-_SD4q1c9-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
