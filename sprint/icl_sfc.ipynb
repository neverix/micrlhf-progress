{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "import jax_smi\n",
    "jax_smi.initialise_tracking()\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\", from_type=\"gemma\", load_eager=True, device_map=\"tpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import load_tasks, ICLRunner\n",
    "tasks = load_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763\n",
      "346\n"
     ]
    }
   ],
   "source": [
    "def check_if_single_token(token):\n",
    "    return len(tokenizer.tokenize(token)) == 1\n",
    "\n",
    "task_name = \"es_en\"\n",
    "\n",
    "task = tasks[task_name]\n",
    "\n",
    "print(len(task))\n",
    "\n",
    "task = {\n",
    "    k:v for k,v in task.items() if check_if_single_token(k) and check_if_single_token(v)\n",
    "}\n",
    "\n",
    "print(len(task))\n",
    "\n",
    "pairs = list(task.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import logprob_loss\n",
    "from functools import partial\n",
    "\n",
    "sep = 3978\n",
    "pad = 0\n",
    "\n",
    "def metric_fn(logits, resids, tokens):\n",
    "    return logprob_loss(logits, tokens, sep=sep, pad_token=pad, n_first=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock, LlamaAttention\n",
    "from micrlhf.utils.activation_manipulation import ActivationAddition, wrap_vector\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "from penzai import pz\n",
    "import jax\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"metric\", \"batched\"))\n",
    "def run_with_add(additions_pre, additions_mid, tokens, metric, batched=False, llama=None):\n",
    "    get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "        pz.nn.Sequential([\n",
    "            pz.de.TellIntermediate.from_config(tag=f\"resid_pre_{i}\"),\n",
    "            x\n",
    "        ])\n",
    "    )\n",
    "    get_resids = get_resids.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda l, b: b.select().at_instances_of(pz.nn.Residual).apply_with_selected_index(lambda i, x: x if i == 0 else pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_mid_{l}\"),\n",
    "        x,\n",
    "    ])))\n",
    "\n",
    "\n",
    "    get_resids = get_resids.select().at_instances_of(LlamaAttention).apply_with_selected_index(lambda i, x: x.select().at_instances_of(pz.nn.Softmax).apply(lambda b: pz.nn.Sequential([\n",
    "        b,\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"attn_{i}\"),\n",
    "    ])))\n",
    "\n",
    "    get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: True)\n",
    "    make_additions = get_resids.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "        pz.nn.Sequential([\n",
    "            ActivationAddition(pz.nx.wrap(additions_pre[i], *((\"batch\",) if batched else ()), \"seq\", \"embedding\"), \"all\"),\n",
    "            x\n",
    "        ])\n",
    "    )\n",
    "    make_additions = make_additions.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda l, b: b.select().at_instances_of(pz.nn.Residual).apply_with_selected_index(lambda i, x: x if i == 0 else pz.nn.Sequential([\n",
    "        ActivationAddition(pz.nx.wrap(additions_mid[l], *((\"batch\",) if batched else ()), \"seq\", \"embedding\"), \"all\"),\n",
    "        x,\n",
    "    ])))\n",
    "    tokens_wrapped = pz.nx.wrap(tokens, \"batch\", \"seq\")\n",
    "    logits, resids = make_additions(llama.inputs.from_basic_segments(tokens_wrapped))\n",
    "    return metric(logits.unwrap(\"batch\", \"seq\", \"vocabulary\"), resids, tokens), (logits, resids[::3], resids[1::3], resids[2::3])\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"metric\",))\n",
    "def get_metric_resid_grad(tokens, llama=llama, metric=metric_fn):\n",
    "    additions = [jnp.zeros(tokens.shape + (llama.config.hidden_size,)) for _ in range(llama.config.num_layers)]\n",
    "    batched = tokens.ndim > 1\n",
    "    (metric, (logits, resids_pre, qk, resids_mid)), (grad_pre, grad_mid) = jax.value_and_grad(run_with_add, argnums=(0, 1), has_aux=True)(additions, additions, tokens, metric, batched=batched, llama=llama)\n",
    "    return (\n",
    "        metric,\n",
    "        [r.value.unwrap(\"batch\", \"seq\", \"embedding\") for r in resids_pre],\n",
    "        [r.value.unwrap(\"batch\", \"seq\", \"embedding\") for r in resids_mid],\n",
    "        [r.value.unwrap(\"batch\", \"kv_heads\", \"q_rep\", \"seq\", \"kv_seq\") for r in qk],\n",
    "        grad_pre,\n",
    "        grad_mid\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 \n",
    "n_shot=20\n",
    "max_seq_len = 128\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Follow the pattern:\\n{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ICLRunner(task_name, pairs, batch_size=batch_size, n_shot=n_shot, max_seq_len=max_seq_len, seed=seed, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import tokenized_to_inputs\n",
    "\n",
    "train_tokens = runner.get_tokens(\n",
    "    runner.train_pairs, tokenizer\n",
    ")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value, resids_pre, resids_mid, qk, grad_pre, grad_mid = get_metric_resid_grad(train_tokens, llama=llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import get_nev_it_sae_suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_to_resid(weights, sae):\n",
    "    if \"s_gate\" in sae:\n",
    "        weights = (weights > 0) * jax.nn.relu(weights * jax.nn.softplus(sae[\"s_gate\"]) * sae.get(\"scaling_factor\", 1.0) + sae[\"b_gate\"])   \n",
    "    else:\n",
    "        weights = jax.nn.relu(weights)\n",
    "\n",
    "    recon = jnp.einsum(\"fv,bsf->bsv\", sae[\"W_dec\"], weights)\n",
    "\n",
    "    if \"norm_factor\" in sae:\n",
    "        recon = recon / sae[\"norm_factor\"]\n",
    "\n",
    "    # recon = recon.astype('bfloat16')\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import sae_encode_gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e215a45660c741c0bfa5a7d0293fd66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc6fb06fd0549da80d499f1322fcdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80cf8e50d4349bca5150ca14132c7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "ie_attn = {}\n",
    "sae_grads_attn = {}\n",
    "ie_resid = {}\n",
    "sae_grads_resid = {}\n",
    "ie_transcoder = {}\n",
    "sae_grads_transcoder = {}\n",
    "\n",
    "ie_error_attn = {}\n",
    "ie_error_resid = {}\n",
    "ie_error_transcoder = {}\n",
    "\n",
    "def sfc_simple(grad, resid, target, sae):\n",
    "    pre_relu, post_relu, recon = sae_encode_gated(sae, resid)\n",
    "    error = target - recon\n",
    "    f = partial(weights_to_resid, sae=sae)\n",
    "\n",
    "    sae_grad, = jax.vjp(f, post_relu)[1](grad,)\n",
    "    indirect_effects = sae_grad * post_relu\n",
    "    indirect_effects_error = grad * error\n",
    "    return indirect_effects, indirect_effects_error, sae_grad\n",
    "\n",
    "\n",
    "layers = list(range(6, 17))\n",
    "for layer in tqdm(layers):\n",
    "    r_pre, r_mid, g_mid = resids_pre[layer], resids_mid[layer], grad_mid[layer]\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"attn_out\")\n",
    "    indirect_effects, indirect_effects_error, sae_grad = sfc_simple(g_mid, r_mid - r_pre, r_mid - r_pre, sae)\n",
    "    # display((indirect_effects > 0).sum(-1))\n",
    "    ie_attn[layer] = indirect_effects\n",
    "    ie_error_attn[layer] = indirect_effects_error\n",
    "    sae_grads_attn[layer] = sae_grad\n",
    "\n",
    "# for layer, (r_pre, g_pre) in enumerate(zip(resids_pre, grad_pre)):\n",
    "for layer in tqdm(layers):\n",
    "    r_pre, g_pre = resids_pre[layer], grad_pre[layer + 1]\n",
    "    sae = get_nev_it_sae_suite(layer=layer)\n",
    "    indirect_effects, indirect_effects_error, sae_grad = sfc_simple(g_pre, r_pre, r_pre, sae)\n",
    "    # display((indirect_effects != 0).sum(-1))\n",
    "    ie_resid[layer] = indirect_effects\n",
    "    ie_error_resid[layer] = indirect_effects_error\n",
    "    sae_grads_resid[layer] = sae_grad\n",
    "\n",
    "for layer in tqdm(layers[:-1]):\n",
    "    r_mid, r_pre, g_pre = resids_mid[layer], resids_pre[layer + 1], grad_pre[layer + 1]\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"transcoder\")\n",
    "    indirect_effects, indirect_effects_error, sae_grad = sfc_simple(g_pre, r_mid, r_pre, sae)\n",
    "    # display((indirect_effects != 0).sum(-1))\n",
    "    ie_transcoder[layer] = indirect_effects\n",
    "    ie_error_transcoder[layer] = indirect_effects_error\n",
    "    sae_grads_transcoder[layer] = sae_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length = len(tokenizer.tokenize(prompt))\n",
    "periods = [\"input\", \"arrow\", \"output\", \"newline\"]\n",
    "masks = {\n",
    "    \"prompt\": jnp.zeros_like(train_tokens).at[:, :prompt_length].set(1).astype(bool),\n",
    "    **{\n",
    "        period: jnp.zeros_like(train_tokens).at[:, prompt_length+i::len(periods)].set(1).astype(bool) * (train_tokens != pad) for i, period in enumerate(periods)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_node_effect(ie, top_k=64):\n",
    "    masked = {}\n",
    "    for mask_name, mask in masks.items():\n",
    "        mask = mask[..., None]\n",
    "        masked[mask_name] = ((mask * jnp.abs(ie)).sum(1) / mask.sum(1)).mean(0)\n",
    "    masked = {k: jax.lax.top_k(v, k=top_k) for k,v in masked.items()}\n",
    "    return masked\n",
    "def compute_all_avg_node_effects(ies):\n",
    "    return {k: compute_avg_node_effect(v) for k,v in ies.items()}\n",
    "# compute_all_avg_node_effects(ie_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resids_to_weights(vector, sae):\n",
    "    inputs = vector\n",
    "\n",
    "    if \"norm_factor\" in sae:\n",
    "        inputs = inputs * sae[\"norm_factor\"]\n",
    "\n",
    "    pre_relu = inputs @ sae[\"W_enc\"]\n",
    "    pre_relu = pre_relu +sae[\"b_enc\"]\n",
    "    post_relu = jax.nn.relu(pre_relu)\n",
    "    \n",
    "    post_relu = (post_relu > 0) * jax.nn.relu((inputs @ sae[\"W_enc\"]) * jax.nn.softplus(sae[\"s_gate\"]) * sae[\"scaling_factor\"] + sae[\"b_gate\"])   \n",
    "\n",
    "    return post_relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_wrapped = pz.nx.wrap(train_tokens, \"batch\", \"seq\")\n",
    "llama_inputs = llama.inputs.from_basic_segments(tokens_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcoder_feature_to_mid(layer, feature_idx, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"transcoder\")\n",
    "    mask = masks[mask]\n",
    "\n",
    "    resid = resids_mid[layer]\n",
    "\n",
    "    def f(resid):\n",
    "        batch_token_feat = resids_to_weights(resid, sae)[:, :, feature_idx] * sae_grads_transcoder[layer][:, :, feature_idx]\n",
    "        token_act = ((mask * batch_token_feat).sum(1) / mask.sum(1)).mean(0)\n",
    "        return token_act\n",
    "\n",
    "    return jax.grad(f)(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_out_feature_to_pre(layer, feature_idx, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer, label=\"attn_out\")\n",
    "    mask = masks[mask]\n",
    "\n",
    "    resid = resids_pre[layer]\n",
    "\n",
    "    subblock = llama.select().at_instances_of(LlamaBlock).pick_nth_selected(layer).at_instances_of(pz.nn.Residual).pick_nth_selected(0)\n",
    "\n",
    "    si_selection = subblock.get().select().at_instances_of(pz.de.HandledSideInputRef)\n",
    "    keys = sorted(set([ref.tag for ref in si_selection.get_sequence()]))\n",
    "    replaced = si_selection.apply(lambda ref: pz.de.SideInputRequest(tag=ref.tag))\n",
    "    subblock = pz.de.WithSideInputsFromInputTuple.handling(replaced, keys)\n",
    "\n",
    "    side_inputs = {\n",
    "        'positions': llama_inputs.positions,\n",
    "        'attn_mask': llama_inputs.attention_mask\n",
    "    }\n",
    "\n",
    "    def f(resid):\n",
    "        resid = pz.nx.wrap(resid, \"batch\", \"seq\", \"embedding\")\n",
    "        attn_out = subblock((resid,) + tuple(side_inputs[tag] for tag in subblock.side_input_tags))\n",
    "\n",
    "        attn_out = attn_out.unwrap(\"batch\", \"seq\", \"embedding\") \n",
    "\n",
    "        batch_token_feat = resids_to_weights(attn_out, sae)[:, :, feature_idx] * sae_grads_attn[layer][:, :, feature_idx]\n",
    "        token_act = ((mask * batch_token_feat).sum(1) / mask.sum(1)).mean(0)\n",
    "        return token_act\n",
    "\n",
    "    return jax.grad(f)(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_feature_to_pre(layer, feature_idx, mask):\n",
    "    sae = get_nev_it_sae_suite(layer=layer)\n",
    "    mask = masks[mask]\n",
    "    resid = resids_pre[layer]\n",
    "\n",
    "    def f(resid):\n",
    "        batch_token_feat = resids_to_weights(resid, sae)[:, :, feature_idx] * sae_grads_resid[layer][:, :, feature_idx]\n",
    "        token_act = ((mask * batch_token_feat).sum(1) / mask.sum(1)).mean(0)\n",
    "        return token_act\n",
    "\n",
    "    return jax.grad(f)(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_pre_to_transcoder_feature(layer, feature, mask):\n",
    "    sae = get_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-_SD4q1c9-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
