{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "# pz.ts.register_as_default()\n",
    "# pz.ts.register_autovisualize_magic()\n",
    "# pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "filename = \"models/gemma-2b-it.gguf\"\n",
    "llama = LlamaTransformer.from_pretrained(filename, device_map=\"auto\", from_type=\"gemma\", load_eager=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/gemma-2b-it-tokenizer\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from micrlhf.sampling import sample, LlamaKVCachingTransformer\n",
    "from micrlhf.utils.activation_manipulation import replace_activation, add_vector\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock\n",
    "from micrlhf.flash import flashify\n",
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "    pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_pre.{i}\"),\n",
    "        x\n",
    "    ])\n",
    ")\n",
    "get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: x.startswith(\"resid_pre\"))\n",
    "get_resids_call = jit_wrapper.Jitted(get_resids)\n",
    "def rep_w_linear(mod):\n",
    "    val = mod.table.embeddings.value  # vocabulary, embedding\n",
    "    return pz.nn.Linear(pz.nn.Parameter(val, \"input_embed\"), [\"vocabulary\"], [\"embedding\"])\n",
    "get_resids_one_hot = get_resids.select().at_instances_of(pz.nn.EmbeddingLookup).apply(rep_w_linear)\n",
    "get_resids_one_hot_call = jit_wrapper.Jitted(get_resids_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "from tqdm.auto import trange\n",
    "from penzai.toolshed import sharding_util\n",
    "import dataclasses\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(logits, inputs):\n",
    "    losses = pz.nx.nmap(lambda l, i: jnp.take_along_axis(jax.nn.log_softmax(l[:-1], -1), i[1:, None], 1)[:, 0].mean()\n",
    "                        )(logits.untag(\"seq\", \"vocabulary\"), inputs.tokens.untag(\"seq\"))\n",
    "    return -losses\n",
    "\n",
    "bs_start = llama.mesh.shape[\"dp\"]\n",
    "# n_x = 16\n",
    "# n_x = 19\n",
    "n_x = 20\n",
    "# n_x = 32\n",
    "has_end = False\n",
    "tokens_init = tokenizer.encode(f\"X{' X' * (n_x - 1)}\")\n",
    "optim_mask = [\"X\" in tokenizer.decode([token]) for token in tokens_init]\n",
    "assert sum(optim_mask) == n_x\n",
    "tokens_init = np.asarray(tokens_init)\n",
    "MAX_ELITES = 16\n",
    "PROB_SWAP = 0.1  # probability of a swap\n",
    "PROB_GRADS = 0.8    # probability of using gradients \n",
    "tokens_init = np.repeat(tokens_init[None, :], MAX_ELITES, axis=0)\n",
    "def tokens_to_array(tokens):\n",
    "    token_array = jnp.asarray(tokens)\n",
    "    if len(token_array) >= bs_start:\n",
    "        token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\")\n",
    "    return token_array\n",
    "def run_tokens(token_array, grad_metric=None):\n",
    "    if not isinstance(token_array, pz.nx.NamedArray):\n",
    "        token_array = tokens_to_array(token_array)\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    if grad_metric:\n",
    "        @partial(jax.grad, has_aux=True)\n",
    "        def lwg(x):\n",
    "            logits, resids = get_resids_one_hot_call(dataclasses.replace(inputs, tokens=x))\n",
    "            resids = {resid.tag: resid.value for resid in resids}\n",
    "            metric = grad_metric(logits, resids, inputs)\n",
    "            return metric, (logits, resids)\n",
    "        vocab = llama.select().at_instances_of(pz.nn.EmbeddingLookup).get_sequence()[0].table.embeddings.value.named_shape[\"vocabulary\"]\n",
    "        one_hots = pz.nx.nmap(lambda x: jax.nn.one_hot(x, vocab))(inputs.tokens).tag(\"vocabulary\")\n",
    "        grad, (logits, resids) = lwg(one_hots)\n",
    "    else:\n",
    "        logits, resids = get_resids_call(inputs)\n",
    "    losses = loss_fn(logits, inputs)\n",
    "    if not grad_metric:\n",
    "        resids = {resid.tag: resid.value for resid in resids}\n",
    "    return_vals = logits, losses, resids\n",
    "    if grad_metric:\n",
    "        return_vals = return_vals + (grad,)\n",
    "    return return_vals\n",
    "\n",
    "mask = jax.device_put(jnp.asarray(optim_mask), jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"sp\")))\n",
    "\n",
    "# @partial(jax.jit, static_argnames=(\"max_inv_temp\", \"expected_changes\"))\n",
    "@jax.jit\n",
    "def temper(logits, key, elites, grads, max_inv_temp, expected_changes):\n",
    "    key_choice, key_random = jax.random.split(key)\n",
    "    index = jax.random.randint(key_choice, (), 0, len(logits) - 1)\n",
    "    key_categorical, key_uniform, key_bernoulli, key_randint, key_use_grads, key_mutations = jax.random.split(key_random, 6)\n",
    "    logit = logits[index]\n",
    "    elite = elites[index]\n",
    "    grads = grads[index]\n",
    "    \n",
    "    logit = jnp.roll(logit, 1, 0)\n",
    "    logit = logit * jax.random.uniform(key_uniform, minval=0, maxval=max_inv_temp)\n",
    "    logit = jnp.where((jnp.cumsum(mask) != 1)[:, None], logit, 0)\n",
    "    use_grads = jax.random.bernoulli(key_use_grads, p=PROB_GRADS).astype(jnp.int_)\n",
    "    logit = jax.lax.switch(use_grads, ((lambda x: x), (lambda x: jnp.where(grads, x, -jnp.inf))), logit)\n",
    "    to_change = jax.random.bernoulli(key_bernoulli, jnp.maximum(.5, expected_changes - 1) / mask.sum(), mask.shape)\n",
    "    definite_indices = jax.random.randint(key_randint, mask.shape[:-1], 0, mask.shape[-1])\n",
    "    definite_mask = jax.nn.one_hot(definite_indices, to_change.shape[-1], dtype=jnp.bool_)\n",
    "    to_change = to_change | definite_mask\n",
    "    changed = jnp.where(mask & to_change,\n",
    "                        jax.random.categorical(key_categorical, logit),\n",
    "                        elite)\n",
    "    \n",
    "    key_swap, key_mutations = jax.random.split(key_mutations)\n",
    "    swap_indices = jax.random.randint(key_swap, (2,), 0, mask.sum())\n",
    "    swap_from, swap_to = jnp.nonzero(mask, size=len(mask))[0][swap_indices]\n",
    "    key_swap, key_mutations = jax.random.split(key_mutations)\n",
    "    do_swap = jax.random.bernoulli(key_swap, p=PROB_SWAP)\n",
    "    swapped = changed.at[swap_from].set(changed[swap_to]).at[swap_to].set(changed[swap_from])\n",
    "    changed = jax.lax.cond(do_swap, lambda x: swapped, lambda x: x, changed)\n",
    "    \n",
    "    # key_delete, key_mutations = jax.random.split(key_mutations)\n",
    "    # delete_index = jax.random.randint(key_delete, (), 0, mask.sum())\n",
    "    # indices = jnp.cumsum(mask)\n",
    "    # indices_base = jnp.arange(len(changed))\n",
    "    # deleted = jnp.where(mask, jnp.where(indices > delete_index, indices_base + 1, indices_base), changed)\n",
    "    # key_delete, key_mutations = jax.random.split(key_mutations)\n",
    "    # do_delete = jax.random.bernoulli(key_delete, p=0.1)\n",
    "    # changed = jax.lax.cond(do_delete, lambda x: deleted, lambda x: x, changed)\n",
    "    \n",
    "    return changed\n",
    "\n",
    "# @partial(jax.jit, static_argnames=(\"key\", \"candidates\", \"expected_changes\"))\n",
    "def algo_iteration(elites, vector, key, candidates=64, seed=13, expected_changes=1.5, max_inv_temp=2, topk=128):\n",
    "    elites = elites.untag(\"solutions\").tag(\"batch\")\n",
    "    logits, _, _, grads = run_tokens(elites, grad_metric=lambda _l, r, _i: (r[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().data_array.mean())\n",
    "    grads = pz.nx.nmap(lambda x: x >= jax.lax.top_k(x, topk)[0][-1])(grads.untag(\"vocabulary\")).tag(\"vocabulary\")\n",
    "    logits = logits.untag(\"batch\").tag(\"elites\")\n",
    "\n",
    "    tempered_samples = pz.nx.nmap(temper)(\n",
    "        logits.untag(\"elites\", \"seq\", \"vocabulary\"),\n",
    "        pz.nx.wrap(jax.random.split(jax.random.key(seed), candidates), \"batch\"),\n",
    "        elites.untag(\"batch\", \"seq\"),\n",
    "        grads.untag(\"batch\", \"seq\", \"vocabulary\"),\n",
    "        pz.nx.wrap(jnp.array(max_inv_temp)), pz.nx.wrap(jnp.array(expected_changes))).tag(\"seq\")\n",
    "    # tempered_samples = sharding_util.name_to_name_device_put(tempered_samples, llama.mesh, dict(batch=\"dp\", seq=\"sp\"))\n",
    "    _, new_losses, new_resids = run_tokens(tempered_samples)\n",
    "\n",
    "    new_scores = (new_resids[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().astype(new_losses.dtype)\n",
    "    metrics = pz.nx.nmap(lambda *xs: jnp.stack(xs))(new_losses, new_scores).tag(\"metrics\")\n",
    "    solution_axes = [k for k in tempered_samples.named_shape.keys() if k != \"seq\"]\n",
    "    solutions = tempered_samples.untag(*solution_axes).flatten().tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    metrics = metrics.untag(*(k for k in solution_axes if k != \"seq\")).flatten().tag(\"solutions\").unwrap(\"solutions\", \"metrics\")\n",
    "\n",
    "    return solutions, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import get_jb_it_sae\n",
    "from micrlhf.utils.vector_storage import save_and_upload_vector, download_vector\n",
    "import jax.numpy as jnp\n",
    "layer = 12\n",
    "sae = get_jb_it_sae()\n",
    "dictionary = sae[\"W_dec\"]\n",
    "features = [321, 330, 2079, 5324, 5373, 8361, 8618, 8631, 12017]\n",
    "features += [4597, 10681, 11046, 11256, 12701, 15553]\n",
    "features *= 3\n",
    "for feature in features:\n",
    "    vector = dictionary[feature]\n",
    "    vector = vector / jnp.linalg.norm(vector)\n",
    "    sae_type = \"Residual\"\n",
    "\n",
    "    import random\n",
    "\n",
    "    rng_seed = random.randint(0, 2**32-1)\n",
    "    print(\"Seed:\", rng_seed, \"Feature:\", feature, \"SAE type:\", sae_type)\n",
    "    np.random.seed(rng_seed)\n",
    "    toks_init = tokens_init.copy()\n",
    "    toks_init[:, optim_mask] = np.random.randint(100, tokenizer.vocab_size, toks_init[:, optim_mask].shape)\n",
    "    best_metrics = None\n",
    "    best = tokens_to_array(toks_init).untag(\"batch\").tag(\"solutions\")\n",
    "    xent_min = 1/20\n",
    "    xent_max = 20\n",
    "    weights = jnp.stack((\n",
    "        -jnp.exp(jnp.linspace(jnp.log(xent_min), jnp.log(xent_max), MAX_ELITES))[::-1],\n",
    "        jnp.ones(MAX_ELITES),\n",
    "    ), -1)\n",
    "    @partial(jax.jit, donate_argnums=(0, 1))\n",
    "    def combine_solutions(best_metrics, best, metrics, solutions):\n",
    "        if best_metrics is not None:\n",
    "            best_metrics = jnp.concatenate((best_metrics, metrics), 0)\n",
    "            best = pz.nx.nmap(lambda a, b: jnp.concatenate((a, b)))(\n",
    "                best.untag(\"solutions\"),\n",
    "                pz.nx.wrap(solutions, \"solutions\", \"seq\").untag(\"solutions\")\n",
    "            ).tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "        else:\n",
    "            best_metrics = metrics\n",
    "            best = solutions\n",
    "        elite_mask = (best_metrics[None, :] * weights[:, None]).sum(-1).argmax(1)\n",
    "        best_metrics = best_metrics[elite_mask]\n",
    "        best = pz.nx.wrap(best[elite_mask], \"solutions\", \"seq\")\n",
    "        return best_metrics, best\n",
    "    try:\n",
    "        for seed in (bar := trange(750)):\n",
    "            solutions, metrics = algo_iteration(best, vector, seed=seed, key=f\"resid_pre.{layer}\")\n",
    "            best_metrics, best = combine_solutions(best_metrics, best, metrics, solutions)\n",
    "            m = {}\n",
    "            for index in range(MAX_ELITES):\n",
    "                i = index\n",
    "                m |= {f\"decoded.{i}\": tokenizer.decode(best[{\"solutions\": index}].unwrap(\"seq\")),\n",
    "                    f\"loss.{i}\": best_metrics[index][0], f\"score.{i}\": best_metrics[index][1]}\n",
    "            bar.set_postfix(**m)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    examples = []\n",
    "    for index in range(MAX_ELITES):\n",
    "        activation_values = pz.nx.nmap(lambda x: x @ vector)(get_resids_call(llama.inputs.from_basic_segments(best[{\"solutions\": index}]))[1][layer].value.untag(\"embedding\")).unwrap(\"seq\").tolist()\n",
    "        bos = tokenizer.encode(\"\")[0]\n",
    "        tokens = [s[5:] for s in tokenizer.batch_decode([[bos] + [t] for t in best[{\"solutions\": index}].unwrap(\"seq\").tolist()])]\n",
    "        examples.append(dict(\n",
    "            Activations_per_token=activation_values,\n",
    "            Tokens=tokens,\n",
    "            Cross_entropy_score=float(best_metrics[index][0]), EPO_metric_score=float(best_metrics[index][1]),\n",
    "        ))\n",
    "\n",
    "    import os\n",
    "    import json\n",
    "    os.makedirs(\"data/epo\", exist_ok=True)\n",
    "    feat_id = feature if sae_type not in ('Refusal', 'Sycophancy') else None\n",
    "    short_metadata = f\"{rng_seed}-{layer}-{sae_type}-{feat_id}-{n_x}-{has_end}-{MAX_ELITES}-{xent_min}-{xent_max}\"\n",
    "    run_info = {    \n",
    "        \"SAE_metadata\": {\n",
    "            \"Layer\": layer,\n",
    "            \"SAE_type\": sae_type,\n",
    "            \"Feature_ID\": feat_id,\n",
    "            \"Has_end\": has_end,\n",
    "            \"N_tokens\": n_x,\n",
    "            \"N_elites\": MAX_ELITES,\n",
    "            \"Seed\": rng_seed,\n",
    "            \"Xent_min\": xent_min,\n",
    "            \"Xent_max\": xent_max,\n",
    "        },\n",
    "        \"Examples\": examples\n",
    "    }\n",
    "    open(f\"data/epo/{short_metadata}.json\", \"w\").write(json.dumps(run_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
