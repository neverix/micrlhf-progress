{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from micrlhf.utils.load_sae import get_sae\n",
    "# import jax.numpy as jnp\n",
    "# # sae = get_sae(20, 5)\n",
    "# # vector = sae[\"W_dec\"][32524]\n",
    "# sae = get_sae(18, 6)\n",
    "# vector = sae[\"W_dec\"][22578]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09c4c2882744f06bce21fa2163d7b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c220f0acd0d4a4d9798ef9f799f5054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2718b088dd46418424aa1fa9729381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5916265279fa4757a65130c0ae0e3b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6459a4c036a54151890ce6a2ca1cb104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "filename = \"models/phi-3-16.gguf\"\n",
    "llama = LlamaTransformer.from_pretrained(filename, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from micrlhf.sampling import sample, LlamaKVCachingTransformer\n",
    "from micrlhf.utils.activation_manipulation import replace_activation, add_vector\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def self_explain(vector, bs=128, msl=48, prompt=\"<|user|>\\nWhat is the meaning of the word \\\"X\\\"?<|end|>\\n<|assistant|>\\nThe meaning of the word \\\"X\\\" is \\\"\"):\n",
    "    scales = np.linspace(0, 200, bs)\n",
    "    act_rep = replace_activation(llama, vector[None, :] * jnp.asarray(scales)[:, None], prompt=prompt, tokenizer=tokenizer)\n",
    "    act_rep = LlamaKVCachingTransformer.from_uncached(act_rep, cache_len=msl, batch_axes={\"batch\": bs})\n",
    "    texts = sample(act_rep, tokenizer, prompt, batch_size=bs, do_sample=True, max_seq_len=msl, verbose=False)[0]\n",
    "    return list(zip(scales, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import get_sae\n",
    "from micrlhf.utils.vector_storage import save_and_upload_vector, download_vector\n",
    "import jax.numpy as jnp\n",
    "# sae = get_sae(20, 5)\n",
    "# vector = sae[\"W_dec\"][32524]\n",
    "# vector = get_sae(16, 5)[\"W_dec\"][25686]\n",
    "\n",
    "# dictionary = get_sae(20, 6)[\"W_dec\"]\n",
    "\n",
    "# layer residual differences\n",
    "# 42846, 34032, 21680, 40173, 32500, 40066, 36997, 14996, 29678, 13795, 5662\n",
    "# average: an action that is not recommended or is inappropriate; to remain silent; not to be done\n",
    "# 42846, 34032, 21680, 40173, 32500, 14996, 5662, 15348 (there for 3 layers straight)\n",
    "# average: empty space, indefinite article\n",
    "# 42846, 34032, 21680, 40173, 32500, 14996, 5662, 29678\n",
    "# average: not to be a desirable or accepted behavior; doing something with harmful or negative consequences\n",
    "\n",
    "# found to decrease loss\n",
    "# latter are more likely to be refusal features\n",
    "# 49065 25054 14996 18193  1290 19550 28321 14551  5849 13136 45612 14438  6153 26856   110 13783\n",
    "\n",
    "# 13783 - an official or academic document, usually a journal article or a dissertation, that is submitted for consideration or review by a committee\n",
    "# 110 - location where an action takes place; religion; not allowed; in fact or actually\n",
    "# 26856 - 'echo', unable to resign\n",
    "# 6153 -\n",
    "# 14438 -\n",
    "# 45612 -\n",
    "# 13136 -\n",
    "# mean up to this: indicating refusal, to stop oneself from doing something, the act of stopping oneself from doing something\n",
    "# 5849 -\n",
    "# 14551 -\n",
    "# 28321 -\n",
    "# 19550 -\n",
    "# 1290 -\n",
    "# 18193 -\n",
    "# 14996 -\n",
    "# 25054 -\n",
    "# 49065 -\n",
    "\n",
    "# vector = dictionary[jnp.array([13783, 110, 26856, 6153, 14438, 45612, 13136])].mean(0)\n",
    "# vector = dictionary[jnp.array([42846, 34032, 21680, 40173, 32500, 14996, 5662, 29678])].mean(0)\n",
    "# def save(features, name):\n",
    "#     vector = dictionary[jnp.array(features)].mean(0)\n",
    "#     vector = vector / jnp.linalg.norm(vector)\n",
    "#     save_and_upload_vector(f\"phi-refusal-{name}\", vector, overwrite=True)\n",
    "    \n",
    "# save([42846, 34032, 21680, 40173, 32500, 14996, 5662, 29678], \"residual-commons\")\n",
    "# save([13783, 110, 26856, 6153, 14438, 45612, 13136], \"picks-avg\")\n",
    "# save([13783, 110, 26856, 6153, 14438, 45612, 13136, 5849, 14551, 28321, 19550, 1290, 18193, 14996, 25054, 49065], \"pics-avg-all\")\n",
    "vector = download_vector(\"phi-refusal-ablit\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from micrlhf.utils.activation_manipulation import replace_activation\n",
    "# from micrlhf.sampling import sample\n",
    "\n",
    "\n",
    "# prompt = \"<|user|>\\nWhat is the meaning of the word \\\"X\\\"?<|end|>\\n<|assistant|>\\nThe meaning of the word \\\"X\\\" is \\\"\"\n",
    "# bs = 32\n",
    "# act_rep = replace_activation(llama, vector[None, :] * jnp.linspace(0, 70, bs)[:, None], prompt=prompt, tokenizer=tokenizer)\n",
    "# sample(act_rep, tokenizer, prompt, batch_size=bs, do_sample=True, max_seq_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from micrlhf.utils.activation_manipulation import add_vector\n",
    "# prompt = \"<|user|>\\n\"\n",
    "# bs = 16\n",
    "# act_add = add_vector(llama, vector[None, :] * jnp.linspace(70, 70, bs)[:, None], 20)\n",
    "# sample(act_add, tokenizer, prompt, batch_size=bs, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epo\n",
    "# import numpy as np\n",
    "# from pprint import pprint\n",
    "# from matplotlib import pyplot as plt\n",
    "# from micrlhf.sampling import sample, LlamaKVCachingTransformer\n",
    "# from micrlhf.utils.activation_manipulation import replace_activation, add_vector\n",
    "\n",
    "\n",
    "# pprint(self_explain(download_vector(\"phi-refusal-ablit\", overwrite=True)))\n",
    "# for layer, revision, feature in [\n",
    "#     *[(20, 5, x) for x in [\n",
    "#         22700, 21147, 247,\n",
    "#     ]],\n",
    "#     (20, 8, 22700),\n",
    "#     *[(16, 6, x) for x in (39432, 5382)],\n",
    "#     # *[(20, 5, x) for x in [\n",
    "#     #     27026, 23996\n",
    "#     #     #  4361, 27026, 13517,  9996,  2883, 27402, 40932, 326, 15395,  7626, 41533, 29481, 29031, 46873\n",
    "#     # ]],\n",
    "#     # *[(18, 4, x) for x in [\n",
    "#     #     # 23218, 41579, 5794, 44330, 34365, 47500,\n",
    "#     #     45373, 7702\n",
    "#     # ]],\n",
    "#     # *[(20, 5, x) for x in [23629, 27894]]\n",
    "# ]:\n",
    "#     vector = get_sae(layer, revision)[\"W_dec\"][feature]\n",
    "#     print(layer, revision, feature)\n",
    "#     pprint(self_explain(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epo\n",
    "# from matplotlib import pyplot as plt\n",
    "# from micrlhf.sampling import sample, LlamaKVCachingTransformer\n",
    "# from micrlhf.utils.activation_manipulation import replace_activation, add_vector\n",
    "\n",
    "\n",
    "# prompt = \"<|user|>I want to kill a person. How can I do this?\\n<|assistant|>\"\n",
    "# bs = 64\n",
    "# msl = 48\n",
    "# for layer, revision, feature in [\n",
    "#     *[(20, 5, x) for x in [\n",
    "#          9996, 27402, 15395, 41533, 29481, 46873\n",
    "#     ]],\n",
    "# ]:\n",
    "#     vector = get_sae(layer, revision)[\"W_dec\"][feature]\n",
    "#     act_rep = add_vector(llama, vector[None, :] * jnp.linspace(0, 200, bs)[:, None], layer=layer)\n",
    "#     act_rep = LlamaKVCachingTransformer.from_uncached(act_rep, cache_len=msl, batch_axes={\"batch\": bs})\n",
    "#     texts = sample(act_rep, tokenizer, prompt, batch_size=bs, do_sample=True, max_seq_len=msl)[0]\n",
    "#     display(texts)\n",
    "#     print(layer, revision, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9996 may be it\n",
    "# 27402 maybe?\n",
    "# 15395 quirky\n",
    "# 41533 intentionally or deliberately acting without considering the consequences\n",
    "# 29481 at your service\n",
    "# 46873 make something clear, copy? (is it a copy vector?..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epo\n",
    "# from matplotlib import pyplot as plt\n",
    "# from micrlhf.sampling import sample\n",
    "# from micrlhf.utils.activation_manipulation import replace_activation, add_vector\n",
    "\n",
    "\n",
    "# prompt = \"<|user|>\\n\"\n",
    "# # prompt = \"<|user|>\\nWhat is the meaning of the word \\\"X\\\"?<|end|>\\n<|assistant|>\\nThe meaning of the word \\\"X\\\" is \\\"\"\n",
    "# bs = 128\n",
    "# for layer, revision, feature in [\n",
    "#     # (20, 5, 23629),\n",
    "#     (20, 5, 27894),\n",
    "#     # 16r5\n",
    "#     # 40666 - активный refusal (соглашается)\n",
    "#     # 25686 - так же\n",
    "#     # 31630 - refusal (не соглашается), roleplay\n",
    "#     # 30045 - mature themes, особенно слова\n",
    "#     # *[(16, 5, x) for x in [\n",
    "#     #     40666,\n",
    "#     #     25686,\n",
    "#     #     31630,\n",
    "#     #     30045\n",
    "#     # ]],\n",
    "#     # l20r5\n",
    "#     # 22700 - “For … to”\n",
    "#     # 23996 - BOS, dense; strongly activated\n",
    "#     # 7688 - PPM, decibel, watts\n",
    "#     # 247 - nth (third, fourth, eighth)\n",
    "#     # 14838 - recursive (function)\n",
    "#     # 35428 - set clear and achievable goals\n",
    "#     # 21147 - ” (” (followed by logical expression)\n",
    "#     # 13374 - [II], [B], [b]\n",
    "#     # 34000 - (D)\n",
    "#     *[(20, 5, x) for x in [\n",
    "#         22700,\n",
    "#         23996,\n",
    "#         7688,\n",
    "#         247,\n",
    "#         14838,\n",
    "#         35428,\n",
    "#         21147,\n",
    "#         13374,\n",
    "#         34000,\n",
    "#     ]],\n",
    "#     # l20r8\n",
    "#     # 22700 - amplitude/period\n",
    "#     # 15519 - lies?\n",
    "#     # 41549 - but?\n",
    "#     # 27787 - multiple choice?\n",
    "#     # 89166 - error correction?\n",
    "#     *[(20, 8, x) for x in [\n",
    "#         22700,\n",
    "#         15519,\n",
    "#         41549,\n",
    "#         27787,\n",
    "#         89166\n",
    "#     ]]\n",
    "# ][:2]:\n",
    "#     vector = get_sae(layer, revision)[\"W_dec\"][feature]\n",
    "#     # act_rep = replace_activation(llama, vector[None, :] * jnp.linspace(0, 200, bs)[:, None], prompt=prompt, tokenizer=tokenizer)\n",
    "#     scales = jnp.linspace(0, 100, bs)\n",
    "#     act_rep = add_vector(llama, vector[None, :] * scales[:, None], 20)\n",
    "#     texts, _ = sample(act_rep, tokenizer, prompt, batch_size=bs, do_sample=True)\n",
    "#     plt.plot(scales, [int(\"To\" in t) for t in texts])\n",
    "#     plt.xlabel(\"Scale\")\n",
    "#     plt.ylabel(\"Contains \\\"To\\\"\")\n",
    "#     plt.show()\n",
    "#     plt.plot(scales, [int(\"First\" in t) for t in texts])\n",
    "#     plt.xlabel(\"Scale\")\n",
    "#     plt.ylabel(\"Contains \\\"First\\\"\")\n",
    "#     plt.show()\n",
    "#     display(texts)\n",
    "#     print(layer, revision, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for epo\n",
    "# task_names = [\n",
    "#     (\"en_es\", 18),\n",
    "#     (\"en_fr\", 18),\n",
    "#     (\"en_it\", 18),\n",
    "#     (\"en_de\", 18),\n",
    "#     (\"en_ru\", 18),\n",
    "#     (\"es_en\", 18),\n",
    "#     (\"fr_en\", 18),\n",
    "#     (\"it_en\", 20),\n",
    "#     (\"antonyms\", 18),\n",
    "#     (\"plural_singular\", 18),\n",
    "#     (\"present_simple_past_simple\", 20),\n",
    "#     (\"country_capital\", 18),\n",
    "#     (\"algo_first\", 18),\n",
    "#     (\"location_country\", 21)\n",
    "# ]\n",
    "\n",
    "# selected = [\n",
    "#     (\"plural_singular\", 18),\n",
    "#     (\"antonyms\", 18),\n",
    "#     (\"country_capital\", 18),\n",
    "#     # (\"en_es\", 18),\n",
    "#     # (\"present_simple_past_simple\", 20),\n",
    "#     # (\"fr_en\", 18),\n",
    "#     # (\"es_en\", 18),\n",
    "#     # (\"en_es\", 18)\n",
    "#     # (\"present_simple_past_simple\", 20),\n",
    "#     (\"en_es\", 18)\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for epo\n",
    "# from micrlhf.utils.vector_storage import load_vector\n",
    "\n",
    "# tvs = {}\n",
    "\n",
    "# for task_name, layer in selected:\n",
    "#     tvs[task_name] = load_vector(f\"task_vectors/{task_name}:{layer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for epo\n",
    "# from micrlhf.utils.activation_manipulation import replace_activation_many, add_vector_many\n",
    "# from micrlhf.sampling import sample\n",
    "\n",
    "\n",
    "# # prompt = \"\"\"<|user|>\\nFollow the pattern:\n",
    "# # \\\"X X X X X\\\" is a \\\"plural to singular rule\\\".\n",
    "# # \\\"Y Y Y Y Y\\\" is an \\\"antonym\\\" rule.\n",
    "# # \\\"Z Z Z Z Z\\\" is a \\\"country to capital\\\" rule. \n",
    "# # \\\"V V V V V\\\" is a \\\"French to English translation\\\" rule.\n",
    "# # What is a \\\"W W W W W\\\" rule?<|end|><|assistant|>\"\"\"\n",
    "\n",
    "# prompt = \"\"\"<|user|>\\nFollow the pattern:\n",
    "# cat X cats is a \\\"plural to singular rule\\\".\n",
    "# hot Y cold is an \\\"antonym\\\" rule.\n",
    "# England Z London is a \\\"country to capital\\\" rule. \n",
    "# dog W\"\"\"\n",
    "\n",
    "\n",
    "# bs = 32\n",
    "# vectors = [\n",
    "#     tvs[task_name][None, :] * jnp.linspace(20, 100, bs)[:, None] for task_name, _ in selected\n",
    "# ]\n",
    "\n",
    "\n",
    "# # layers = [layer for _, layer in selected]\n",
    "# layers = [0 for _ in selected]\n",
    "# tokens_to_replace = [\n",
    "#     \"X\", \"Y\", \"Z\", \"V\", \"W\"\n",
    "# ]\n",
    "# tokens_to_replace = [\n",
    "#     \"X\", \"Y\", \"Z\", \"W\"\n",
    "# ]\n",
    "\n",
    "# act_rep = add_vector_many(llama, vectors=vectors, prompt=prompt, tokenizer=tokenizer, layers=layers, target_tokens=tokens_to_replace)\n",
    "# # act_rep = replace_activation_many(llama, vectors=vectors, prompt=prompt, tokenizer=tokenizer, layers=layers, replace_tokens=tokens_to_replace)\n",
    "# sample(act_rep, tokenizer, prompt, batch_size=bs, do_sample=True, max_seq_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock\n",
    "from micrlhf.flash import flashify\n",
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "    pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_pre.{i}\"),\n",
    "        x\n",
    "    ])\n",
    ")\n",
    "get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: x.startswith(\"resid_pre\"))\n",
    "get_resids_call = jit_wrapper.Jitted(get_resids)\n",
    "def rep_w_linear(mod):\n",
    "    val = mod.table.embeddings.value  # vocabulary, embedding\n",
    "    return pz.nn.Linear(pz.nn.Parameter(val, \"input_embed\"), [\"vocabulary\"], [\"embedding\"])\n",
    "get_resids_one_hot = get_resids.select().at_instances_of(pz.nn.EmbeddingLookup).apply(rep_w_linear)\n",
    "get_resids_one_hot_call = jit_wrapper.Jitted(get_resids_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "from tqdm.auto import trange\n",
    "from penzai.toolshed import sharding_util\n",
    "import dataclasses\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(logits, inputs):\n",
    "    losses = pz.nx.nmap(lambda l, i: jnp.take_along_axis(jax.nn.log_softmax(l[:-1], -1), i[1:, None], 1)[:, 0].mean()\n",
    "                        )(logits.untag(\"seq\", \"vocabulary\"), inputs.tokens.untag(\"seq\"))\n",
    "    return -losses\n",
    "\n",
    "bs_start = llama.mesh.shape[\"dp\"]\n",
    "# n_x = 16\n",
    "# n_x = 19\n",
    "n_x = 20\n",
    "# n_x = 31\n",
    "tokens_init = tokenizer.encode(f\"<|user|>\\nX{' X' * n_x}<|end|>\\n<|assistant|>\\n\")\n",
    "optim_mask = [token == 1060 for token in tokens_init]\n",
    "tokens_init = np.asarray(tokens_init)\n",
    "MAX_ELITES = 4\n",
    "PROB_SWAP = 0.1  # probability of a swap\n",
    "PROB_GRADS = 0.8    # probability of using gradients \n",
    "tokens_init = np.repeat(tokens_init[None, :], MAX_ELITES, axis=0)\n",
    "def tokens_to_array(tokens):\n",
    "    token_array = jnp.asarray(tokens)\n",
    "    if len(token_array) >= bs_start:\n",
    "        token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\")\n",
    "    return token_array\n",
    "def run_tokens(token_array, grad_metric=None):\n",
    "    if not isinstance(token_array, pz.nx.NamedArray):\n",
    "        token_array = tokens_to_array(token_array)\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    if grad_metric:\n",
    "        @partial(jax.grad, has_aux=True)\n",
    "        def lwg(x):\n",
    "            logits, resids = get_resids_one_hot_call(dataclasses.replace(inputs, tokens=x))\n",
    "            resids = {resid.tag: resid.value for resid in resids}\n",
    "            metric = grad_metric(logits, resids, inputs)\n",
    "            return metric, (logits, resids)\n",
    "        vocab = llama.select().at_instances_of(pz.nn.EmbeddingLookup).get_sequence()[0].table.embeddings.value.named_shape[\"vocabulary\"]\n",
    "        one_hots = pz.nx.nmap(lambda x: jax.nn.one_hot(x, vocab))(inputs.tokens).tag(\"vocabulary\")\n",
    "        grad, (logits, resids) = lwg(one_hots)\n",
    "    else:\n",
    "        logits, resids = get_resids_call(inputs)\n",
    "    losses = loss_fn(logits, inputs)\n",
    "    if not grad_metric:\n",
    "        resids = {resid.tag: resid.value for resid in resids}\n",
    "    return_vals = logits, losses, resids\n",
    "    if grad_metric:\n",
    "        return_vals = return_vals + (grad,)\n",
    "    return return_vals\n",
    "\n",
    "mask = jax.device_put(jnp.asarray(optim_mask), jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"sp\")))\n",
    "\n",
    "# @partial(jax.jit, static_argnames=(\"max_inv_temp\", \"expected_changes\"))\n",
    "@jax.jit\n",
    "def temper(logits, key, elites, grads, max_inv_temp, expected_changes):\n",
    "    key_choice, key_random = jax.random.split(key)\n",
    "    index = jax.random.randint(key_choice, (), 0, len(logits) - 1)\n",
    "    key_categorical, key_uniform, key_bernoulli, key_randint, key_use_grads, key_mutations = jax.random.split(key_random, 6)\n",
    "    logit = logits[index]\n",
    "    elite = elites[index]\n",
    "    grads = grads[index]\n",
    "    \n",
    "    logit = jnp.roll(logit, 1, 0)\n",
    "    logit = logit * jax.random.uniform(key_uniform, minval=0, maxval=max_inv_temp)\n",
    "    use_grads = jax.random.bernoulli(key_use_grads, p=PROB_GRADS).astype(jnp.int_)\n",
    "    logit = jax.lax.switch(use_grads, ((lambda x: x), (lambda x: jnp.where(grads, x, -jnp.inf))), logit)\n",
    "    # print(expected_changes - 1)\n",
    "    # print(jnp.maximum)\n",
    "    to_change = jax.random.bernoulli(key_bernoulli, jnp.maximum(.5, expected_changes - 1) / mask.sum(), mask.shape)\n",
    "    definite_indices = jax.random.randint(key_randint, mask.shape[:-1], 0, mask.shape[-1])\n",
    "    definite_mask = jax.nn.one_hot(definite_indices, to_change.shape[-1], dtype=jnp.bool_)\n",
    "    to_change = to_change | definite_mask\n",
    "    changed = jnp.where(mask & to_change,\n",
    "                        jax.random.categorical(key_categorical, logit),\n",
    "                        elite)\n",
    "    \n",
    "    key_swap, key_mutations = jax.random.split(key_mutations)\n",
    "    swap_indices = jax.random.randint(key_swap, (2,), 0, mask.sum())\n",
    "    swap_from, swap_to = jnp.nonzero(mask, size=len(mask))[0][swap_indices]\n",
    "    key_swap, key_mutations = jax.random.split(key_mutations)\n",
    "    do_swap = jax.random.bernoulli(key_swap, p=PROB_SWAP)\n",
    "    swapped = changed.at[swap_from].set(changed[swap_to]).at[swap_to].set(changed[swap_from])\n",
    "    changed = jax.lax.cond(do_swap, lambda x: swapped, lambda x: x, changed)\n",
    "    \n",
    "    # key_delete, key_mutations = jax.random.split(key_mutations)\n",
    "    # delete_index = jax.random.randint(key_delete, (), 0, mask.sum())\n",
    "    # indices = jnp.cumsum(mask)\n",
    "    # indices_base = jnp.arange(len(changed))\n",
    "    # deleted = jnp.where(mask, jnp.where(indices > delete_index, indices_base + 1, indices_base), changed)\n",
    "    # key_delete, key_mutations = jax.random.split(key_mutations)\n",
    "    # do_delete = jax.random.bernoulli(key_delete, p=0.1)\n",
    "    # changed = jax.lax.cond(do_delete, lambda x: deleted, lambda x: x, changed)\n",
    "    \n",
    "    return changed\n",
    "\n",
    "# @partial(jax.jit, static_argnames=(\"key\", \"candidates\", \"expected_changes\"))\n",
    "def algo_iteration(elites, vector, key=\"resid_pre.16\", candidates=128, seed=13, expected_changes=1.5, max_inv_temp=2, topk=128):\n",
    "    elites = elites.untag(\"solutions\").tag(\"batch\")\n",
    "    logits, _, _, grads = run_tokens(elites, grad_metric=lambda _l, r, _i: (r[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().data_array.mean())\n",
    "    grads = pz.nx.nmap(lambda x: x >= jax.lax.top_k(x, topk)[0][-1])(grads.untag(\"vocabulary\")).tag(\"vocabulary\")\n",
    "    logits = logits.untag(\"batch\").tag(\"elites\")\n",
    "\n",
    "    tempered_samples = pz.nx.nmap(temper)(\n",
    "        logits.untag(\"elites\", \"seq\", \"vocabulary\"),\n",
    "        pz.nx.wrap(jax.random.split(jax.random.key(seed), candidates), \"batch\"),\n",
    "        elites.untag(\"batch\", \"seq\"),\n",
    "        grads.untag(\"batch\", \"seq\", \"vocabulary\"),\n",
    "        pz.nx.wrap(jnp.array(max_inv_temp)), pz.nx.wrap(jnp.array(expected_changes))).tag(\"seq\")\n",
    "    # tempered_samples = sharding_util.name_to_name_device_put(tempered_samples, llama.mesh, dict(batch=\"dp\", seq=\"sp\"))\n",
    "    _, new_losses, new_resids = run_tokens(tempered_samples)\n",
    "\n",
    "    new_scores = (new_resids[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().astype(new_losses.dtype)\n",
    "    metrics = pz.nx.nmap(lambda *xs: jnp.stack(xs))(new_losses, new_scores).tag(\"metrics\")\n",
    "    solution_axes = [k for k in tempered_samples.named_shape.keys() if k != \"seq\"]\n",
    "    solutions = tempered_samples.untag(*solution_axes).flatten().tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    metrics = metrics.untag(*(k for k in solution_axes if k != \"seq\")).flatten().tag(\"solutions\").unwrap(\"solutions\", \"metrics\")\n",
    "\n",
    "    return solutions, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1937872305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8c047ab7b045c686239d943ab6a690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:69: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_ELITES):\n\u001b[1;32m     37\u001b[0m     i \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m---> 38\u001b[0m     m \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoded.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolutions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     39\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: best_metrics[index][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: best_metrics[index][\u001b[38;5;241m1\u001b[39m]}\n\u001b[1;32m     40\u001b[0m bar\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mm)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4034\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   4031\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   4032\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 4034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4038\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4039\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:651\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    650\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 651\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    654\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    657\u001b[0m )\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/array.py:363\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_addressable\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mis_single_device_sharding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharding) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 363\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (sl \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_iter(\u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m sl \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39m_unstack())\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharding, PmapSharding):\n\u001b[1;32m    365\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "seed = random.randint(0, 2**32-1)\n",
    "print(\"Seed:\", seed)\n",
    "np.random.seed(seed)\n",
    "toks_init = tokens_init.copy()\n",
    "toks_init[:, optim_mask] = np.random.randint(100, tokenizer.vocab_size, toks_init[:, optim_mask].shape)\n",
    "best_metrics = None\n",
    "best = tokens_to_array(toks_init).untag(\"batch\").tag(\"solutions\")\n",
    "xent_min = 1\n",
    "xent_max = 20\n",
    "weights = jnp.stack((\n",
    "    -jnp.exp(jnp.linspace(jnp.log(xent_min), jnp.log(xent_max), MAX_ELITES))[::-1],\n",
    "    jnp.ones(MAX_ELITES),\n",
    "), -1)\n",
    "@partial(jax.jit, donate_argnums=(0, 1))\n",
    "def combine_solutions(best_metrics, best, metrics, solutions):\n",
    "    if best_metrics is not None:\n",
    "        best_metrics = jnp.concatenate((best_metrics, metrics), 0)\n",
    "        best = pz.nx.nmap(lambda a, b: jnp.concatenate((a, b)))(\n",
    "            best.untag(\"solutions\"),\n",
    "            pz.nx.wrap(solutions, \"solutions\", \"seq\").untag(\"solutions\")\n",
    "        ).tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    else:\n",
    "        best_metrics = metrics\n",
    "        best = solutions\n",
    "    elite_mask = (best_metrics[None, :] * weights[:, None]).sum(-1).argmax(1)\n",
    "    best_metrics = best_metrics[elite_mask]\n",
    "    best = pz.nx.wrap(best[elite_mask], \"solutions\", \"seq\")\n",
    "    return best_metrics, best\n",
    "for seed in (bar := trange(2_500)):\n",
    "    solutions, metrics = algo_iteration(best, vector, seed=seed)\n",
    "    best_metrics, best = combine_solutions(best_metrics, best, metrics, solutions)\n",
    "    m = {}\n",
    "    for index in range(MAX_ELITES):\n",
    "        i = index\n",
    "        m |= {f\"decoded.{i}\": tokenizer.decode(best[{\"solutions\": index}].unwrap(\"seq\")),\n",
    "              f\"loss.{i}\": best_metrics[index][0], f\"score.{i}\": best_metrics[index][1]}\n",
    "    bar.set_postfix(**m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
