{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\", from_type=\"gemma\", load_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = \"data/gemma-2b-explanations.json\"\n",
    "if os.path.exists(save_path):\n",
    "    with open(save_path, \"r\") as f:\n",
    "        index_explanations = {int(k): v for k, v in json.load(f).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ba711a9d234a049e97593340265419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "import requests\n",
    "max_neurons = 40_000\n",
    "try:\n",
    "    index_explanations\n",
    "except NameError:\n",
    "    index_explanations = {}\n",
    "step = 25\n",
    "for offset in trange(max(index_explanations) // step, max_neurons, step):\n",
    "    gemma_neurons = requests.post(\"https://www.neuronpedia.org/api/neurons-offset\", json={\"modelId\": \"gemma-2b\", \"layer\": \"6-res-jb\", \"offset\": offset}).json()\n",
    "    for n in gemma_neurons:\n",
    "        for e in n[\"explanations\"]:\n",
    "            index_explanations[int(n[\"index\"])] = dict(\n",
    "                explanation=e[\"description\"],\n",
    "                max_acts=[dict(\n",
    "                    tokens=a[\"tokens\"],\n",
    "                    values=a[\"values\"],\n",
    "                ) for a in n[\"activations\"]]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"output_0da48869e4f149efaefd0724760466ba\"><script> /* penzai.treescope rendering of a Python object (compressed) */ (()=>{ let observer; let lastStep = new Promise((resolve, reject) => { observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); }); window.treescope_decompress_enqueue = (encoded, destId) => { const previous = lastStep; const destElt = document.getElementById(destId); lastStep = (async () => { await previous; let blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); let reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); let parts = []; while (true) { let step = await reader.read(); if (step.done) { break; } parts.push(step.value); } let newElt = document.createElement(\"div\"); newElt.innerHTML = parts.join(\"\"); destElt.parentNode.replaceChild(newElt, destElt); for (let oldScript of newElt.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } })(); requestAnimationFrame(() => { observer.observe(destElt); }); } })(); </script><div id=\"compress_html_d5035dfd125c441b99577b8d1aa5e9a3\"><script>window.treescope_decompress_enqueue(\"eNqVVUuP2zYQ/isTBQjkINI+ggSpbRlYpC16CHpJgR6KQKClkUWYIlVytI4b+L93SFpeeddptvBBnuE8vnkvHe0VrnKyiK4yPZbWGIJv0BsnSRo9B4tKkLzHBTRGU9aITqr9HDqjjetFxfxdKwmzQMyht8xR0lEWTGe075mrjWb2WlTbjTWDrrPKKGPnUXUBR2qtWIDtyZraOTSSWEwTalpAJ+xG6kxhQ3O4rVrvQ2PWoty0zLnJ3y3gsLyK4SxdZWVPK5ANpDupa7N7iBCKogCGgA0bqGcc61MJ+HZYPGHnZY+6lnpzV/nMOBb768sPxX4TulbepB6UuiC9QSofZb+AdEx6SWYGxYpBXr2GT8ZsuQgWqEUIktrUmMPrK1BIcNSdqIbSsPM0vr0ooDbV0HFC87Wp9/DqFbzwL3mlhHOfuGi5T7iQ2qXJOahkBj5Vow+v1AvLln5R2IUKHdgzDVaHRyYvxGoH/dkY7QPcGbs9RsYuHTGyP5nln87YJCvP7NFy4J3QFeba7NLZKbQnL5BFpSW8vfWQL/XA41LmCvWGWm4NuL7UEf9d0xi3zwAqhw/Q20F77D907lrZkA8paPg/B/49F0M6Slr8e0BHd1p2wkv8akWHaczrbHa5Io+h9INrY20Wz0ncCKKIqfg/qXs+7IvAUbPWgD9jg9Zi/Qd2Pbc9uqcN1CvPPDU+q9n9Z1RYkbF3SnGjH3WnS3DQpeNOTWaLMHCpny+2BKYJBn2g/J3MjcXO3OPZ1ExsfG8S0hGrt99Y8w/64QiW4+I7h5smcbF5k16DqWlolUUO4ziQU1n+l/s9zMLRSaCOfPxKH6OzyfMDcxHgxFn/3a8bi5ytCj/y+NUpW3jjBXydvlsrMpuNimukDNufGFjYcsxBRW8A79n6MRW+7QKdb3HvJzKxybh6WHiS82g3TU42y44BJiOOA/A9iIdgWct7CIrF470GJNYMGL8WyXUCRrNTRq8ncpfxp9RKNyJP+OTEQxouWamHbo029GA4bC+vf/pw8+792YnqhR4htVwNy/dsWxKfOWRQtSCRMZReUFskybn41Eeyunn/9sMtm2WB1enD8Z6u4LNnJ52dEvYvIzbmbw==\", \"compress_html_d5035dfd125c441b99577b8d1aa5e9a3\");</script><span style=\"color: #aaaaaa; font-family: monospace\">(Loading...)</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"output_dest_0da48869e4f149efaefd0724760466ba\"><script> (()=>{ const output = document.getElementById(\"output_0da48869e4f149efaefd0724760466ba\"); const dest = document.getElementById(\"output_dest_0da48869e4f149efaefd0724760466ba\"); dest.parentNode.replaceChild(output, dest); })(); </script></div>"
      ],
      "text/plain": [
       "16382"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(index_explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(index_explanations, f)\n",
    "else:\n",
    "    existing = {int(k): v for k, v in json.load(open(save_path)).items()}\n",
    "    existing.update(index_explanations)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(existing, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/sae\n",
    "!wget -c 'https://huggingface.co/jbloom/Gemma-2b-Residual-Stream-SAEs/resolve/main/gemma_2b_blocks.6.hook_resid_post_16384_anthropic_fast_lr/sae_weights.safetensors?download=true' -O 'models/sae/gemma-jb-6.safetensors'\n",
    "from safetensors import safe_open\n",
    "with safe_open(\"models/sae/gemma-jb-6.safetensors\", framework=\"numpy\") as st:\n",
    "    w_dec = st.get_tensor(\"W_dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.activation_manipulation import ActivationReplacement\n",
    "from micrlhf.utils.activation_manipulation import replace_activation, collect_activations\n",
    "from micrlhf.sampling import sample\n",
    "from penzai.toolshed.jit_wrapper import Jitted\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "FEAT_BATCH = 1\n",
    "PICK_BATCH = 64\n",
    "MIN_SCALE = 0\n",
    "MAX_SCALE = 200\n",
    "REP_LAYER = 2\n",
    "MAX_LENGTH = 64\n",
    "PROBE_LAYER = 16\n",
    "CFG = 1.0\n",
    "PROMPT_TEMPLATE = '<start_of_turn>user\\nWhat is the meaning of the word \"X\"?<end_of_turn>\\n<start_of_turn>model\\nThe meaning of the word \"X\" is \"'\n",
    "POSITIONS = tuple(i for i, a in enumerate(tokenizer.encode(PROMPT_TEMPLATE)) if tokenizer.decode([a]) == \"X\")\n",
    "\n",
    "embeds = llama.select().at_instances_of(pz.nn.EmbeddingLookup).get_sequence()[0].table.embeddings.value.unwrap(\"vocabulary\", \"embedding\")\n",
    "embed_mean = embeds.mean(axis=0)\n",
    "embed_vector = embed_mean / jnp.linalg.norm(embed_mean)\n",
    "tiled_embed = jnp.tile(embed_vector, (PICK_BATCH, 1))\n",
    "act_rep_base = Jitted(collect_activations(replace_activation(llama, tiled_embed, POSITIONS, layer=REP_LAYER)))\n",
    "\n",
    "def benchmark_vector(vector, tokens, positions, replacement_layer):\n",
    "    assert replacement_layer == REP_LAYER\n",
    "    dumb = False\n",
    "    if vector.ndim == 1:\n",
    "        dumb = True\n",
    "        vector = jnp.tile(vector[None, :], (PICK_BATCH, 1))\n",
    "    assert vector.shape == tiled_embed.shape\n",
    "    assert positions == POSITIONS\n",
    "    # act_rep = collect_activations(replace_activation(llama, vector, positions, layer=replacement_layer))\n",
    "    act_rep = act_rep_base.select().at_instances_of(ActivationReplacement).apply(lambda x: ActivationReplacement.replace_vector(x, vector))\n",
    "    logits, residuals = act_rep(tokens)\n",
    "    result = logits, [r.value for r in residuals]\n",
    "    if dumb:\n",
    "        return result[0].untag(\"batch\")[:1].tag(\"batch\"), [r.untag(\"batch\")[:1].tag(\"batch\") for r in result[1]]\n",
    "    return result\n",
    "\n",
    "\n",
    "def tokens_to_inputs(tokens):\n",
    "    token_array = jnp.asarray(tokens)\n",
    "    token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\").untag(\"batch\").tag(\"batch\")\n",
    "\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# def logits_to_loss(logits, tokens, answer_start, pad_token=32000):\n",
    "#     logits = jax.nn.log_softmax(logits)\n",
    "\n",
    "#     logits = logits[:, :-1]\n",
    "#     logits = jnp.take_along_axis(logits, tokens[:, 1:, None], axis=-1).squeeze(-1)\n",
    "\n",
    "#     mask = tokens[:, 1:] != pad_token\n",
    "\n",
    "#     mask[:, :answer_start-1] = False\n",
    "\n",
    "#     logits = logits * mask\n",
    "\n",
    "#     return -logits.sum(axis=-1) / mask.sum(axis=-1)\n",
    "\n",
    "\n",
    "def pick_scale(feature, batch_size=PICK_BATCH, min_scale=MIN_SCALE, max_scale=MAX_SCALE, layer=REP_LAYER):\n",
    "    scales = np.linspace(min_scale, max_scale, batch_size)\n",
    "    vector = feature[None, :] * jnp.array(scales)[:, None]\n",
    "    text = [PROMPT_TEMPLATE for _ in range(batch_size)]\n",
    "    tokenized = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", max_length=64, truncation=True)\n",
    "    tokens = tokenized[\"input_ids\"]\n",
    "    inputs = tokens_to_inputs(tokens)\n",
    "    \n",
    "    logits, residuals = benchmark_vector(vector, inputs, POSITIONS, layer)\n",
    "    \n",
    "    # rand_vector = jax.random.normal(key=jax.random.key(random.randint(-10, 10)), shape=vector.shape)\n",
    "    # rand_vector = rand_vector / jnp.linalg.norm(rand_vector, axis=-1, keepdims=True)\n",
    "    # logits_rand, _ = benchmark_vector(rand_vector, inputs, positions, layer)\n",
    "    \n",
    "    logits_mean, _ = benchmark_vector(embed_vector, inputs, POSITIONS, layer)\n",
    "\n",
    "    logits = logits.unwrap(\"batch\", \"seq\", \"vocabulary\")\n",
    "    entropies = -jnp.sum(jax.nn.log_softmax(logits) * jnp.exp(jax.nn.log_softmax(logits)), axis=-1)\n",
    "    entropy_first = entropies[:, -1]\n",
    "    \n",
    "    resid_cos_features = []\n",
    "    for residual in residuals:\n",
    "        resid = residual.unwrap(\"batch\", \"seq\", \"embedding\")[:, -1]\n",
    "        resid_cos_feature = resid @ feature / jnp.linalg.norm(resid) / jnp.linalg.norm(feature)\n",
    "        resid_cos_feature = resid_cos_feature - resid_cos_feature[0]\n",
    "        resid_cos_features.append(resid_cos_feature)\n",
    "    entropy_first = entropy_first - entropy_first[0]\n",
    "    \n",
    "    crossents = []\n",
    "    # for baseline in (logits_rand, logits_mean):\n",
    "    for baseline in (logits_mean,):\n",
    "        baseline = baseline.unwrap(\"batch\", \"seq\", \"vocabulary\")\n",
    "        # baseline_probs = jax.nn.softmax(baseline)\n",
    "        # crossents.append(-jnp.sum(jax.nn.log_softmax(logits) * baseline_probs, axis=-1)[:, -1])\n",
    "        crossents.append(-jnp.sum(jax.nn.log_softmax(baseline) * jax.nn.softmax(logits), axis=-1)[:, -1])\n",
    "    \n",
    "    return scales, entropy_first, resid_cos_features, crossents\n",
    "\n",
    "def generate_explanations(feature=None, batch_size=32, min_scale=MIN_SCALE, max_scale=MAX_SCALE, layer=REP_LAYER, cfg=CFG, for_cache=False, cached=None, cache_batch=FEAT_BATCH):\n",
    "    if feature is None:\n",
    "        feature = w_dec[0]\n",
    "        if cache_batch > 1:\n",
    "            feature = jnp.tile(feature[None], (cache_batch, 1))\n",
    "    scales = np.linspace(min_scale, max_scale, batch_size, dtype=np.float32)\n",
    "    if feature.ndim == 1:\n",
    "        vector = feature[None, :] * jnp.array(scales)[:, None]\n",
    "    else:\n",
    "        vector = feature[:, None, :] * jnp.array(scales)[None, :, None]\n",
    "        vector = vector.reshape(-1, vector.shape[-1])\n",
    "    if cfg != 1.0:\n",
    "        vector = jnp.concatenate([embed_vector[None, :] * jnp.array(scales)[:, None], vector], axis=0)\n",
    "    if cached is not None:\n",
    "        act_rep = cached[0].select().at_instances_of(ActivationReplacement).apply(lambda x: ActivationReplacement.replace_vector(x, vector)), cached[1]\n",
    "        # act_rep = cached\n",
    "    else:\n",
    "        act_rep = replace_activation(llama, vector, POSITIONS, layer=layer)\n",
    "    completions, model = sample(act_rep, tokenizer,\n",
    "                         PROMPT_TEMPLATE, batch_size=(batch_size if cfg == 1.0 else batch_size * 2) * (1 if feature.ndim == 1 else feature.shape[0]),\n",
    "                         do_sample=True, max_seq_len=MAX_LENGTH,\n",
    "                         return_only_completion=True,\n",
    "                         verbose=False, cfg_strength=cfg,\n",
    "                         return_model=for_cache, only_cache=for_cache)\n",
    "    if for_cache:\n",
    "        return model\n",
    "    if feature.ndim == 1:\n",
    "        return list(zip(np.concatenate((scales, scales)), completions))[batch_size if cfg != 1.0 else 0:]\n",
    "    else:\n",
    "        completions = np.array(completions).reshape(-1, batch_size if cfg == 1.0 else 2 * batch_size)\n",
    "        explanations = []\n",
    "        for c in completions:\n",
    "            explanations.append(list(zip(np.concatenate((scales, scales)), c))[batch_size if cfg != 1.0 else 0:])\n",
    "        return explanations\n",
    "    \n",
    "cached_model = generate_explanations(for_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "generations_filename = \"data/gemma-2b-generations.json\"\n",
    "if os.path.exists(generations_filename):\n",
    "    with open(generations_filename, \"r\") as f:\n",
    "        generations = {int(k): v for k, v in json.load(f).items()}\n",
    "else:\n",
    "    generations = {}\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "showy = False\n",
    "random.seed(9)\n",
    "feat_batch = FEAT_BATCH\n",
    "data_points = []\n",
    "try:\n",
    "    for i, e in tqdm(list(index_explanations.items())):\n",
    "        if i in generations:\n",
    "            continue\n",
    "        scales, entropy, selfsims, crossents = pick_scale(w_dec[i])\n",
    "        selfsim = selfsims[PROBE_LAYER]\n",
    "        scale_idx = np.argmax(selfsim[1:]) - 1\n",
    "        highest = selfsim[scale_idx]\n",
    "        scale = scales[scale_idx]\n",
    "        scale = scales[scale_idx]\n",
    "        data_point = dict(\n",
    "            feature=i,\n",
    "            explanation=e[\"explanation\"],\n",
    "            settings=dict(\n",
    "                min_scale=MIN_SCALE,\n",
    "                max_scale=MAX_SCALE,\n",
    "                rep_layer=REP_LAYER,\n",
    "                probe_layer=PROBE_LAYER,\n",
    "                cfg=CFG,\n",
    "            ),\n",
    "            scale_tuning=dict(\n",
    "                scales=scales.tolist(),\n",
    "                entropy=entropy.tolist(),\n",
    "                selfsims=[s.tolist() for s in selfsims],\n",
    "                crossents=[c.tolist() for c in crossents],\n",
    "            )\n",
    "        )\n",
    "        data_points.append(data_point)\n",
    "        if len(data_points) < feat_batch:\n",
    "            continue\n",
    "        explanations = generate_explanations(jnp.stack([w_dec[p[\"feature\"]] for p in data_points]), cached=cached_model)\n",
    "        for p, e in zip(data_points, explanations):\n",
    "            if showy:\n",
    "                st = p[\"scale_tuning\"]\n",
    "                plt.plot(st[\"scales\"], st[\"entropy\"], label=f\"Entropy\")\n",
    "                for l in range(10, PROBE_LAYER + 1):\n",
    "                    plt.plot(st[\"scales\"], np.array(st[\"selfsims\"][l]) / max(st[\"selfsims\"][l]) * max(st[\"selfsims\"][PROBE_LAYER]), label=f\"Self-similarity [{l}]\")\n",
    "                for j, c in enumerate(crossents):\n",
    "                    plt.plot(st[\"scales\"], c, label=f\"Cross-entropy ({j})\")\n",
    "                plt.title(f\"Feature {p['feature']}: \\\"{p['explanation']}\\\"\")\n",
    "                plt.xlabel(\"Scale\")\n",
    "                # plt.ylim(-10, 10)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "            if showy:\n",
    "                display(e)\n",
    "            data_point = dict(\n",
    "                **p,\n",
    "                generations=[(float(a), b) for a, b in e],\n",
    "            )\n",
    "            generations[p[\"feature\"]] = data_point\n",
    "        \n",
    "        data_points = []\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "json.dump(generations, open(f\"data/gemma-2b-generations.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(generations, open(f\"data/gemma-2b-generations.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
