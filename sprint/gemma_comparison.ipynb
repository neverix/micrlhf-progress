{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\", from_type=\"gemma\", load_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = \"data/gemma-2b-explanations.json\"\n",
    "if os.path.exists(save_path):\n",
    "    with open(save_path, \"r\") as f:\n",
    "        index_explanations = {int(k): v for k, v in json.load(f).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import trange\n",
    "# import requests\n",
    "# max_neurons = 10_000\n",
    "# try:\n",
    "#     index_explanations\n",
    "# except NameError:\n",
    "#     index_explanations = {}\n",
    "# step = 25\n",
    "# for offset in trange(0, max_neurons, step):\n",
    "#     if all(i in index_explanations for i in range(offset, offset + step)):\n",
    "#         continue\n",
    "#     gemma_neurons = requests.post(\"https://www.neuronpedia.org/api/neurons-offset\", json={\"modelId\": \"gemma-2b\", \"layer\": \"6-res-jb\", \"offset\": offset}).json()\n",
    "#     for n in gemma_neurons:\n",
    "#         for e in n[\"explanations\"]:\n",
    "#             index_explanations[int(n[\"index\"])] = e[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(index_explanations, f)\n",
    "else:\n",
    "    existing = {int(k): v for k, v in json.load(open(save_path)).items()}\n",
    "    existing.update(index_explanations)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(existing, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-28 22:01:27--  https://huggingface.co/jbloom/Gemma-2b-Residual-Stream-SAEs/resolve/main/gemma_2b_blocks.6.hook_resid_post_16384_anthropic_fast_lr/sae_weights.safetensors?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 108.156.211.90, 108.156.211.51, 108.156.211.95, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.156.211.90|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/85/1c/851cab7a7e482c3ae3ab106128eb2a9086b354ced93766afbe639bb954bf89f8/16696a71f8b197c814cb959eb0ecf5ac4a7096963f2f6c526367daf95fc71792?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sae_weights.safetensors%3B+filename%3D%22sae_weights.safetensors%22%3B&Expires=1719871287&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTg3MTI4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzg1LzFjLzg1MWNhYjdhN2U0ODJjM2FlM2FiMTA2MTI4ZWIyYTkwODZiMzU0Y2VkOTM3NjZhZmJlNjM5YmI5NTRiZjg5ZjgvMTY2OTZhNzFmOGIxOTdjODE0Y2I5NTllYjBlY2Y1YWM0YTcwOTY5NjNmMmY2YzUyNjM2N2RhZjk1ZmM3MTc5Mj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=iCN%7EHhIuOgSWAf1jhR4thhbm8yBbxUcF-CaV0RZ-GEY85fhIxdr1vnL7zazdzVhZ6GS%7EWp-vsWzyYF63DJtG-MzZcjXdSJJ8hOdFzTRAdzKZwrJz0kyEihQTDYim4VTwBsEe0t51Mxb1kLuT5ZdZkrjGTkIcutHCkZqUfny-q4iO6zJ5WyTRpQvpwh5l-QpyXSdPTl6Tb4j%7Eg5shIhSGRhSSagebjcDg7OhqIdHhxZzZdPsdxkX9YJRnm9g%7Ekt0a6eOnKGaHEydV2FS2ju7X%7EbUWfpbi9nCngUORrc1F34jrz1Xf8aSW5cBeMl70xHxgdCpdu7I1GotfNsNbTfS73g__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2024-06-28 22:01:27--  https://cdn-lfs-us-1.huggingface.co/repos/85/1c/851cab7a7e482c3ae3ab106128eb2a9086b354ced93766afbe639bb954bf89f8/16696a71f8b197c814cb959eb0ecf5ac4a7096963f2f6c526367daf95fc71792?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sae_weights.safetensors%3B+filename%3D%22sae_weights.safetensors%22%3B&Expires=1719871287&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTg3MTI4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzg1LzFjLzg1MWNhYjdhN2U0ODJjM2FlM2FiMTA2MTI4ZWIyYTkwODZiMzU0Y2VkOTM3NjZhZmJlNjM5YmI5NTRiZjg5ZjgvMTY2OTZhNzFmOGIxOTdjODE0Y2I5NTllYjBlY2Y1YWM0YTcwOTY5NjNmMmY2YzUyNjM2N2RhZjk1ZmM3MTc5Mj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=iCN%7EHhIuOgSWAf1jhR4thhbm8yBbxUcF-CaV0RZ-GEY85fhIxdr1vnL7zazdzVhZ6GS%7EWp-vsWzyYF63DJtG-MzZcjXdSJJ8hOdFzTRAdzKZwrJz0kyEihQTDYim4VTwBsEe0t51Mxb1kLuT5ZdZkrjGTkIcutHCkZqUfny-q4iO6zJ5WyTRpQvpwh5l-QpyXSdPTl6Tb4j%7Eg5shIhSGRhSSagebjcDg7OhqIdHhxZzZdPsdxkX9YJRnm9g%7Ekt0a6eOnKGaHEydV2FS2ju7X%7EbUWfpbi9nCngUORrc1F34jrz1Xf8aSW5cBeMl70xHxgdCpdu7I1GotfNsNbTfS73g__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.157.150.21, 108.157.150.122, 108.157.150.29, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.157.150.21|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p models/sae\n",
    "!wget -c 'https://huggingface.co/jbloom/Gemma-2b-Residual-Stream-SAEs/resolve/main/gemma_2b_blocks.6.hook_resid_post_16384_anthropic_fast_lr/sae_weights.safetensors?download=true' -O 'models/sae/gemma-jb-6.safetensors'\n",
    "from safetensors import safe_open\n",
    "with safe_open(\"models/sae/gemma-jb-6.safetensors\", framework=\"numpy\") as st:\n",
    "    w_dec = st.get_tensor(\"W_dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.activation_manipulation import ActivationReplacement\n",
    "from micrlhf.utils.activation_manipulation import replace_activation, collect_activations\n",
    "from micrlhf.sampling import sample\n",
    "from penzai.toolshed.jit_wrapper import Jitted\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "FEAT_BATCH = 1\n",
    "PICK_BATCH = 64\n",
    "MIN_SCALE = 0\n",
    "MAX_SCALE = 200\n",
    "REP_LAYER = 2\n",
    "MAX_LENGTH = 64\n",
    "PROBE_LAYER = 16\n",
    "CFG = 1.0\n",
    "PROMPT_TEMPLATE = '<start_of_turn>user\\nWhat is the meaning of the word \"X\"?<end_of_turn>\\n<start_of_turn>model\\nThe meaning of the word \"X\" is \"'\n",
    "POSITIONS = tuple(i for i, a in enumerate(tokenizer.encode(PROMPT_TEMPLATE)) if tokenizer.decode([a]) == \"X\")\n",
    "\n",
    "embeds = llama.select().at_instances_of(pz.nn.EmbeddingLookup).get_sequence()[0].table.embeddings.value.unwrap(\"vocabulary\", \"embedding\")\n",
    "embed_mean = embeds.mean(axis=0)\n",
    "embed_vector = embed_mean / jnp.linalg.norm(embed_mean)\n",
    "tiled_embed = jnp.tile(embed_vector, (PICK_BATCH, 1))\n",
    "act_rep_base = Jitted(collect_activations(replace_activation(llama, tiled_embed, POSITIONS, layer=REP_LAYER)))\n",
    "\n",
    "def benchmark_vector(vector, tokens, positions, replacement_layer):\n",
    "    assert replacement_layer == REP_LAYER\n",
    "    dumb = False\n",
    "    if vector.ndim == 1:\n",
    "        dumb = True\n",
    "        vector = jnp.tile(vector[None, :], (PICK_BATCH, 1))\n",
    "    assert vector.shape == tiled_embed.shape\n",
    "    assert positions == POSITIONS\n",
    "    # act_rep = collect_activations(replace_activation(llama, vector, positions, layer=replacement_layer))\n",
    "    act_rep = act_rep_base.select().at_instances_of(ActivationReplacement).apply(lambda x: ActivationReplacement.replace_vector(x, vector))\n",
    "    logits, residuals = act_rep(tokens)\n",
    "    result = logits, [r.value for r in residuals]\n",
    "    if dumb:\n",
    "        return result[0].untag(\"batch\")[:1].tag(\"batch\"), [r.untag(\"batch\")[:1].tag(\"batch\") for r in result[1]]\n",
    "    return result\n",
    "\n",
    "\n",
    "def tokens_to_inputs(tokens):\n",
    "    token_array = jnp.asarray(tokens)\n",
    "    token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\").untag(\"batch\").tag(\"batch\")\n",
    "\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# def logits_to_loss(logits, tokens, answer_start, pad_token=32000):\n",
    "#     logits = jax.nn.log_softmax(logits)\n",
    "\n",
    "#     logits = logits[:, :-1]\n",
    "#     logits = jnp.take_along_axis(logits, tokens[:, 1:, None], axis=-1).squeeze(-1)\n",
    "\n",
    "#     mask = tokens[:, 1:] != pad_token\n",
    "\n",
    "#     mask[:, :answer_start-1] = False\n",
    "\n",
    "#     logits = logits * mask\n",
    "\n",
    "#     return -logits.sum(axis=-1) / mask.sum(axis=-1)\n",
    "\n",
    "\n",
    "def pick_scale(feature, batch_size=PICK_BATCH, min_scale=MIN_SCALE, max_scale=MAX_SCALE, layer=REP_LAYER):\n",
    "    scales = np.linspace(min_scale, max_scale, batch_size)\n",
    "    vector = feature[None, :] * jnp.array(scales)[:, None]\n",
    "    text = [PROMPT_TEMPLATE for _ in range(batch_size)]\n",
    "    tokenized = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", max_length=64, truncation=True)\n",
    "    tokens = tokenized[\"input_ids\"]\n",
    "    inputs = tokens_to_inputs(tokens)\n",
    "    \n",
    "    logits, residuals = benchmark_vector(vector, inputs, POSITIONS, layer)\n",
    "    \n",
    "    # rand_vector = jax.random.normal(key=jax.random.key(random.randint(-10, 10)), shape=vector.shape)\n",
    "    # rand_vector = rand_vector / jnp.linalg.norm(rand_vector, axis=-1, keepdims=True)\n",
    "    # logits_rand, _ = benchmark_vector(rand_vector, inputs, positions, layer)\n",
    "    \n",
    "    logits_mean, _ = benchmark_vector(embed_vector, inputs, POSITIONS, layer)\n",
    "\n",
    "    logits = logits.unwrap(\"batch\", \"seq\", \"vocabulary\")\n",
    "    entropies = -jnp.sum(jax.nn.log_softmax(logits) * jnp.exp(jax.nn.log_softmax(logits)), axis=-1)\n",
    "    entropy_first = entropies[:, -1]\n",
    "    \n",
    "    resid_cos_features = []\n",
    "    for residual in residuals:\n",
    "        resid = residual.unwrap(\"batch\", \"seq\", \"embedding\")[:, -1]\n",
    "        resid_cos_feature = resid @ feature / jnp.linalg.norm(resid) / jnp.linalg.norm(feature)\n",
    "        resid_cos_feature = resid_cos_feature - resid_cos_feature[0]\n",
    "        resid_cos_features.append(resid_cos_feature)\n",
    "    entropy_first = entropy_first - entropy_first[0]\n",
    "    \n",
    "    crossents = []\n",
    "    # for baseline in (logits_rand, logits_mean):\n",
    "    for baseline in (logits_mean,):\n",
    "        baseline = baseline.unwrap(\"batch\", \"seq\", \"vocabulary\")\n",
    "        # baseline_probs = jax.nn.softmax(baseline)\n",
    "        # crossents.append(-jnp.sum(jax.nn.log_softmax(logits) * baseline_probs, axis=-1)[:, -1])\n",
    "        crossents.append(-jnp.sum(jax.nn.log_softmax(baseline) * jax.nn.softmax(logits), axis=-1)[:, -1])\n",
    "    \n",
    "    return scales, entropy_first, resid_cos_features, crossents\n",
    "\n",
    "def generate_explanations(feature=None, batch_size=32, min_scale=MIN_SCALE, max_scale=MAX_SCALE, layer=REP_LAYER, cfg=CFG, for_cache=False, cached=None, cache_batch=FEAT_BATCH):\n",
    "    if feature is None:\n",
    "        feature = w_dec[0]\n",
    "        if cache_batch > 1:\n",
    "            feature = jnp.tile(feature[None], (cache_batch, 1))\n",
    "    scales = np.linspace(min_scale, max_scale, batch_size, dtype=np.float32)\n",
    "    if feature.ndim == 1:\n",
    "        vector = feature[None, :] * jnp.array(scales)[:, None]\n",
    "    else:\n",
    "        vector = feature[:, None, :] * jnp.array(scales)[None, :, None]\n",
    "        vector = vector.reshape(-1, vector.shape[-1])\n",
    "    if cfg != 1.0:\n",
    "        vector = jnp.concatenate([embed_vector[None, :] * jnp.array(scales)[:, None], vector], axis=0)\n",
    "    if cached is not None:\n",
    "        act_rep = cached[0].select().at_instances_of(ActivationReplacement).apply(lambda x: ActivationReplacement.replace_vector(x, vector)), cached[1]\n",
    "        # act_rep = cached\n",
    "    else:\n",
    "        act_rep = replace_activation(llama, vector, POSITIONS, layer=layer)\n",
    "    completions, model = sample(act_rep, tokenizer,\n",
    "                         PROMPT_TEMPLATE, batch_size=(batch_size if cfg == 1.0 else batch_size * 2) * (1 if feature.ndim == 1 else feature.shape[0]),\n",
    "                         do_sample=True, max_seq_len=MAX_LENGTH,\n",
    "                         return_only_completion=True,\n",
    "                         verbose=False, cfg_strength=cfg,\n",
    "                         return_model=for_cache, only_cache=for_cache)\n",
    "    if for_cache:\n",
    "        return model\n",
    "    if feature.ndim == 1:\n",
    "        return list(zip(np.concatenate((scales, scales)), completions))[batch_size if cfg != 1.0 else 0:]\n",
    "    else:\n",
    "        completions = np.array(completions).reshape(-1, batch_size if cfg == 1.0 else 2 * batch_size)\n",
    "        explanations = []\n",
    "        for c in completions:\n",
    "            explanations.append(list(zip(np.concatenate((scales, scales)), c))[batch_size if cfg != 1.0 else 0:])\n",
    "        return explanations\n",
    "    \n",
    "cached_model = generate_explanations(for_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d5330336884d4d940fa2ba40477d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "generations_filename = \"data/gemma-2b-generations.json\"\n",
    "if os.path.exists(generations_filename):\n",
    "    with open(generations_filename, \"r\") as f:\n",
    "        generations = json.load(f)\n",
    "else:\n",
    "    generations = {}\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "showy = False\n",
    "random.seed(9)\n",
    "feat_batch = FEAT_BATCH\n",
    "data_points = []\n",
    "for i, e in tqdm(list(index_explanations.items())):\n",
    "    if e in generations:\n",
    "        continue\n",
    "    scales, entropy, selfsims, crossents = pick_scale(w_dec[i])\n",
    "    selfsim = selfsims[PROBE_LAYER]\n",
    "    scale_idx = np.argmax(selfsim[1:]) - 1\n",
    "    highest = selfsim[scale_idx]\n",
    "    scale = scales[scale_idx]\n",
    "    scale = scales[scale_idx]\n",
    "    data_point = dict(\n",
    "        feature=i,\n",
    "        explanation=e,\n",
    "        settings=dict(\n",
    "            min_scale=MIN_SCALE,\n",
    "            max_scale=MAX_SCALE,\n",
    "            rep_layer=REP_LAYER,\n",
    "            probe_layer=PROBE_LAYER,\n",
    "            cfg=CFG,\n",
    "        ),\n",
    "        scale_tuning=dict(\n",
    "            scales=scales.tolist(),\n",
    "            entropy=entropy.tolist(),\n",
    "            selfsims=[s.tolist() for s in selfsims],\n",
    "            crossents=[c.tolist() for c in crossents],\n",
    "        )\n",
    "    )\n",
    "    data_points.append(data_point)\n",
    "    if len(data_points) < feat_batch:\n",
    "        continue\n",
    "    explanations = generate_explanations(jnp.stack([w_dec[p[\"feature\"]] for p in data_points]), cached=cached_model)\n",
    "    for p, e in zip(data_points, explanations):\n",
    "        if showy:\n",
    "            st = p[\"scale_tuning\"]\n",
    "            plt.plot(st[\"scales\"], st[\"entropy\"], label=f\"Entropy\")\n",
    "            for l in range(10, PROBE_LAYER + 1):\n",
    "                plt.plot(st[\"scales\"], np.array(st[\"selfsims\"][l]) / max(st[\"selfsims\"][l]) * max(st[\"selfsims\"][PROBE_LAYER]), label=f\"Self-similarity [{l}]\")\n",
    "            for j, c in enumerate(crossents):\n",
    "                plt.plot(st[\"scales\"], c, label=f\"Cross-entropy ({j})\")\n",
    "            plt.title(f\"Feature {p['feature']}: \\\"{p['explanation']}\\\"\")\n",
    "            plt.xlabel(\"Scale\")\n",
    "            # plt.ylim(-10, 10)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        if showy:\n",
    "            display(e)\n",
    "        data_point = dict(\n",
    "            **p,\n",
    "            generations=[(float(a), b) for a, b in e],\n",
    "        )\n",
    "        generations[p[\"feature\"]] = data_point\n",
    "    \n",
    "    json.dump(generations, open(f\"data/gemma-2b-generations.json\", \"w\"))\n",
    "    data_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
