{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"../..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from penzai import pz\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import random\n",
    "from penzai.data_effects.side_output import SideOutputValue\n",
    "from micrlhf.utils.activation_manipulation import add_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to initialize backend 'tpu': ABORTED: The TPU is already in use by process with pid 1484045. Not attempting to load libtpu.so in this process. (set JAX_PLATFORMS='' to automatically choose an available backend)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:887\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    885\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43m_init_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m _backends[platform] \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:973\u001b[0m, in \u001b[0;36m_init_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    972\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing backend \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, platform)\n\u001b[0;32m--> 973\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# TODO(skye): consider raising more descriptive errors directly from backend\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# factories instead of returning None.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:146\u001b[0m, in \u001b[0;36mtpu_client_timer_callback\u001b[0;34m(timer_secs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m   client \u001b[38;5;241m=\u001b[39m \u001b[43mxla_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_tpu_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mget_tpu_library_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_options_from_jax_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jaxlib/xla_client.py:210\u001b[0m, in \u001b[0;36mmake_tpu_client\u001b[0;34m(library_path, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m   profiler\u001b[38;5;241m.\u001b[39mregister_plugin_profiler(c_api)\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_tfrt_tpu_c_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jaxlib/xla_client.py:129\u001b[0m, in \u001b[0;36mmake_tfrt_tpu_c_api_client\u001b[0;34m(options)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pjrt_plugin_initialized(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m   \u001b[43minitialize_pjrt_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jaxlib/xla_client.py:177\u001b[0m, in \u001b[0;36minitialize_pjrt_plugin\u001b[0;34m(plugin_name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes a PJRT plugin.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mThe plugin needs to be loaded first (through load_pjrt_plugin_dynamically or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m  plugin_name: the name of the PJRT plugin.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_pjrt_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: ABORTED: The TPU is already in use by process with pid 1484045. Not attempting to load libtpu.so in this process.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmicrlhf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTransformer\n\u001b[0;32m----> 2\u001b[0m llama \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/gemma-2-2b-it.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mfrom_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mload_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micrlhf-progress/micrlhf/llama.py:389\u001b[0m, in \u001b[0;36mLlamaTransformer.from_pretrained\u001b[0;34m(cls, gguf_path, from_type, device_map, extract_layer, load_eager, transpose_rotary, load_on_cpu)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, gguf_path: os\u001b[38;5;241m.\u001b[39mPathLike \u001b[38;5;241m|\u001b[39m Iterable[os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    383\u001b[0m                     from_type: Literal[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m                     load_on_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    388\u001b[0m                     ):\n\u001b[0;32m--> 389\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     gguf \u001b[38;5;241m=\u001b[39m read_gguf(gguf_path)\n\u001b[1;32m    392\u001b[0m     is_gemma \u001b[38;5;241m=\u001b[39m (from_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m from_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micrlhf-progress/micrlhf/llama.py:373\u001b[0m, in \u001b[0;36mLlamaTransformer.make_mesh\u001b[0;34m(cls, device_map)\u001b[0m\n\u001b[1;32m    371\u001b[0m             mp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(part\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;66;03m# TODO SP support\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m jshard\u001b[38;5;241m.\u001b[39mMesh(np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, mp)), axis_names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpu:\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    375\u001b[0m     tpu_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(device_map\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:1085\u001b[0m, in \u001b[0;36mdevices\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdevices\u001b[39m(\n\u001b[1;32m   1061\u001b[0m     backend: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m xla_client\u001b[38;5;241m.\u001b[39mClient \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[xla_client\u001b[38;5;241m.\u001b[39mDevice]:\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of all devices for a given backend.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m  .. currentmodule:: jaxlib.xla_extension\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03m    List of Device subclasses.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdevices()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:1019\u001b[0m, in \u001b[0;36mget_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# don't use util.memoize because there is no X64 dependence.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_backend\u001b[39m(\n\u001b[1;32m   1017\u001b[0m     platform: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m xla_client\u001b[38;5;241m.\u001b[39mClient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xla_client\u001b[38;5;241m.\u001b[39mClient:\n\u001b[0;32m-> 1019\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:998\u001b[0m, in \u001b[0;36m_get_backend_uncached\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    994\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m platform\n\u001b[1;32m    996\u001b[0m platform \u001b[38;5;241m=\u001b[39m (platform \u001b[38;5;129;01mor\u001b[39;00m _XLA_BACKEND\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m _PLATFORM_NAME\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 998\u001b[0m bs \u001b[38;5;241m=\u001b[39m \u001b[43mbackends\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m   platform \u001b[38;5;241m=\u001b[39m canonicalize_platform(platform)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:903\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    901\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 903\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _default_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_platforms\u001b[38;5;241m.\u001b[39mvalue:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to initialize backend 'tpu': ABORTED: The TPU is already in use by process with pid 1484045. Not attempting to load libtpu.so in this process. (set JAX_PLATFORMS='' to automatically choose an available backend)"
     ]
    }
   ],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2-2b-it.gguf\",\n",
    "                                         from_type=\"gemma2\",\n",
    "                                         load_eager=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import jax\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import load_tasks, ICLDataset, ICLSequence\n",
    "tasks = load_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "\n",
    "with open(\"cleanup_results_gemma_2_algo.jsonl\", \"r\") as f:\n",
    "    cleanup_results = [json.loads(line) for line in f][18:]\n",
    "\n",
    "\n",
    "# with open(\"cleanup_results_gemma_2_all.jsonl\", \"r\") as f:\n",
    "#     tmp = [json.loads(line) for line in f]\n",
    "#     tmp = [x for x in tmp if not x[\"task\"].startswith(\"algo\")]\n",
    "\n",
    "#     cleanup_results += tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [14, 16, 18, 20, 22, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import sae_encode, get_dm_res_sae\n",
    "\n",
    "thresholds = {\n",
    "    layer: get_dm_res_sae(layer, load_65k=True).get(\"threshold\", 0) for layer in layers\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "task = \"antonyms\"\n",
    "task_results = [result for result in cleanup_results if result[\"layer\"] in layers and result[\"task\"] == task]   \n",
    "\n",
    "print(len(task_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'antonyms', 'weights': [51.93465042114258, 30.026456832885742, 19.8310489654541, 8.384634017944336, 7.020708084106445, 5.786473751068115], 'indices': [46209, 65229, 64797, 50557, 29998, 16645], 'tv loss': 2.875, 'cleaning loss': 1.765625, 'ito loss': 2.375, 'layer': 14}\n",
      "{'task': 'antonyms', 'weights': [58.69618606567383, 38.361087799072266, 37.61246109008789, 33.8771858215332], 'indices': [59184, 56016, 710, 38724], 'tv loss': 1.6015625, 'cleaning loss': 1.65625, 'ito loss': 1.875, 'layer': 16}\n",
      "{'task': 'antonyms', 'weights': [42.359962463378906, 38.76250457763672, 33.361412048339844, 26.453126907348633, 21.615398406982422, 18.74950408935547, 7.838016033172607, 4.738954067230225], 'indices': [50800, 37029, 44111, 1874, 4239, 8259, 3923, 36201], 'tv loss': 2.40625, 'cleaning loss': 3.046875, 'ito loss': 2.984375, 'layer': 18}\n",
      "{'task': 'antonyms', 'weights': [68.01600646972656, 47.43989562988281, 34.77153396606445, 22.7160587310791, 12.102900505065918], 'indices': [17969, 35629, 8113, 51920, 22490], 'tv loss': 3.078125, 'cleaning loss': 3.609375, 'ito loss': 3.65625, 'layer': 20}\n",
      "{'task': 'antonyms', 'weights': [18.081117630004883, 15.471077919006348, 12.165534019470215, 11.754265785217285, 8.32287311553955, 7.265712738037109], 'indices': [30143, 53243, 48410, 7547, 47121, 34041], 'tv loss': 3.609375, 'cleaning loss': 4.71875, 'ito loss': 3.671875, 'layer': 22}\n",
      "{'task': 'antonyms', 'weights': [29.24742889404297, 25.344867706298828, 25.226619720458984, 17.678293228149414, 16.97735595703125, 14.776976585388184], 'indices': [30027, 18518, 27714, 52668, 62219, 4390], 'tv loss': 3.84375, 'cleaning loss': 4.34375, 'ito loss': 3.109375, 'layer': 24}\n"
     ]
    }
   ],
   "source": [
    "with open(\"gemma_2_cleaning_compact_65k.jsonl\", \"w\") as f:\n",
    "    for r in task_results:\n",
    "        task = r[\"task\"]\n",
    "        layer = r[\"layer\"]\n",
    "        \n",
    "        weights = np.array(r[\"weights\"])\n",
    "\n",
    "        i = np.argwhere(weights > thresholds[layer]).flatten()\n",
    "        w = weights[i]\n",
    "\n",
    "        idx = np.argsort(w)[::-1]\n",
    "\n",
    "        i = i[idx]\n",
    "        w = w[idx]\n",
    "\n",
    "        data = {\n",
    "            \"task\": task,\n",
    "            \"weights\": w.tolist(),\n",
    "            \"indices\": i.tolist(),\n",
    "            \"tv loss\": r[\"tv_loss\"],\n",
    "            \"cleaning loss\": r[\"loss\"],\n",
    "            \"ito loss\": r[\"ito_loss\"],\n",
    "            \"layer\": layer\n",
    "        }\n",
    "\n",
    "        print(data)\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-_SD4q1c9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
