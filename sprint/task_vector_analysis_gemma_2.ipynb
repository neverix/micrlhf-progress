{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"../..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from penzai import pz\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import random\n",
    "from penzai.data_effects.side_output import SideOutputValue\n",
    "from micrlhf.utils.activation_manipulation import add_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to initialize backend 'tpu': ABORTED: The TPU is already in use by process with pid 1484045. Not attempting to load libtpu.so in this process. (set JAX_PLATFORMS='' to automatically choose an available backend)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:887\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    885\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43m_init_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m _backends[platform] \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:973\u001b[0m, in \u001b[0;36m_init_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    972\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing backend \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, platform)\n\u001b[0;32m--> 973\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# TODO(skye): consider raising more descriptive errors directly from backend\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# factories instead of returning None.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:146\u001b[0m, in \u001b[0;36mtpu_client_timer_callback\u001b[0;34m(timer_secs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m   client \u001b[38;5;241m=\u001b[39m \u001b[43mxla_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_tpu_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mget_tpu_library_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_options_from_jax_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jaxlib/xla_client.py:210\u001b[0m, in \u001b[0;36mmake_tpu_client\u001b[0;34m(library_path, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m   profiler\u001b[38;5;241m.\u001b[39mregister_plugin_profiler(c_api)\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_tfrt_tpu_c_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jaxlib/xla_client.py:129\u001b[0m, in \u001b[0;36mmake_tfrt_tpu_c_api_client\u001b[0;34m(options)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pjrt_plugin_initialized(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m   \u001b[43minitialize_pjrt_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jaxlib/xla_client.py:177\u001b[0m, in \u001b[0;36minitialize_pjrt_plugin\u001b[0;34m(plugin_name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes a PJRT plugin.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mThe plugin needs to be loaded first (through load_pjrt_plugin_dynamically or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m  plugin_name: the name of the PJRT plugin.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_pjrt_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: ABORTED: The TPU is already in use by process with pid 1484045. Not attempting to load libtpu.so in this process.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmicrlhf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTransformer\n\u001b[0;32m----> 2\u001b[0m llama \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/gemma-2-2b-it.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mfrom_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mload_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micrlhf-progress/micrlhf/llama.py:389\u001b[0m, in \u001b[0;36mLlamaTransformer.from_pretrained\u001b[0;34m(cls, gguf_path, from_type, device_map, extract_layer, load_eager, transpose_rotary, load_on_cpu)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, gguf_path: os\u001b[38;5;241m.\u001b[39mPathLike \u001b[38;5;241m|\u001b[39m Iterable[os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    383\u001b[0m                     from_type: Literal[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m                     load_on_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    388\u001b[0m                     ):\n\u001b[0;32m--> 389\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     gguf \u001b[38;5;241m=\u001b[39m read_gguf(gguf_path)\n\u001b[1;32m    392\u001b[0m     is_gemma \u001b[38;5;241m=\u001b[39m (from_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m from_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micrlhf-progress/micrlhf/llama.py:373\u001b[0m, in \u001b[0;36mLlamaTransformer.make_mesh\u001b[0;34m(cls, device_map)\u001b[0m\n\u001b[1;32m    371\u001b[0m             mp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(part\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;66;03m# TODO SP support\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m jshard\u001b[38;5;241m.\u001b[39mMesh(np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, mp)), axis_names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpu:\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    375\u001b[0m     tpu_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(device_map\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:1085\u001b[0m, in \u001b[0;36mdevices\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdevices\u001b[39m(\n\u001b[1;32m   1061\u001b[0m     backend: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m xla_client\u001b[38;5;241m.\u001b[39mClient \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[xla_client\u001b[38;5;241m.\u001b[39mDevice]:\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of all devices for a given backend.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m  .. currentmodule:: jaxlib.xla_extension\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03m    List of Device subclasses.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdevices()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:1019\u001b[0m, in \u001b[0;36mget_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# don't use util.memoize because there is no X64 dependence.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_backend\u001b[39m(\n\u001b[1;32m   1017\u001b[0m     platform: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m xla_client\u001b[38;5;241m.\u001b[39mClient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xla_client\u001b[38;5;241m.\u001b[39mClient:\n\u001b[0;32m-> 1019\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:998\u001b[0m, in \u001b[0;36m_get_backend_uncached\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    994\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m platform\n\u001b[1;32m    996\u001b[0m platform \u001b[38;5;241m=\u001b[39m (platform \u001b[38;5;129;01mor\u001b[39;00m _XLA_BACKEND\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m _PLATFORM_NAME\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 998\u001b[0m bs \u001b[38;5;241m=\u001b[39m \u001b[43mbackends\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m   platform \u001b[38;5;241m=\u001b[39m canonicalize_platform(platform)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-_SD4q1c9-py3.10/lib/python3.10/site-packages/jax/_src/xla_bridge.py:903\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    901\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 903\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _default_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_platforms\u001b[38;5;241m.\u001b[39mvalue:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to initialize backend 'tpu': ABORTED: The TPU is already in use by process with pid 1484045. Not attempting to load libtpu.so in this process. (set JAX_PLATFORMS='' to automatically choose an available backend)"
     ]
    }
   ],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2-2b-it.gguf\",\n",
    "                                         from_type=\"gemma2\",\n",
    "                                         load_eager=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import jax\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import load_tasks, ICLDataset, ICLSequence\n",
    "tasks = load_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "\n",
    "with open(\"cleanup_results_gemma_2_algo.jsonl\", \"r\") as f:\n",
    "    cleanup_results = [json.loads(line) for line in f][18:]\n",
    "\n",
    "\n",
    "# with open(\"cleanup_results_gemma_2_all.jsonl\", \"r\") as f:\n",
    "#     tmp = [json.loads(line) for line in f]\n",
    "#     tmp = [x for x in tmp if not x[\"task\"].startswith(\"algo\")]\n",
    "\n",
    "#     cleanup_results += tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [14, 16, 18, 20, 22, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import sae_encode, get_dm_res_sae\n",
    "\n",
    "thresholds = {\n",
    "    layer: get_dm_res_sae(layer, load_65k=True).get(\"threshold\", 0) for layer in layers\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "task = \"present_simple_past_simple\"\n",
    "task_results = [result for result in cleanup_results if result[\"layer\"] in layers and result[\"task\"] == task] \n",
    "\n",
    "print(len(task_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'present_simple_past_simple', 'weights': [99.33623504638672, 80.54329681396484, 58.63016128540039, 34.633750915527344, 31.384580612182617, 12.398971557617188, 12.3705415725708], 'indices': [7678, 57043, 17899, 5446, 18436, 5447, 55982], 'tv loss': 6.8125, 'cleaning loss': 2.34375, 'ito loss': 6.8125, 'layer': 14}\n",
      "{'task': 'present_simple_past_simple', 'weights': [43.28398132324219, 42.81904220581055, 35.30847930908203, 34.42615509033203, 33.54819869995117, 29.884140014648438, 27.383426666259766, 18.86416244506836, 16.213579177856445, 7.187838554382324], 'indices': [34279, 61613, 39700, 29721, 43597, 27146, 2702, 61107, 47139, 37576], 'tv loss': 3.9375, 'cleaning loss': 0.734375, 'ito loss': 6.53125, 'layer': 16}\n",
      "{'task': 'present_simple_past_simple', 'weights': [44.88591003417969, 40.624420166015625, 39.58631896972656, 37.01276397705078, 28.482345581054688, 27.461191177368164, 26.271484375, 23.297405242919922], 'indices': [47899, 46985, 63994, 57630, 45455, 35308, 36806, 23825], 'tv loss': 1.3984375, 'cleaning loss': 0.95703125, 'ito loss': 2.953125, 'layer': 18}\n",
      "{'task': 'present_simple_past_simple', 'weights': [53.803524017333984, 49.11246871948242, 45.80107879638672, 42.53937530517578, 34.56544876098633, 31.230567932128906, 27.928409576416016, 14.6514892578125], 'indices': [60631, 10342, 44346, 16279, 55249, 11048, 16304, 8343], 'tv loss': 1.5703125, 'cleaning loss': 1.3125, 'ito loss': 2.015625, 'layer': 20}\n",
      "{'task': 'present_simple_past_simple', 'weights': [82.71891784667969, 68.75457000732422, 62.47108459472656, 53.615352630615234], 'indices': [54287, 45464, 40568, 61352], 'tv loss': 3.171875, 'cleaning loss': 1.8359375, 'ito loss': 2.140625, 'layer': 22}\n",
      "{'task': 'present_simple_past_simple', 'weights': [106.0670394897461, 82.15068817138672, 56.087730407714844, 36.18806838989258, 32.8807258605957, 27.190357208251953, 10.08976936340332], 'indices': [29545, 58815, 1103, 30027, 26888, 38175, 7664], 'tv loss': 5.09375, 'cleaning loss': 2.640625, 'ito loss': 2.234375, 'layer': 24}\n"
     ]
    }
   ],
   "source": [
    "# with open(\"gemma_2_cleaning_compact_65k.jsonl\", \"w\") as f:\n",
    "for r in task_results:\n",
    "    task = r[\"task\"]\n",
    "    layer = r[\"layer\"]\n",
    "    \n",
    "    weights = np.array(r[\"weights\"])\n",
    "\n",
    "    i = np.argwhere(weights > thresholds[layer]).flatten()\n",
    "    w = weights[i]\n",
    "\n",
    "    idx = np.argsort(w)[::-1]\n",
    "\n",
    "    i = i[idx]\n",
    "    w = w[idx]\n",
    "\n",
    "    data = {\n",
    "        \"task\": task,\n",
    "        \"weights\": w.tolist(),\n",
    "        \"indices\": i.tolist(),\n",
    "        \"tv loss\": r[\"tv_loss\"],\n",
    "        \"cleaning loss\": r[\"loss\"],\n",
    "        \"ito loss\": r[\"ito_loss\"],\n",
    "        \"layer\": layer\n",
    "    }\n",
    "\n",
    "    print(data)\n",
    "    # f.write(json.dumps(data) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-_SD4q1c9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
