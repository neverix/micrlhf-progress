{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import penzai\n",
    "from penzai import pz\n",
    "# %%\n",
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"../..\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import random\n",
    "from penzai.data_effects.side_output import SideOutputValue\n",
    "from micrlhf.utils.activation_manipulation import add_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\", from_type=\"gemma\", load_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import jax\n",
    "\n",
    "\n",
    "# %%\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sprint.task_vector_utils import load_tasks, ICLDataset, ICLSequence\n",
    "tasks = load_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "from micrlhf.llama import LlamaBlock\n",
    "from micrlhf.sampling import sample, jit_wrapper\n",
    "get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "    pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_pre_{i}\"),\n",
    "        x\n",
    "    ])\n",
    ")\n",
    "get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: x.startswith(\"resid_pre\"))\n",
    "get_resids_call = jit_wrapper.Jitted(get_resids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_to_inputs(input_ids, attention_mask):\n",
    "    token_array = jnp.asarray(input_ids)\n",
    "    token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\").untag(\"batch\").tag(\"batch\")\n",
    "\n",
    "    mask_array = jnp.asarray(attention_mask, dtype=jnp.bool)\n",
    "    mask_array = jax.device_put(mask_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    mask_array = pz.nx.wrap(mask_array, \"batch\", \"seq\").untag(\"batch\").tag(\"batch\")\n",
    "\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sprint.task_vector_utils import ICLRunner, logprob_loss, get_tv, make_act_adder\n",
    "\n",
    "from micrlhf.utils.load_sae import sae_encode, sae_encode_gated\n",
    "\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import get_nev_it_sae_suite\n",
    "\n",
    "sae = get_nev_it_sae_suite(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "sep = 3978\n",
    "pad = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = [\n",
    "    \"es_en\"\n",
    "]\n",
    "n_seeds = 10\n",
    "\n",
    "# n_few_shots, batch_size, max_seq_len = 64, 64, 512\n",
    "n_few_shots, batch_size, max_seq_len = 20, 16, 256\n",
    "\n",
    "prompt = \"Follow the pattern:\\n{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83edaf773e0a4a498a38b659609bbfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV: es_en, L: 12, Loss: 3.1711559295654297\n",
      "Recon TV: es_en, L: 12, Loss: 3.467447280883789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d672a5644a64cc1b9bdf8cf5f851e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recon fs: es_en, L: 12, Loss: 3.6704518795013428\n"
     ]
    }
   ],
   "source": [
    "from sprint.task_vector_utils import ICLRunner, logprob_loss, get_tv, make_act_adder\n",
    "\n",
    "from micrlhf.utils.load_sae import sae_encode, sae_encode_gated\n",
    "\n",
    "from safetensors import safe_open\n",
    "from sprint.task_vector_utils import FeatureSearch\n",
    "\n",
    "seed = 10\n",
    "\n",
    "\n",
    "collected_weights = []\n",
    "\n",
    "for task in tqdm(task_names):\n",
    "    pairs = list(tasks[task].items())\n",
    "\n",
    "    n_shot = 40\n",
    "\n",
    "    runner = ICLRunner(task, pairs, batch_size=batch_size, n_shot=n_few_shots-1, max_seq_len=max_seq_len, seed=seed, prompt=prompt)\n",
    "\n",
    "\n",
    "    tokenized = runner.get_tokens([\n",
    "        x[:n_shot] for x in runner.train_pairs\n",
    "    ], tokenizer)\n",
    "\n",
    "    inputs = tokenized_to_inputs(**tokenized)\n",
    "    train_tokens = tokenized[\"input_ids\"]\n",
    "\n",
    "    _, resids = get_resids_call(inputs)\n",
    "\n",
    "    resids = resids[layer].value.unwrap(\n",
    "        \"batch\", \"seq\", \"embedding\"\n",
    "    )\n",
    "\n",
    "    tv = get_tv(resids, train_tokens, shift = 0, sep=sep)\n",
    "\n",
    "    tokenized = runner.get_tokens(runner.eval_pairs, tokenizer)\n",
    "    inputs = tokenized_to_inputs(**tokenized)\n",
    "    tokens = tokenized[\"input_ids\"]\n",
    "\n",
    "    add_act = make_act_adder(llama, tv.astype('bfloat16'), tokens, layer, length=1, shift= 0, sep=sep)\n",
    "\n",
    "    logits = add_act(inputs)\n",
    "\n",
    "    loss = logprob_loss(\n",
    "        logits.unwrap(\"batch\", \"seq\", \"vocabulary\"), tokens, shift=1 if task.startswith(\"algo\") else 0, n_first=2, sep=sep, pad_token=0\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"TV: {task}, L: {layer}, Loss: {loss}\"  \n",
    "    )\n",
    "\n",
    "    _, pr, rtv = sae_encode_gated(sae, tv)\n",
    "\n",
    "    add_act = make_act_adder(llama, rtv.astype('bfloat16'), tokens, layer, length=1, shift= 0, sep=sep)\n",
    "\n",
    "    logits = add_act(inputs)\n",
    "\n",
    "    loss = logprob_loss(\n",
    "        logits.unwrap(\"batch\", \"seq\", \"vocabulary\"), tokens, shift=1 if task.startswith(\"algo\") else 0, n_first=2, sep=sep, pad_token=0\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Recon TV: {task}, L: {layer}, Loss: {loss}\"  \n",
    "    )\n",
    "\n",
    "    fs = FeatureSearch(task, pairs, layer, llama, tokenizer, n_shot=1, seed=seed+100, init_w=pr, early_stopping_steps=200, n_first=2, sep=sep, pad_token=0, sae_v=8, sae=sae, batch_size=24, iterations=400, prompt=prompt, l1_coeff=0.05)\n",
    "\n",
    "    w, m = fs.find_weights()\n",
    "\n",
    "    collected_weights.append(\n",
    "        w                                           \n",
    "    )\n",
    "\n",
    "    # weights = (w > 0) * jax.nn.relu(w * jax.nn.softplus(sae[\"s_gate\"]) + sae[\"b_gate\"])   \n",
    "\n",
    "    weights = jax.nn.relu(w)\n",
    "\n",
    "    recon = jnp.einsum(\"fv,f->v\", sae[\"W_dec\"], weights) + sae[\"b_dec\"]\n",
    "    recon = recon.astype('bfloat16')\n",
    "\n",
    "    add_act = make_act_adder(llama, recon, tokens, layer, length=1, shift= 0, sep=sep)\n",
    "\n",
    "    logits = add_act(inputs)\n",
    "\n",
    "    loss = logprob_loss(\n",
    "        logits.unwrap(\"batch\", \"seq\", \"vocabulary\"), tokens, shift=1 if task.startswith(\"algo\") else 0, n_first=2, sep=sep, pad_token=0\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Recon fs: {task}, L: {layer}, Loss: {loss}\"  \n",
    "    )\n",
    "\n",
    "    with open(\"cleanup_results.jsonl\", \"a\") as f:\n",
    "        item = {\n",
    "            \"task\": task,\n",
    "            \"weights\": w.tolist(),\n",
    "            \"loss\": loss.tolist(),\n",
    "            \"tv\": tv.tolist(),\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_es\n",
      "3.0577216148376465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([7.781368  , 7.687472  , 4.5929112 , 2.3076181 , 1.8058963 ,\n",
       "        0.9904503 , 0.26809523, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],      dtype=float32),\n",
       " Array([  330, 15886,  3079,  3196,  4643,  7800, 11586,     0,     1,\n",
       "            2,     3,     4,     5,     6,     7], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_es\n",
      "2.7183780670166016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([9.630995 , 4.7369976, 3.1871123, 2.4100933, 2.3127606, 1.0788292,\n",
       "        0.8785468, 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       ], dtype=float32),\n",
       " Array([15886,  3079,  2207, 11836,  4643,  7018,  7800,     0,     1,\n",
       "            2,     3,     4,     5,     6,     7], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_fr\n",
      "9.213409423828125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([12.659199  ,  9.891936  ,  3.5718477 ,  2.161456  ,  1.9997382 ,\n",
       "         0.20197022,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],      dtype=float32),\n",
       " Array([13367,   330,  7018,  3079,   369,  8618,     0,     1,     2,\n",
       "            3,     4,     5,     6,     7,     8], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antonyms\n",
      "1.9884886741638184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([5.149931  , 4.4392924 , 4.424511  , 2.9623194 , 1.0613811 ,\n",
       "        0.41651246, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],      dtype=float32),\n",
       " Array([ 2207, 12017, 15392,  2468,  8226,  4871,     0,     1,     2,\n",
       "            3,     4,     5,     6,     7,     8], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_en\n",
      "2.3643341064453125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([5.7215486, 4.077082 , 1.6318746, 1.3926536, 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       ], dtype=float32),\n",
       " Array([ 883, 4061,  181, 1908,    0,    1,    2,    3,    4,    5,    6,\n",
       "           7,    8,    9,   10], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_it\n",
      "6.378209114074707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([10.971681  ,  6.341071  ,  4.6003017 ,  2.8350635 ,  0.43321684,\n",
       "         0.36964452,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],      dtype=float32),\n",
       " Array([15886,  7018,  7800,   369,  4424,  3079,     0,     1,     2,\n",
       "            3,     4,     5,     6,     7,     8], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_en\n",
      "3.789034843444824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([5.05083   , 4.956336  , 3.0223713 , 1.8414422 , 0.06206318,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],      dtype=float32),\n",
       " Array([  883,   181, 15288,  8684,  2207,     0,     1,     2,     3,\n",
       "            4,     5,     6,     7,     8,     9], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_continent\n",
      "2.0601282119750977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([11.718188  ,  8.453872  ,  7.960262  ,  6.295211  ,  6.0035276 ,\n",
       "         3.7814338 ,  2.8840249 ,  2.545268  ,  0.85108435,  0.45151338,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],      dtype=float32),\n",
       " Array([11896,  1685, 12805, 12803,   330,  2207, 15176, 15315,  6637,\n",
       "         1236,     0,     1,     2,     3,     4], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_language\n",
      "2.383928060531616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([9.012982 , 4.8249397, 2.3631427, 2.3532834, 1.392308 , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       ], dtype=float32),\n",
       " Array([ 2417, 12975,  1236,  6637, 11552,     0,     1,     2,     3,\n",
       "            4,     5,     6,     7,     8,     9], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person_profession\n",
      "3.4659793376922607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([5.0680375 , 4.5605073 , 3.216459  , 2.551515  , 1.98678   ,\n",
       "        1.8147309 , 1.5620106 , 0.94064075, 0.16524048, 0.07524845,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],      dtype=float32),\n",
       " Array([15885, 13589,  4760,   482,  1174, 16341,  6637,  8226,  1236,\n",
       "         4478,     0,     1,     2,     3,     4], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_en\n",
      "2.475567102432251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([4.174203  , 4.085805  , 2.9904673 , 2.4196987 , 2.383186  ,\n",
       "        2.2566912 , 1.9916719 , 1.9025337 , 1.0599831 , 0.9431557 ,\n",
       "        0.60823464, 0.5850021 , 0.08400264, 0.        , 0.        ],      dtype=float32),\n",
       " Array([20995, 25680,  8668,  4126, 17274, 31871, 28080, 28491, 27988,\n",
       "         7028, 26180,  9258,  9497,     0,     1], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antonyms\n",
      "2.1853365898132324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([4.3000865 , 4.0614967 , 3.4217446 , 2.8159714 , 2.2184155 ,\n",
       "        2.0036063 , 1.8057712 , 1.629368  , 0.8533016 , 0.49197406,\n",
       "        0.34810975, 0.3253169 , 0.        , 0.        , 0.        ],      dtype=float32),\n",
       " Array([10472,  2670,  6462, 19190, 11906, 27988,  8668,  9258,  8159,\n",
       "        20995, 15648,  9497,     0,     1,     2], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_en\n",
      "2.4363996982574463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([4.991836  , 3.4801493 , 3.4285762 , 3.2275271 , 3.22435   ,\n",
       "        3.2031236 , 3.1699693 , 2.7637427 , 1.4658012 , 0.8083773 ,\n",
       "        0.60823464, 0.13834275, 0.01402632, 0.        , 0.        ],      dtype=float32),\n",
       " Array([25680, 17274, 28080,  8668, 28491,  4126, 31871, 20995, 27988,\n",
       "        32590, 26180,  7028,  9497,     0,     1], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_en\n",
      "2.7721495628356934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([4.981508  , 4.1341496 , 3.6995676 , 3.5769396 , 3.1237977 ,\n",
       "        3.0046246 , 2.640297  , 1.8352877 , 1.7989156 , 0.5231221 ,\n",
       "        0.34189785, 0.181504  , 0.        , 0.        , 0.        ],      dtype=float32),\n",
       " Array([10574,  6382,  1777,  6882,  9180, 31923, 25260, 18303, 10722,\n",
       "        28088, 12773, 22236,     0,     1,     2], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_en\n",
      "3.3540031909942627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([5.964342 , 5.4778223, 5.4725738, 5.438234 , 4.9570684, 4.7402883,\n",
       "        4.710356 , 4.6115384, 4.248613 , 3.0572634, 2.3268917, 2.1025603,\n",
       "        1.8345332, 1.5063884, 1.4786938], dtype=float32),\n",
       " Array([21043, 20013, 29597, 12039, 25421,  1272, 10807, 21523,  4560,\n",
       "        27329,  3925, 30363,  5217,  2114, 17969], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_en\n",
      "2.688720464706421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([7.1359415e+00, 7.1008859e+00, 6.5918088e+00, 6.4541144e+00,\n",
       "        5.4941392e+00, 5.4563355e+00, 3.1585410e+00, 2.6410753e-03,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00], dtype=float32),\n",
       " Array([ 8379,  1481,  2612, 12626,  8426, 15973, 22035,  6277,     0,\n",
       "            1,     2,     3,     4,     5,     6], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_en\n",
      "3.6704518795013428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Array([6.8068395 , 5.6830826 , 5.675987  , 5.1919236 , 0.09097664,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],      dtype=float32),\n",
       " Array([32345,  9157, 28537, 23723, 17160,     0,     1,     2,     3,\n",
       "            4,     5,     6,     7,     8,     9], dtype=int32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"cleanup_results.jsonl\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        print(data[\"task\"])\n",
    "        print(data[\"loss\"])\n",
    "        w = jnp.array(data[\"weights\"])\n",
    "        display(jax.lax.top_k(w, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import HfFileSystem\n",
    "\n",
    "fs = HfFileSystem()\n",
    "weights = fs.glob(f\"nev/gemma-2b-saex-test/it-l{layer}-residual-test-run-1-*/*.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nev/gemma-2b-saex-test/it-l12-residual-test-run-1-2.00E-05/sae_weights.safetensors']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.13326340913772583,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(28.125, dtype=bfloat16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sae[\"mean_norm\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-_SD4q1c9-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
