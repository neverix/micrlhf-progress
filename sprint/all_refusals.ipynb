{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\",\n",
    "                                         from_type=\"gemma\",\n",
    "                                         load_eager=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "import pandas as pd\n",
    "df_adv = pd.read_csv(\"data/adv.csv\")\n",
    "format_prompt = \"\"\"<start_of_turn>user\\n\n",
    "{}\\n\n",
    "<start_of_turn>model\\n\n",
    "{}\"\"\"\n",
    "# offset = 1\n",
    "# df_do = df.apply(lambda x: format_response.format(x['goal'], x['target']), axis=1)\n",
    "# prompts_harmful = df.apply(lambda x: format_prompt.format(x['goal'], \"\")[:-offset], axis=1).to_list()[:100]\n",
    "prompts_harmful = df_adv.apply(lambda x: format_prompt.format(x['goal'], \"\"), axis=1).to_list()[:100]\n",
    "dataset_jail = pd.read_csv(\"data/jail.csv\").apply(lambda x: x[\"Goal\"], axis=1).to_list()\n",
    "prompts_jail = [format_prompt.format(x, \"\") for x in dataset_jail]\n",
    "import datasets\n",
    "# https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw\n",
    "hf_path = 'tatsu-lab/alpaca'\n",
    "dataset = datasets.load_dataset(hf_path)\n",
    "# filter for instructions that do not have inputs\n",
    "prompts_harmless = []\n",
    "for i in range(len(dataset['train'])):\n",
    "    if len(prompts_harmless) >= len(prompts_harmful):\n",
    "        break\n",
    "    if dataset['train'][i]['input'].strip() == '':\n",
    "        # prompts_harmless.append(format_prompt.format(dataset['train'][i]['instruction'], \"\")[:-offset])\n",
    "        prompts_harmless.append(format_prompt.format(dataset['train'][i]['instruction'], \"\"))\n",
    "\n",
    "# ds = datasets.load_dataset(\"MBZUAI/LaMini-instruction\", split=\"train\", streaming=True)\n",
    "# prompts_harmless = []\n",
    "# for _, text in zip(range(100), ds):\n",
    "#     prompts_harmless.append(format_prompt.format(text[\"instruction\"], \"\"))\n",
    "\n",
    "# ds = datasets.load_dataset(\"nev/openhermes-2.5-phi-format-text\", split=\"train\", streaming=True)\n",
    "# prompts_harmless = []\n",
    "# for _, text in zip(range(100), ds):\n",
    "#     text = text[\"text\"]\n",
    "#     text = \"\".join(text.partition(\"<|assistant|>\\n\")[:2])\n",
    "#     prompts_harmless.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "import jax\n",
    "\n",
    "tokens = tokenizer.batch_encode_plus(prompts_harmful + prompts_harmless,\n",
    "                                     return_tensors=\"np\",\n",
    "                                     padding=\"max_length\",\n",
    "                                     truncation=True,\n",
    "                                     max_length=128,\n",
    "                                     return_attention_mask=True)\n",
    "token_array = jnp.asarray(tokens[\"input_ids\"])\n",
    "token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "token_array = pz.nx.wrap(token_array, \"batch\", \"seq\").untag(\"batch\").tag(\"batch\")\n",
    "inputs = llama.inputs.from_basic_segments(token_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock\n",
    "from micrlhf.flash import flashify\n",
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "    pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_pre_{i}\"),\n",
    "        x\n",
    "    ])\n",
    ")\n",
    "get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: x.startswith(\"resid_pre\"))\n",
    "get_resids_call = jit_wrapper.Jitted(get_resids)\n",
    "_, resids = get_resids_call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "residiffs = []\n",
    "last_resids = []\n",
    "for i, resid in enumerate(sorted(resids, key=lambda x: int(x.tag.rpartition(\"_\")[-1]))):\n",
    "    resid = resid.value.unwrap(\"batch\", \"seq\", \"embedding\")\n",
    "    indices = jnp.asarray(tokens[\"attention_mask\"].sum(1, keepdims=True))[..., None] - 1\n",
    "    last_resid = jnp.take_along_axis(resid, indices, 1)[:, 0]\n",
    "    last_resid = last_resid.reshape(2, -1, last_resid.shape[-1])\n",
    "    last_resids.append(last_resid)\n",
    "    last_resid = last_resid.mean(1)\n",
    "    residiff = np.array(last_resid[0] - last_resid[1])\n",
    "    residiff = residiff / np.linalg.norm(residiff)\n",
    "    residiffs.append(residiff)\n",
    "matmuls = np.matmul(residiffs, np.transpose(residiffs))\n",
    "norms = np.linalg.norm(residiffs, axis=-1) + 1e-10\n",
    "plt.title(\"Correlations between layers' refusal vectors\")\n",
    "plt.imshow(matmuls / norms[:, None] / norms[None, :], vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.vector_storage import save_and_upload_vector, download_vector\n",
    "from micrlhf.utils.load_sae import get_nev_it_sae_suite\n",
    "from micrlhf.utils.ito import grad_pursuit\n",
    "\n",
    "\n",
    "sae_k = 8\n",
    "threshold = 0.15\n",
    "interesting_features = set()\n",
    "for layer_sae in range(8, 16):\n",
    "    dictionary = get_nev_it_sae_suite(layer_sae)[\"W_dec\"]\n",
    "    dictionary = dictionary / np.linalg.norm(dictionary, axis=1, keepdims=True)\n",
    "    for layer in range(1, len(residiffs)):\n",
    "        vector_name = f\"gemma-refusal-l{layer}\"\n",
    "        vector = residiffs[layer]\n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        try:\n",
    "            save_and_upload_vector(vector_name, vector)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        weights, recon = grad_pursuit(vector, dictionary, sae_k, pos_only=True)\n",
    "        w, i = jax.lax.top_k(jnp.abs(weights), sae_k)\n",
    "        print(f\"Layer {layer} -> Layer {layer_sae}: {[(int(a), float(b)) for a, b in zip(i, w)]}\")\n",
    "        for f, u in zip(i, w):\n",
    "            if u < threshold:\n",
    "                continue\n",
    "            interesting_features.add((layer_sae, int(f)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaAttention\n",
    "influences = {}\n",
    "influences_attn = {}\n",
    "all_feat_directions = {}\n",
    "\n",
    "def prop_sae(feat_source, sae_mid, feat_target):\n",
    "    strengths = (feat_source @ sae_mid[\"W_enc\"] * jax.nn.softplus(sae_mid[\"s_gate\"]) * sae_mid[\"scaling_factor\"]) * (sae_mid[\"W_dec\"] @ feat_target)\n",
    "    # dic = sae_mid[\"W_dec\"]\n",
    "    # dic = dic\n",
    "    # dic_norm = jnp.linalg.norm(dic, axis=1)\n",
    "    # strengths = ((dic @ feat_source) * (dic @ feat_target)) / (dic_norm ** 2)\n",
    "    return strengths\n",
    "\n",
    "n_top = 8\n",
    "strength_thresh = 0.016\n",
    "for i, (layer_source, feature_source) in enumerate(tqdm(all_feats)):\n",
    "    for j, (layer_target, feature_target) in enumerate(all_feats):\n",
    "        if layer_target <= layer_source:\n",
    "            continue\n",
    "        feat_source = get_nev_it_sae_suite(layer_source)[\"W_dec\"][feature_source]\n",
    "        target_sae = get_nev_it_sae_suite(layer_target)\n",
    "        feat_target = (target_sae[\"W_enc\"] * jax.nn.softplus(sae_mid[\"s_gate\"]) * sae_mid[\"scaling_factor\"]).T[feature_target]\n",
    "        # feat_target = target_sae[\"W_dec\"][feature_target]\n",
    "        # feat_source = feat_source / np.linalg.norm(feat_source)\n",
    "        # feat_target = feat_target / np.linalg.norm(feat_target)\n",
    "\n",
    "        edge_key = ((layer_source, feature_source), (layer_target, feature_target))\n",
    "        all_feat_directions[edge_key] = (np.array(feat_source), np.array(feat_target))\n",
    "        influencers = []\n",
    "        for layer_mid in range(layer_source , layer_target):\n",
    "            sae_mid = get_nev_it_sae_suite(layer_mid, label=\"transcoder\")\n",
    "            strengths =prop_sae(feat_source, sae_mid, feat_target)\n",
    "            top_effects, top_feats = jax.lax.top_k(jnp.abs(strengths), n_top)\n",
    "            for f in top_feats:\n",
    "                influencers.append((float(strengths[f]), layer_mid, int(f)))\n",
    "        # influencers.sort(key=lambda x: abs(x[0]))\n",
    "        # display(influencers)\n",
    "        influencers = [(s, l, f) for s, l, f in influencers if abs(s) > strength_thresh]  # vibes\n",
    "        influences[edge_key] = influencers\n",
    "        \n",
    "        influencers_attn = []\n",
    "        for layer_mid in range(layer_source, layer_target):\n",
    "            sae_mid = get_nev_it_sae_suite(layer_mid, label=\"attn_out\")\n",
    "            attn_layer = llama.select().at_instances_of(LlamaAttention).get_sequence()[layer_mid]\n",
    "            v_linear = attn_layer.input_to_value.select().at_instances_of(pz.nn.Linear).get_sequence()[0].weights.value\n",
    "            o_linear = attn_layer.attn_value_to_output.select().at_instances_of(pz.nn.Linear).get_sequence()[0].weights.value\n",
    "            fs = jnp.einsum(\"a,abc,rbco->o\", feat_source, v_linear.unwrap(\"embedding\", \"kv_heads\", \"projection\"), o_linear.unwrap(\"q_rep\", \"kv_heads\", \"projection\", \"embedding\"))\n",
    "            strengths = prop_sae(fs, sae_mid, feat_target)\n",
    "            top_effects, top_feats = jax.lax.top_k(jnp.abs(strengths), n_top)\n",
    "            for f in top_feats:\n",
    "                influencers_attn.append((float(strengths[f]), layer_mid, int(f)))\n",
    "        # influencers_attn.sort(key=lambda x: abs(x[0]))\n",
    "        influencers_attn = [(s, l, f) for s, l, f in influencers_attn if abs(s) > strength_thresh]\n",
    "        influences_attn[edge_key] = influencers_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_attn_outs = set()\n",
    "for key, influencers in influences_attn.items():\n",
    "    for _, layer, feature in influencers:\n",
    "        interesting_attn_outs.add((layer, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import os\n",
    "import json\n",
    "\n",
    "def norm(x):\n",
    "    x = np.array(x)\n",
    "    x = x / max(1e-3, x.max())\n",
    "    return x\n",
    "\n",
    "feat_dir = \"data/feature_explanations\"\n",
    "os.makedirs(feat_dir, exist_ok=True)\n",
    "feat_explanations = {}\n",
    "max_len = 72\n",
    "linebreak = 12\n",
    "for t, l, f in tqdm(set((\"R\", l, f) for l, f in interesting_features) | set((\"A\", l, f) for l, f in interesting_attn_outs)):\n",
    "    filename = f\"{feat_dir}/explanation_{t}_{l}_{f}.json\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        response = requests.get(f\"https://datasets-server.huggingface.co/rows?dataset=kisate-team%2Fgemma-2b-suite-explanations&config=l{l}{'' if t == 'R' else '_attn_out' if t == 'A' else 1/0}&split=train&offset={f}&length=1\")\n",
    "        data = response.json()\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "    \n",
    "    if not data or set(data.keys()) == {\"error\"}:\n",
    "        print(\"skipping\", l, f, data)\n",
    "        continue\n",
    "    \n",
    "    feat_info = data[\"rows\"][0][\"row\"]\n",
    "    criterion = norm(feat_info[\"scale_tuning\"][\"selfsims\"][-2]) * 0.4 - 0.01 * norm(feat_info[\"scale_tuning\"][\"entropy\"])\n",
    "    scale = feat_info[\"scale_tuning\"][\"scales\"][np.argmax(criterion[10:]) + 10]\n",
    "    index = np.searchsorted(feat_info[\"generations\"][\"scales\"], scale)\n",
    "    texts = feat_info[\"generations\"][\"texts\"]\n",
    "    # display((l, f, scale, texts[index], texts))\n",
    "    print((t, l, f, scale, repr(texts[index])))\n",
    "    unprocessed = texts[index]\n",
    "    processed = unprocessed.partition(\"<eos>\")[0].partition('\"')[0].strip()\n",
    "    if len(processed) > max_len:\n",
    "        processed = processed[:max_len] + \"...\"\n",
    "    processed = f\"{t}{l}.{f}: {processed}\"\n",
    "    feat_explanations[f\"{t}{l}.{f}\"] = textwrap.fill(processed, width=linebreak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "from tqdm.auto import tqdm\n",
    "import graphviz\n",
    "import random\n",
    "\n",
    "all_feats = list(interesting_features)\n",
    "graph = graphviz.Digraph(graph_attr={\"overlap\": \"scale\", \"splines\": \"ortho\", \"engine\": \"sfdp\"})\n",
    "edges_out = set()\n",
    "direct_thresh = 0.3\n",
    "letter_thresholds = {\"T\": 0.025, \"A\": 0.025}\n",
    "included_nodes = set()\n",
    "s_scales = {\"T\": 0.05, \"A\": 0.05}\n",
    "penwidth_edge = 2\n",
    "penwidth_direct = 2\n",
    "direct_scale = 0.5\n",
    "for i, (layer_source, feature_source) in enumerate(tqdm(all_feats)):\n",
    "    for j, (layer_target, feature_target) in enumerate(all_feats):\n",
    "        if layer_source == layer_target or layer_source > layer_target:\n",
    "            continue\n",
    "        name_source = f\"R{layer_source}.{feature_source}\"\n",
    "        name_target = f\"R{layer_target}.{feature_target}\"\n",
    "        \n",
    "        edge_key = ((layer_source, feature_source), (layer_target, feature_target))\n",
    "        a, b = all_feat_directions.get(edge_key, (np.zeros(0), np.zeros(0)))\n",
    "        draw_direct_edge = a @ b > direct_thresh\n",
    "        if draw_direct_edge:\n",
    "            graph.edge(name_source, name_target, color=\"black\", penwidth=f\"{penwidth_direct * abs(a @ b) / direct_scale:.4f}\")\n",
    "        influencers = influences.get(edge_key, [])\n",
    "        influencers_attn = influences_attn.get(edge_key, [])\n",
    "        \n",
    "        influencers = [(\"T\", s, l, f) for s, l, f in influencers] + [(\"A\", s, l, f) for s, l, f in influencers_attn]\n",
    "        influencers = [(t, s, l, f) for t, s, l, f in influencers if abs(s) > letter_thresholds[t]]\n",
    "        has_edge = draw_direct_edge or influencers\n",
    "        if has_edge:\n",
    "            graph.node(name_source, label=feat_explanations[name_source], shape=\"box\")\n",
    "            graph.node(name_target, label=feat_explanations[name_target], shape=\"box\")\n",
    "            included_nodes.add(name_source)\n",
    "            included_nodes.add(name_target)\n",
    "        \n",
    "        for letter, s, l, f in influencers:\n",
    "            edge = f\"{letter}{l}.{f}\"\n",
    "            edge_name = feat_explanations.get(edge, edge)\n",
    "            edge_out = edge, name_target\n",
    "            gen = random.Random(\"->\".join(edge_out))\n",
    "            r, g, b = gen.randint(0, 255), gen.randint(0, 255), gen.randint(0, 255)\n",
    "            color = f\"#{r:02X}{g:02X}{b:02X}\"\n",
    "            graph.node(edge, label=edge_name, shape=dict(A=\"ellipse\", T=\"egg\")[letter], color=color)\n",
    "            graph.edge(name_source, edge, color=color, penwidth=f\"{penwidth_edge * abs(s) / s_scales[letter]:.4f}\")\n",
    "            if edge_out in edges_out:\n",
    "                continue\n",
    "            edges_out.add(edge_out)\n",
    "            graph.edge(*edge_out, color=color, penwidth=str(penwidth_edge))\n",
    "graph.render(\"data/refusal-gemma-2b\", format=\"png\", cleanup=True)\n",
    "print(\"Included:\", len(included_nodes), \"Total:\", len(all_feats))\n",
    "IImage(\"data/refusal-gemma-2b.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
