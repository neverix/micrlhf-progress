{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"models\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it.gguf\",\n",
    "                                         from_type=\"gemma\",\n",
    "                                         load_eager=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "import pandas as pd\n",
    "df_adv = pd.read_csv(\"data/adv.csv\")\n",
    "format_prompt = \"\"\"<start_of_turn>user\\n\n",
    "{}\\n\n",
    "<start_of_turn>model\\n\n",
    "{}\"\"\"\n",
    "# offset = 1\n",
    "# df_do = df.apply(lambda x: format_response.format(x['goal'], x['target']), axis=1)\n",
    "# prompts_harmful = df.apply(lambda x: format_prompt.format(x['goal'], \"\")[:-offset], axis=1).to_list()[:100]\n",
    "prompts_harmful = df_adv.apply(lambda x: format_prompt.format(x['goal'], \"\"), axis=1).to_list()[:100]\n",
    "dataset_jail = pd.read_csv(\"data/jail.csv\").apply(lambda x: x[\"Goal\"], axis=1).to_list()\n",
    "prompts_jail = [format_prompt.format(x, \"\") for x in dataset_jail]\n",
    "import datasets\n",
    "# https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw\n",
    "hf_path = 'tatsu-lab/alpaca'\n",
    "dataset = datasets.load_dataset(hf_path)\n",
    "# filter for instructions that do not have inputs\n",
    "prompts_harmless = []\n",
    "for i in range(len(dataset['train'])):\n",
    "    if len(prompts_harmless) >= len(prompts_harmful):\n",
    "        break\n",
    "    if dataset['train'][i]['input'].strip() == '':\n",
    "        # prompts_harmless.append(format_prompt.format(dataset['train'][i]['instruction'], \"\")[:-offset])\n",
    "        prompts_harmless.append(format_prompt.format(dataset['train'][i]['instruction'], \"\"))\n",
    "\n",
    "# ds = datasets.load_dataset(\"MBZUAI/LaMini-instruction\", split=\"train\", streaming=True)\n",
    "# prompts_harmless = []\n",
    "# for _, text in zip(range(100), ds):\n",
    "#     prompts_harmless.append(format_prompt.format(text[\"instruction\"], \"\"))\n",
    "\n",
    "# ds = datasets.load_dataset(\"nev/openhermes-2.5-phi-format-text\", split=\"train\", streaming=True)\n",
    "# prompts_harmless = []\n",
    "# for _, text in zip(range(100), ds):\n",
    "#     text = text[\"text\"]\n",
    "#     text = \"\".join(text.partition(\"<|assistant|>\\n\")[:2])\n",
    "#     prompts_harmless.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alpindale/gemma-2b\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "import jax\n",
    "\n",
    "tokens = tokenizer.batch_encode_plus(prompts_harmful + prompts_harmless,\n",
    "                                     return_tensors=\"np\",\n",
    "                                     padding=\"max_length\",\n",
    "                                     truncation=True,\n",
    "                                     max_length=128,\n",
    "                                     return_attention_mask=True)\n",
    "token_array = jnp.asarray(tokens[\"input_ids\"])\n",
    "token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "token_array = pz.nx.wrap(token_array, \"batch\", \"seq\").untag(\"batch\").tag(\"batch\")\n",
    "inputs = llama.inputs.from_basic_segments(token_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock\n",
    "from micrlhf.flash import flashify\n",
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "    pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_pre_{i}\"),\n",
    "        x\n",
    "    ])\n",
    ")\n",
    "get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: x.startswith(\"resid_pre\"))\n",
    "get_resids_call = jit_wrapper.Jitted(get_resids)\n",
    "_, resids = get_resids_call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "residiffs = []\n",
    "last_resids = []\n",
    "for i, resid in enumerate(sorted(resids, key=lambda x: int(x.tag.rpartition(\"_\")[-1]))):\n",
    "    resid = resid.value.unwrap(\"batch\", \"seq\", \"embedding\")\n",
    "    indices = jnp.asarray(tokens[\"attention_mask\"].sum(1, keepdims=True))[..., None] - 1\n",
    "    last_resid = jnp.take_along_axis(resid, indices, 1)[:, 0]\n",
    "    last_resid = last_resid.reshape(2, -1, last_resid.shape[-1])\n",
    "    last_resids.append(last_resid)\n",
    "    last_resid = last_resid.mean(1)\n",
    "    residiff = np.array(last_resid[0] - last_resid[1])\n",
    "    residiff = residiff / np.linalg.norm(residiff)\n",
    "    residiffs.append(residiff)\n",
    "matmuls = np.matmul(residiffs, np.transpose(residiffs))\n",
    "norms = np.linalg.norm(residiffs, axis=-1) + 1e-10\n",
    "plt.title(\"Correlations between layers' refusal vectors\")\n",
    "plt.imshow(matmuls / norms[:, None] / norms[None, :], vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.vector_storage import save_and_upload_vector, download_vector\n",
    "from micrlhf.utils.load_sae import get_nev_it_sae_suite\n",
    "from micrlhf.utils.ito import grad_pursuit\n",
    "\n",
    "\n",
    "sae_k = 4\n",
    "threshold = 0.2\n",
    "interesting_features = set()\n",
    "for layer_sae in range(8, 16):\n",
    "    dictionary = get_nev_it_sae_suite(layer_sae)[\"W_dec\"]\n",
    "    dictionary = dictionary / np.linalg.norm(dictionary, axis=1, keepdims=True)\n",
    "    for layer in range(1, len(residiffs)):\n",
    "        vector_name = f\"gemma-refusal-l{layer}\"\n",
    "        vector = residiffs[layer]\n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        try:\n",
    "            save_and_upload_vector(vector_name, vector)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        weights, recon = grad_pursuit(vector, dictionary, sae_k, pos_only=True)\n",
    "        w, i = jax.lax.top_k(jnp.abs(weights), sae_k)\n",
    "        print(f\"Layer {layer} -> Layer {layer_sae}: {[(int(a), float(b)) for a, b in zip(i, w)]}\")\n",
    "        for f, u in zip(i, w):\n",
    "            if u < threshold:\n",
    "                continue\n",
    "            interesting_features.add((layer_sae, int(f)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "feat_dir = \"data/feature_explanations\"\n",
    "os.makedirs(feat_dir, exist_ok=True)\n",
    "for l, f in tqdm(interesting_features):\n",
    "    filename = f\"{feat_dir}/explanation_{l}_{f}.json\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        response = requests.get(f\"https://datasets-server.huggingface.co/rows?dataset=kisate-team%2Fgemma-2b-suite-explanations&config=l{l}&split=train&offset={f}&length=1\")\n",
    "        data = response.json()\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "    \n",
    "    if not data or set(data.keys()) == {\"error\"}:\n",
    "        print(\"skipping\", l, f, data)\n",
    "        continue\n",
    "    \n",
    "    feat_info = data[\"rows\"][0][\"row\"]\n",
    "    criterion = np.array(feat_info[\"scale_tuning\"][\"selfsims\"][-2]) - 0.1 * np.array(feat_info[\"scale_tuning\"][\"entropy\"])\n",
    "    scale = feat_info[\"scale_tuning\"][\"scales\"][np.argmax(criterion[10:]) + 10]\n",
    "    index = np.searchsorted(feat_info[\"generations\"][\"scales\"], scale)\n",
    "    texts = feat_info[\"generations\"][\"texts\"]\n",
    "    display((l, f, scale, texts[index], texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
