{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(\"models/gemma-2b-it-q4_k.gguf\", device_map=\"tpu:0\",\n",
    "                                         from_type=\"gemma\",\n",
    "                                        #  load_eager=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/gemma-2b-it-tokenizer\")\n",
    "tokenizer.padding_side = \"right\"\n",
    "format_prompt = \"\"\"<start_of_turn>user\\n\n",
    "{}\\n\n",
    "<start_of_turn>model\\n\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: Mosaic failed to compile TPU kernel: unsupported operand shapes or layouts\n\nThe MLIR operation involved:\n  %97 = \"tpu.matmul\"(%96, %95, %21) <{transpose_lhs = false, transpose_rhs = false}> : (vector<128x32xf32>, vector<32x128xbf16>, vector<128x128xf32>) -> vector<128x128xf32>\n\nPlease report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmicrlhf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample\n\u001b[0;32m----> 2\u001b[0m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/micrlhf-progress/micrlhf/sampling.py:114\u001b[0m, in \u001b[0;36msample\u001b[0;34m(llama, tokenizer, prompt, batch_size, max_seq_len, pad_token_id, do_sample, return_model, strip_padding, return_only_completion, seed, verbose, use_jit, cache_kwargs, cfg_strength, only_cache)\u001b[0m\n\u001b[1;32m    109\u001b[0m base_inputs \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39mreplace(base_inputs,\n\u001b[1;32m    110\u001b[0m                                   positions\u001b[38;5;241m=\u001b[39mbase_inputs\u001b[38;5;241m.\u001b[39mpositions \u001b[38;5;241m-\u001b[39m offsets,\n\u001b[1;32m    111\u001b[0m                                   attention_mask\u001b[38;5;241m=\u001b[39mbase_inputs\u001b[38;5;241m.\u001b[39mattention_mask \u001b[38;5;241m&\u001b[39m base_mask\u001b[38;5;241m.\u001b[39muntag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkv_seq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m                                   )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mdisable_jit(disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m use_jit):\n\u001b[0;32m--> 114\u001b[0m     logits, cache \u001b[38;5;241m=\u001b[39m \u001b[43mcall_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_cached\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg_strength \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_cfg\u001b[39m(logits):\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/compiler.py:237\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    233\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: Mosaic failed to compile TPU kernel: unsupported operand shapes or layouts\n\nThe MLIR operation involved:\n  %97 = \"tpu.matmul\"(%96, %95, %21) <{transpose_lhs = false, transpose_rhs = false}> : (vector<128x32xf32>, vector<32x128xbf16>, vector<128x128xf32>) -> vector<128x128xf32>\n\nPlease report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke\n"
     ]
    }
   ],
   "source": [
    "from micrlhf.sampling import sample\n",
    "sample(llama, tokenizer, format_prompt.format(\"Hi!\", \"\"),\n",
    "       batch_size=4, do_sample=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
