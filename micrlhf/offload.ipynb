{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax_smi\n",
    "jax_smi.initialise_tracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 2**19 + 2**18\n",
    "input_size = 2**18\n",
    "hidden_size = 2**14\n",
    "w_chunk = 2**12\n",
    "w_size = 2**20\n",
    "k = 64\n",
    "# n_dp, n_mp = 1, 4\n",
    "n_dp, n_mp = 4, 1\n",
    "input_size = input_size * n_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For chunk: 0.06397158561605819\n",
      "For task: 16.376725917710896\n"
     ]
    }
   ],
   "source": [
    "one_v4_chip_flops = 275 * 1e12  # https://cloud.google.com/tpu/docs/v4\n",
    "v4_8_flops = one_v4_chip_flops * 4\n",
    "chunk_flops = input_size * hidden_size * w_chunk * n_mp\n",
    "task_flops = input_size * hidden_size * w_size\n",
    "print(\"For chunk:\", chunk_flops / v4_8_flops)\n",
    "print(\"For task:\", task_flops / v4_8_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = np.asarray(jax.local_devices()).reshape(n_dp, n_mp)\n",
    "mesh = jax.sharding.Mesh(devices, (\"dp\", \"mp\"))\n",
    "\n",
    "cpu = jax.devices(\"cpu\")[0]\n",
    "to_cpu = lambda x: jax.device_put(x, cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b820c4ed334560b01d01d11fb717bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s, Generating weights...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from functools import partial\n",
    "\n",
    "\n",
    "# with jax.default_device(cpu):\n",
    "#     weight_dec = jnp.asarray(np.empty((w_size, hidden_size), dtype=np.uint16)).view(jnp.bfloat16)\n",
    "#     @partial(jax.jit, device=cpu, donate_argnums=(0, 1, 2))\n",
    "#     def generate_weight(weight, key, counter, scale):\n",
    "#         key, rng = jax.random.split(key)\n",
    "#         weight = jax.lax.dynamic_update_slice(weight, jax.random.uniform(rng, (w_chunk, hidden_size), dtype=jnp.bfloat16, minval=-scale, maxval=scale), (counter, 0))\n",
    "#         return weight, key, counter + w_chunk\n",
    "#     key = jax.random.key(0)\n",
    "#     counter = 0\n",
    "#     n_chunks = w_size // w_chunk // n_mp\n",
    "#     try:\n",
    "#         for _ in trange(n_chunks, postfix=\"Generating weights...\"):\n",
    "#             key, subkey = jax.random.split(key)\n",
    "#             scale = hidden_size ** -0.5\n",
    "#             for _ in range(n_mp):\n",
    "#                 weight_dec, key, counter = generate_weight(weight_dec, key, counter, scale)\n",
    "#                 # key, subkey = jax.random.split(key)\n",
    "#                 # weights_dec.append(jax.random.uniform(subkey, (w_chunk, hidden_size), dtype=jnp.bfloat16, minval=-scale, maxval=scale))\n",
    "#     except KeyboardInterrupt:\n",
    "#         pass\n",
    "#     weight_enc = weight_dec.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splatmul import matmat\n",
    "with jax.default_device(cpu):\n",
    "    weight_enc, weight_dec = map(lambda x: jnp.asarray(x).view(jnp.bfloat16), matmat(w_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "# key_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(\"dp\"))\n",
    "key_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())\n",
    "data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(\"dp\", None))\n",
    "weight_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(None, \"mp\"))\n",
    "topk_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(\"dp\", \"mp\"))\n",
    "# single_mesh = jax.sharding.Mesh(devices[:1, :], (\"dp\", \"mp\"))\n",
    "# single_sharding = jax.sharding.NamedSharding(single_mesh, jax.sharding.PartitionSpec(None, \"mp\"))\n",
    "\n",
    "def split(key):\n",
    "    a, b = jax.random.split(key)\n",
    "    return a, b\n",
    "\n",
    "def gen_inputs(input_key):\n",
    "    # input_key, subkey = split(input_key)\n",
    "    input_key, subkey = input_key + 1, input_key\n",
    "    \n",
    "    # inputs = jax.vmap(lambda x: jax.random.randint(x, (input_size // mesh.shape[\"dp\"], hidden_size), -100, 100, jnp.int8))(subkey).reshape(input_size, hidden_size)\n",
    "    # inputs = jax.random.randint(subkey, (input_size, hidden_size), -100, 100, jnp.int8)\n",
    "    # inputs = jax.random.bits(subkey, shape=(input_size, hidden_size), dtype=jnp.uint8)\n",
    "    inputs = jax.lax.broadcasted_iota(jnp.int4, (input_size, hidden_size), 0).astype(jnp.int8) + jax.lax.broadcasted_iota(jnp.int4, (input_size, hidden_size), 1).astype(jnp.int8) + subkey\n",
    "    # inputs = jnp.ones((input_size, hidden_size), jnp.int8)\n",
    "    return inputs, input_key\n",
    "gen_inputs_jit = jax.jit(gen_inputs, in_shardings=(key_sharding,), out_shardings=(data_sharding, key_sharding), donate_argnums=(0,))\n",
    "\n",
    "def matmul_topk(inputs, weight_chunk, old_weights, old_indices, offset):\n",
    "    output_chunk = inputs @ weight_chunk\n",
    "    # weights, indices = jax.lax.top_k(output_chunk, k=k)\n",
    "    # weights, indices = jax.lax.approx_max_k(output_chunk, k=k)\n",
    "    weights, indices = jax.lax.approx_max_k(output_chunk.astype(jnp.bfloat16), k=k, recall_target=0.8)\n",
    "    weights = weights / 127.5\n",
    "    indices += offset\n",
    "    # weights, indices = jax.lax.approx_max_k(output_chunk, k=k, recall_target=0.5)\n",
    "    # weights, indices = jax.lax.approx_max_k(output_chunk, k=k, recall_target=0.25)\n",
    "    \n",
    "    # _, indices = jax.lax.approx_max_k(output_chunk[:, :k * 4], k=k)\n",
    "    # _, indices = jax.lax.approx_max_k(output_chunk[:, :k], k=k)\n",
    "    # weights = output_chunk.reshape(output_chunk.shape[0], k, -1).sum(-1)\n",
    "    # weights, indices = jax.lax.top_k(output_chunk, k=k)\n",
    "    \n",
    "    if old_weights is None or old_indices is None:\n",
    "        return weights, indices\n",
    "    else:\n",
    "        # replace_mask = weights > old_weights\n",
    "        # return jnp.where(replace_mask, weights, old_weights), jnp.where(replace_mask, indices, old_indices)\n",
    "\n",
    "        new_full_weights = jnp.concatenate((weights, old_weights), axis=1)\n",
    "        new_full_indices = jnp.concatenate((indices, old_indices), axis=1)\n",
    "        \n",
    "        # over_indices = jnp.argsort(new_full_weights)[:, -k:]\n",
    "        # return jnp.take_along_axis(new_full_weights, over_indices, axis=1), jnp.take_along_axis(new_full_indices, over_indices, axis=1)\n",
    "        \n",
    "        _, over_indices = jax.lax.top_k(new_full_weights, k=k)\n",
    "        return jnp.take_along_axis(new_full_weights, over_indices, axis=1), jnp.take_along_axis(new_full_indices, over_indices, axis=1)\n",
    "\n",
    "matmul_topk_jit = jax.jit(matmul_topk, in_shardings=(data_sharding, weight_sharding, topk_sharding, topk_sharding, None), out_shardings=(topk_sharding, topk_sharding), donate_argnums=(2, 3))\n",
    "\n",
    "# weights_0_put = jax.device_put(weights_enc[0], weight_sharding)\n",
    "# send_weights = lambda _: weights_0_put\n",
    "def send_weights(weights):\n",
    "    # weights = jax.device_put(weights, single_sharding)\n",
    "    return jax.device_put(weights, weight_sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_key = jax.device_put(jax.random.split(jax.random.key(0), mesh.shape[\"dp\"]), key_sharding)\n",
    "# %time inputs, input_key = jax.block_until_ready(gen_inputs_jit(input_key))\n",
    "\n",
    "\n",
    "# for idx in trange(3):\n",
    "#     %time current_chunk = jax.block_until_ready(send_weights(weights_enc[idx]))\n",
    "#     %time inputs.block_until_ready()\n",
    "#     %time current_chunk.block_until_ready()\n",
    "#     %time weights, indices = jax.block_until_ready(matmul_topk_jit(inputs, current_chunk))\n",
    "# compiled = matmul_topk_jit.lower(inputs, current_chunk).compile()\n",
    "# estimated_chunk_flops = compiled.cost_analysis()[0]['flops']\n",
    "# print(\"Our flops estimate:\", estimated_chunk_flops / v4_8_flops)\n",
    "# print(\"Estimate from compiler:\", compiled.cost_analysis()[0][\"optimal_seconds\"])\n",
    "# for name in (\"current_chunk\", \"next_chunk\", \"weights\", \"indices\", \"inputs\"):\n",
    "#     try:\n",
    "#         del globals()[name]\n",
    "#     except KeyError:\n",
    "#         pass\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splatmul\n",
    "import gc\n",
    "\n",
    "\n",
    "def sparse_matmul(weights, indices, encoder_weight, decoder_weight, input_embeds=None, target_embeds=None):\n",
    "    with jax.default_device(cpu):\n",
    "        weights = np.asarray(weights.view(jnp.uint16))\n",
    "        indices = np.asarray(indices.astype(jnp.uint32))\n",
    "        decoder_weight = np.asarray(decoder_weight.view(jnp.uint16))\n",
    "        encoder_weight = np.asarray(encoder_weight.view(jnp.uint16))\n",
    "        result = splatmul.splatmul(weights, indices, decoder_weight)\n",
    "        gc.collect()\n",
    "        grads = None, None\n",
    "        if input_embeds is not None and target_embeds is not None:\n",
    "            input_embeds = np.asarray(input_embeds)\n",
    "            target_embeds = np.asarray(target_embeds)\n",
    "            encoder_grads, decoder_grads = splatmul.matsplat(input_embeds, target_embeds, encoder_weight, decoder_weight, weights, indices, result)\n",
    "            print(\"Got encoder grads:\", encoder_grads.shape)\n",
    "            print(\"Got decoder grads:\", decoder_grads.shape)\n",
    "            grads = (jnp.asarray(encoder_grads.view(jnp.bfloat16)), jnp.asarray(decoder_grads.view(jnp.bfloat16)))\n",
    "        gc.collect()\n",
    "        return result, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time result = sparse_matmul_jit(saved_encodings[0], saved_encodings[1], weight_dec)\n",
    "# %time result.block_until_ready();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "def sparse_matmul_async(weights, indices, past_embeds=None):\n",
    "    out_queue = Queue()\n",
    "    def worker():\n",
    "        out_queue.put(sparse_matmul(weights, indices, weight_enc, weight_dec, past_embeds, past_embeds))\n",
    "    thread = Thread(target=worker)\n",
    "    thread.start()\n",
    "    return out_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c22f3285da483f96180d3e1a3f16bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s, Measuring forward speed...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12506d10100946ffac97c81657d5b608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s, Encoder forward pass]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for encoder...\n",
      "Encoder time: 8.964083909988403\n",
      "Encoder to CPU time: 12.234130859375\n",
      "(1048576, 16384)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ac107efe414a4cab934241efda2127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s, Encoder forward pass]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decoder/grads...\n",
      "Got encoder grads: (1048576, 16384)\n",
      "Got decoder grads: (1048576, 16384)\n",
      "Decoder outputs: [[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Encoder grads: [[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 -3.8147e-06 -3.8147e-06 -3.8147e-06 -1.90735e-06]\n",
      " [-7.62939e-06 -3.8147e-06 -3.8147e-06 -3.8147e-06 0 0 0 0]\n",
      " [-9.53674e-07 2.40281e-07 9.53674e-07 9.53674e-07 1.90735e-06\n",
      "  1.90735e-06 -3.8147e-06 -3.8147e-06]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Decoder grads: [[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 -3.8147e-06 -3.8147e-06 -3.8147e-06 -1.90735e-06]\n",
      " [-7.62939e-06 -7.62939e-06 -7.62939e-06 -7.62939e-06 0 0 0 0]\n",
      " [-9.53674e-07 2.38419e-07 9.53674e-07 9.53674e-07 1.90735e-06\n",
      "  1.90735e-06 -7.62939e-06 -7.62939e-06]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Decoder time: 173.81895780563354\n",
      "Waiting for encoder...\n",
      "Encoder time: 0.0001361370086669922\n",
      "Encoder to CPU time: 11.88055944442749\n",
      "(1048576, 16384)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61dca02cf0684ac9b258780dd26b44a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s, Encoder forward pass]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m             saved_encodings \u001b[38;5;241m=\u001b[39m encodings\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtrial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 34\u001b[0m, in \u001b[0;36mtrial\u001b[0;34m(save_encodings)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m past_inputs, inputs\n\u001b[1;32m     33\u001b[0m inputs, input_key \u001b[38;5;241m=\u001b[39m gen_inputs_jit(input_key)\n\u001b[0;32m---> 34\u001b[0m weights, indices \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing decoder/grads...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 13\u001b[0m, in \u001b[0;36mmatmul_trial\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_start \u001b[38;5;129;01min\u001b[39;00m bar:\n\u001b[0;32m---> 13\u001b[0m     next_chunk \u001b[38;5;241m=\u001b[39m send_weights(\u001b[43mweight_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mchunk_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_mp\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     14\u001b[0m     weights, indices \u001b[38;5;241m=\u001b[39m matmul_topk_jit(inputs, current_chunk, weights, indices, offset)\n\u001b[1;32m     15\u001b[0m     offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m w_chunk \u001b[38;5;241m*\u001b[39m n_mp\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/array.py:355\u001b[0m, in \u001b[0;36mArrayImpl.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax_numpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rewriting_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:7841\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   7832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rewriting_take\u001b[39m(arr, idx, indices_are_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, unique_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   7833\u001b[0m                     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   7834\u001b[0m   \u001b[38;5;66;03m# Computes arr[idx].\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7838\u001b[0m   \u001b[38;5;66;03m# For simplicity of generated primitives, we call lax.dynamic_slice in the\u001b[39;00m\n\u001b[1;32m   7839\u001b[0m   \u001b[38;5;66;03m# simplest cases: i.e. non-dynamic arrays indexed with integers and slices.\u001b[39;00m\n\u001b[0;32m-> 7841\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (result \u001b[38;5;241m:=\u001b[39m \u001b[43m_attempt_rewriting_take_via_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   7842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   7844\u001b[0m   \u001b[38;5;66;03m# TODO(mattjj,dougalm): expand dynamic shape indexing support\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:7818\u001b[0m, in \u001b[0;36m_attempt_rewriting_take_via_slice\u001b[0;34m(arr, idx, mode)\u001b[0m\n\u001b[1;32m   7816\u001b[0m   int_start_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m start_indices]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   7817\u001b[0m   int_limit_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;241m+\u001b[39m s \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(int_start_indices, slice_sizes)]\n\u001b[0;32m-> 7818\u001b[0m   arr \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7819\u001b[0m \u001b[43m      \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint_start_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint_limit_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   7821\u001b[0m   \u001b[38;5;66;03m# We must be careful with dtypes because dynamic_slice requires all\u001b[39;00m\n\u001b[1;32m   7822\u001b[0m   \u001b[38;5;66;03m# start indices to have matching types.\u001b[39;00m\n\u001b[1;32m   7823\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(start_indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/lax/slicing.py:108\u001b[0m, in \u001b[0;36mslice\u001b[0;34m(operand, start_indices, limit_indices, strides)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice\u001b[39m(operand: ArrayLike, start_indices: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m     58\u001b[0m           limit_indices: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m     59\u001b[0m           strides: Sequence[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Wraps XLA's `Slice\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m  <https://www.tensorflow.org/xla/operation_semantics#slice>`_\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m  operator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    - :func:`jax.lax.dynamic_slice`\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mslice_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/core.py:429\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    427\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    428\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 429\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/core.py:433\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    432\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 433\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/core.py:939\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    937\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/lax/slicing.py:1169\u001b[0m, in \u001b[0;36m_slice_impl\u001b[0;34m(x, start_indices, limit_indices, strides)\u001b[0m\n\u001b[1;32m   1165\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mapply_primitive(\n\u001b[1;32m   1166\u001b[0m     slice_p, x, start_indices\u001b[38;5;241m=\u001b[39mstart_indices,\n\u001b[1;32m   1167\u001b[0m     limit_indices\u001b[38;5;241m=\u001b[39mlimit_indices, strides\u001b[38;5;241m=\u001b[39mstrides)\n\u001b[1;32m   1168\u001b[0m slice_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(limit_indices) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(start_indices))\n\u001b[0;32m-> 1169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_slice_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstart_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mslice_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslice_sizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.10/lib/python3.10/site-packages/jax/_src/dispatch.py:87\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "\n",
    "def matmul_trial(inputs):\n",
    "    current_chunk = send_weights(weight_enc[:w_chunk * n_mp].T)\n",
    "    weights, indices = None, None\n",
    "    # bar = trange(0, w_size, w_chunk * n_mp, postfix=f\"Encoder forward pass\")\n",
    "    bar = trange(0, w_size // 8, w_chunk * n_mp, postfix=f\"Encoder forward pass\")\n",
    "    offset = 0\n",
    "    for chunk_start in bar:\n",
    "        next_chunk = send_weights(weight_enc[chunk_start:chunk_start + w_chunk * n_mp].T)\n",
    "        weights, indices = matmul_topk_jit(inputs, current_chunk, weights, indices, offset)\n",
    "        offset += w_chunk * n_mp\n",
    "        current_chunk = next_chunk\n",
    "        gc.collect()\n",
    "    return weights, indices\n",
    "\n",
    "\n",
    "def trial(save_encodings=False):\n",
    "    global saved_encodings\n",
    "    gc.collect()\n",
    "    input_key = 0  # jax.random.split(jax.random.key(0), mesh.shape[\"dp\"])\n",
    "    encodings, inputs, decoder_outputs = None, None, None\n",
    "    for _ in trange(100, postfix=\"Measuring forward speed...\"):\n",
    "        gc.collect()\n",
    "        if encodings is not None:\n",
    "            past_inputs = jax.device_put(inputs, cpu)\n",
    "            print(past_inputs.shape)\n",
    "            decoder_outputs = sparse_matmul_async(encodings[0], encodings[1], past_embeds=past_inputs)\n",
    "            del past_inputs, inputs\n",
    "        inputs, input_key = gen_inputs_jit(input_key)\n",
    "        weights, indices = matmul_trial(inputs)\n",
    "        if decoder_outputs is not None:\n",
    "            print(\"Computing decoder/grads...\")\n",
    "            decoder_start = time.time()\n",
    "            with jax.default_device(cpu):\n",
    "                decoder_outputs, (encoder_grads, decoder_grads) = jax.block_until_ready(decoder_outputs.get())\n",
    "                print(\"Decoder outputs:\", decoder_outputs[:8, :8])\n",
    "                print(\"Encoder grads:\", encoder_grads[:8, :8])\n",
    "                print(\"Decoder grads:\", decoder_grads[:8, :8])\n",
    "            decoder_end = time.time()\n",
    "            print(\"Decoder time:\", decoder_end - decoder_start)\n",
    "            gc.collect()\n",
    "        print(\"Waiting for encoder...\")\n",
    "        before_compute = time.time()\n",
    "        weights, indices = weights.block_until_ready(), indices.block_until_ready()\n",
    "        # wi = jax.block_until_ready((weights, indices))\n",
    "        after_compute = time.time()\n",
    "        print(\"Encoder time:\", (after_compute - before_compute))\n",
    "        weights, indices = to_cpu(weights), to_cpu(indices)\n",
    "        # weights, indices = jax.block_until_ready(jax.device_put((wi), cpu))\n",
    "        after_to_cpu = time.time()\n",
    "        print(\"Encoder to CPU time:\", after_to_cpu - after_compute)\n",
    "        encodings = weights, indices\n",
    "        gc.collect()\n",
    "        if save_encodings:\n",
    "            saved_encodings = encodings\n",
    "            break\n",
    "\n",
    "trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, indices = saved_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort on CPU is slow\n",
    "# import jax.experimental.sparse\n",
    "\n",
    "\n",
    "# def sort_encodings(weights, indices):\n",
    "#     index_array = indices.ravel().astype(jnp.uint32)\n",
    "#     weight_array = weights.ravel().astype(jnp.bfloat16)\n",
    "#     indices_to_sort = jnp.argsort(index_array)\n",
    "#     sorted_indices = index_array[indices_to_sort]\n",
    "#     sorted_weights = weight_array[indices_to_sort]\n",
    "#     inverse_indices = jnp.argsort(indices_to_sort)\n",
    "#     bcoo = jax.experimental.sparse.BCOO((sorted_weights, jnp.stack((inverse_indices, sorted_indices), axis=-1).reshape(-1, 2)), shape=weights.shape)\n",
    "#     return bcoo\n",
    "\n",
    "# sort_encodings_jit = jax.jit(sort_encodings, backend=\"cpu\")\n",
    "# %time bcoo = sort_encodings_jit(weights, indices).block_until_ready();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Numba doesn't support bfloat16\n",
    "# import ml_dtypes\n",
    "# import numba as nb\n",
    "\n",
    "# index_array_np = np.asarray(indices.ravel().astype(np.uint32))\n",
    "# weight_array_np = np.asarray(weights.ravel().view(jnp.float16))\n",
    "# %time indices_to_sort = np.argsort(index_array_np);\n",
    "# %time inverse_indices = np.argsort(indices_to_sort);\n",
    "# %time sorted_indices = index_array_np[indices_to_sort];\n",
    "# %time sorted_weights = weight_array_np[indices_to_sort];\n",
    "\n",
    "# @nb.njit\n",
    "# def coo_matmul(inverse_indices, sorted_indices, sorted_weights, weights_dec):\n",
    "#     N = inverse_indices.shape[0] // k\n",
    "#     D = weights_dec.shape[1]\n",
    "#     out = np.zeros((N, D), dtype=ml_dtypes.bfloat16)\n",
    "#     for i in range(len(inverse_indices)):\n",
    "#         out[inverse_indices[i] // k] += (sorted_weights[i].view(ml_dtypes.bfloat16).astype(np.float32) * weights_dec[sorted_indices[i]].view(ml_dtypes.bfloat16).astype(np.float32))\n",
    "#     return out\n",
    "\n",
    "# %time coo_matmul(inverse_indices, sorted_indices, np.asarray(sorted_weights.view(np.float16)), np.asarray(weight_dec.view(np.float16)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time index_array_np = np.asarray(indices.ravel().astype(np.uint32))\n",
    "%time weight_array_np = np.asarray(weights.ravel().view(jnp.float16))\n",
    "%time indices_to_sort = np.argsort(index_array_np);\n",
    "%time inverse_indices = np.argsort(indices_to_sort);\n",
    "%time sorted_indices = index_array_np[indices_to_sort];\n",
    "%time sorted_weights = weight_array_np[indices_to_sort];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Too slow\n",
    "# import numba as nb\n",
    "\n",
    "# @nb.njit\n",
    "# def coo_matmul(inverse_indices, sorted_indices, sorted_weights, weights_dec):\n",
    "#     N = inverse_indices.shape[0] // k\n",
    "#     D = weights_dec.shape[1]\n",
    "#     out = np.zeros((N, D), dtype=np.float32)\n",
    "#     for i in range(len(inverse_indices)):\n",
    "#         out[inverse_indices[i] // k] += (sorted_weights[i] * weights_dec[sorted_indices[i]])\n",
    "#     return out\n",
    "\n",
    "# %time result = coo_matmul(inverse_indices, sorted_indices, np.asarray(sorted_weights.view(jnp.bfloat16).astype(np.float32)), np.asarray(weight_dec.view(jnp.bfloat16).astype(np.float32)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "@nb.njit\n",
    "def to_bf16(x):\n",
    "    x = x.view(np.uint32)\n",
    "    conv_lowbit = x >> 16 + (x & (1 << 15))\n",
    "    return conv_lowbit.astype(np.uint16)\n",
    "\n",
    "@nb.njit\n",
    "def from_bf16(x):\n",
    "    x = x.astype(np.uint32) << 16\n",
    "    return np.ascontiguousarray(x.view(np.float32)[..., ::2])\n",
    "    # return x.view(np.float32)\n",
    "    # return (x.astype(np.uint32) << 16).view(np.float32)\n",
    "\n",
    "print(to_bf16(np.array([200], dtype=np.float32)))\n",
    "print(from_bf16(np.array([[12, 13, 14], [15, 16, 17]], dtype=np.uint16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Too slow\n",
    "# @nb.njit\n",
    "# def coo_matmul(inverse_indices, sorted_indices, sorted_weights, weights_dec):\n",
    "#     N = inverse_indices.shape[0] // k\n",
    "#     D = weights_dec.shape[1]\n",
    "#     out = np.zeros((N, D), dtype=np.uint16)\n",
    "#     for i in range(len(inverse_indices)):\n",
    "#         out[inverse_indices[i] // k] = to_bf16(from_bf16(out[inverse_indices[i] // k]) + (sorted_weights[i] * from_bf16(weights_dec[sorted_indices[i]])))\n",
    "#     return out\n",
    "\n",
    "# %time result = coo_matmul(inverse_indices, sorted_indices, np.asarray(sorted_weights.view(jnp.uint16)), np.asarray(weight_dec.view(jnp.uint16)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba_progress import ProgressBar\n",
    "%env OPENBLAS_NUM_THREADS=64\n",
    "\n",
    "# @nb.njit(parallel=True)\n",
    "# def sparse_matmul(indices, weights, weights_dec):\n",
    "#     out = np.zeros((indices.shape[0], weights_dec.shape[1]), dtype=np.uint16)\n",
    "#     for i in nb.prange(indices.shape[0]):\n",
    "#         out[i] = to_bf16(from_bf16(weights[i]) @ from_bf16(weights_dec[indices[i]]))\n",
    "#     return out\n",
    "\n",
    "\n",
    "@nb.njit(nogil=True, parallel=True)\n",
    "# def sparse_matmul(indices, weights, weights_dec, bar):\n",
    "def sparse_matmul(indices, weights, weights_dec):\n",
    "    out = np.zeros((indices.shape[0], weights_dec.shape[1]), dtype=np.uint16)\n",
    "    for i in nb.prange(indices.shape[0]):\n",
    "        out[i] = to_bf16(from_bf16(weights[i]) @ from_bf16(weights_dec[indices[i]]))\n",
    "        # bar.update(1)\n",
    "    # chunk = 64\n",
    "    # for i_ in range(0, indices.shape[0], chunk):\n",
    "        # for i in nb.prange(i_, min(i_ + chunk, indices.shape[0])):\n",
    "        #     out[i] = to_bf16(from_bf16(weights[i]) @ from_bf16(weights_dec[indices[i]]))\n",
    "        # bar.update(chunk)\n",
    "    # return out\n",
    "\n",
    "%time indices_np = np.asarray(indices.astype(np.uint32))\n",
    "%time weights_np = np.asarray(weights.view(np.uint16))\n",
    "%time weight_dec_np = np.asarray(weight_dec.view(np.uint16))\n",
    "# with ProgressBar(total=indices_np.shape[0]) as bar:\n",
    "#     result = sparse_matmul(indices_np, weights_np, weight_dec_np, bar)\n",
    "%time result = sparse_matmul(indices_np, weights_np, weight_dec_np);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time bcoo = jax.experimental.sparse.BCOO((jnp.asarray(sorted_weights).view(jnp.bfloat16), jnp.stack((jnp.asarray(inverse_indices // k), jnp.asarray(sorted_indices)), axis=-1).reshape(-1, 2)), shape=(weights.shape[0], weight_dec.shape[0])).block_until_ready();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax.experimental.sparse\n",
    "\n",
    "\n",
    "# chunk_size = 2**12\n",
    "# def sparse_matmul_cpu(row_indices, col_indices, weights, weight_dec):\n",
    "#     def sparse_matmul_cpu_chunk(carry, chunk):\n",
    "#         row_ind, col_ind, w = chunk\n",
    "#         bcoo = jax.experimental.sparse.BCOO((w, jnp.stack((row_ind, col_ind), axis=-1).reshape(-1, 2)), shape=(weights.shape[0], weight_dec.shape[0]))\n",
    "#         return carry + bcoo @ weight_dec, None\n",
    "#     out = jnp.zeros((weights.shape[0], weight_dec.shape[1]), jnp.bfloat16)\n",
    "#     return jax.lax.scan(sparse_matmul_cpu_chunk, out, tuple(x.reshape(-1, chunk_size) for x in (row_indices, col_indices, weights)))[0]\n",
    "\n",
    "# sparse_matmul_cpu_jit = jax.jit(sparse_matmul_cpu, device=cpu)\n",
    "# with jax.default_device(cpu):\n",
    "#     %time result = sparse_matmul_cpu_jit(jnp.asarray(inverse_indices // k), jnp.asarray(sorted_indices), jnp.asarray(sorted_weights).view(jnp.bfloat16), weight_dec).block_until_ready();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax.experimental.sparse\n",
    "\n",
    "\n",
    "# chunk_size = 2**12\n",
    "# def sparse_matmul_cpu(row_ind, col_ind, w, weight_dec):\n",
    "#     bcoo = jax.experimental.sparse.BCOO((w, jnp.stack((row_ind, col_ind), axis=-1).reshape(-1, 2)), shape=(weights.shape[0], weight_dec.shape[0]))\n",
    "#     return bcoo @ weight_dec\n",
    "\n",
    "# sparse_matmul_cpu_jit = jax.jit(sparse_matmul_cpu, device=cpu)\n",
    "# with jax.default_device(cpu):\n",
    "#     out = 0\n",
    "#     for i in trange(0, len(sorted_indices), chunk_size):\n",
    "#         out += sparse_matmul_cpu_jit(jnp.asarray(inverse_indices[i:i+chunk_size] // k), jnp.asarray(sorted_indices[i:i+chunk_size]), jnp.asarray(sorted_weights[i:i+chunk_size]).view(jnp.bfloat16), weight_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import trange\n",
    "# masks = []\n",
    "# chunked_weights = []\n",
    "# chunked_indices = []\n",
    "# for i in trange(0, w_size, w_chunk):\n",
    "#     mask = (i < index_array_np) & (index_array_np < (i + w_chunk))\n",
    "#     chunked_indices.append(index_array_np[mask] - i)\n",
    "#     chunked_weights.append(weight_array_np[mask])\n",
    "#     masks.append(np.nonzero(mask)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.hist([x.shape[0] for x in chunked_indices])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_chunk = 2**12\n",
    "for idx in trange(0, w_size, b_chunk):\n",
    "    sparse_matmul_out = sparse_matmul_jit(*(u[idx:idx + b_chunk] for u in encback), weight_dec)\n",
    "    del sparse_matmul_out\n",
    "    gc.collect()\n",
    "# %time sparse_matmul_out = sparse_matmul_jit(*encback, weight_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encback[0].shape, encback[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dec[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
