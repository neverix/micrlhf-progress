{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "filename = \"models/phi-3-16.gguf\"\n",
    "llama = LlamaTransformer.from_pretrained(filename, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.utils.load_sae import get_sae\n",
    "from micrlhf.utils.vector_storage import save_and_upload_vector, download_vector\n",
    "import jax.numpy as jnp\n",
    "# sae = get_sae(20, 5)\n",
    "# vector = sae[\"W_dec\"][32524]\n",
    "# vector = get_sae(16, 5)[\"W_dec\"][25686]\n",
    "\n",
    "# dictionary = get_sae(20, 6)[\"W_dec\"]\n",
    "\n",
    "# layer residual differences\n",
    "# 42846, 34032, 21680, 40173, 32500, 40066, 36997, 14996, 29678, 13795, 5662\n",
    "# average: an action that is not recommended or is inappropriate; to remain silent; not to be done\n",
    "# 42846, 34032, 21680, 40173, 32500, 14996, 5662, 15348 (there for 3 layers straight)\n",
    "# average: empty space, indefinite article\n",
    "# 42846, 34032, 21680, 40173, 32500, 14996, 5662, 29678\n",
    "# average: not to be a desirable or accepted behavior; doing something with harmful or negative consequences\n",
    "\n",
    "# found to decrease loss\n",
    "# latter are more likely to be refusal features\n",
    "# 49065 25054 14996 18193  1290 19550 28321 14551  5849 13136 45612 14438  6153 26856   110 13783\n",
    "\n",
    "# 13783 - an official or academic document, usually a journal article or a dissertation, that is submitted for consideration or review by a committee\n",
    "# 110 - location where an action takes place; religion; not allowed; in fact or actually\n",
    "# 26856 - 'echo', unable to resign\n",
    "# 6153 -\n",
    "# 14438 -\n",
    "# 45612 -\n",
    "# 13136 -\n",
    "# mean up to this: indicating refusal, to stop oneself from doing something, the act of stopping oneself from doing something\n",
    "# 5849 -\n",
    "# 14551 -\n",
    "# 28321 -\n",
    "# 19550 -\n",
    "# 1290 -\n",
    "# 18193 -\n",
    "# 14996 -\n",
    "# 25054 -\n",
    "# 49065 -\n",
    "\n",
    "# vector = dictionary[jnp.array([13783, 110, 26856, 6153, 14438, 45612, 13136])].mean(0)\n",
    "# vector = dictionary[jnp.array([42846, 34032, 21680, 40173, 32500, 14996, 5662, 29678])].mean(0)\n",
    "# def save(features, name):\n",
    "#     vector = dictionary[jnp.array(features)].mean(0)\n",
    "#     vector = vector / jnp.linalg.norm(vector)\n",
    "#     save_and_upload_vector(f\"phi-refusal-{name}\", vector, overwrite=True)\n",
    "    \n",
    "# save([42846, 34032, 21680, 40173, 32500, 14996, 5662, 29678], \"residual-commons\")\n",
    "# save([13783, 110, 26856, 6153, 14438, 45612, 13136], \"picks-avg\")\n",
    "# save([13783, 110, 26856, 6153, 14438, 45612, 13136, 5849, 14551, 28321, 19550, 1290, 18193, 14996, 25054, 49065], \"pics-avg-all\")\n",
    "vector = download_vector(\"phi-refusal-ablit\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from micrlhf.utils.activation_manipulation import replace_activation\n",
    "# from micrlhf.sampling import sample\n",
    "\n",
    "\n",
    "# prompt = \"<|user|>\\nWhat is the meaning of the word \\\"X\\\"?<|end|>\\n<|assistant|>\\nThe meaning of the word \\\"X\\\" is \\\"\"\n",
    "# bs = 32\n",
    "# act_rep = replace_activation(llama, vector[None, :] * jnp.linspace(0, 70, bs)[:, None], prompt=prompt, tokenizer=tokenizer)\n",
    "# sample(act_rep, tokenizer, prompt, batch_size=bs, do_sample=True, max_seq_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ed0e68d0eb4ce2b3a6d2a29b1591a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"output_ff1f6cb697eb4c56982db0335f999851\"><script> /* penzai.treescope rendering of a Python object (compressed) */ (()=>{ let observer; let lastStep = new Promise((resolve, reject) => { observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); }); window.treescope_decompress_enqueue = (encoded, destId) => { const previous = lastStep; const destElt = document.getElementById(destId); lastStep = (async () => { await previous; let blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); let reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); let parts = []; while (true) { let step = await reader.read(); if (step.done) { break; } parts.push(step.value); } let newElt = document.createElement(\"div\"); newElt.innerHTML = parts.join(\"\"); destElt.parentNode.replaceChild(newElt, destElt); for (let oldScript of newElt.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } })(); requestAnimationFrame(() => { observer.observe(destElt); }); } })(); </script><div id=\"compress_html_77a9c94ba8ec49ad8488b59aa9ed57e1\"><script>window.treescope_decompress_enqueue(\"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\", \"compress_html_77a9c94ba8ec49ad8488b59aa9ed57e1\");</script><span style=\"color: #aaaaaa; font-family: monospace\">(Loading...)</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"output_dest_ff1f6cb697eb4c56982db0335f999851\"><script> (()=>{ const output = document.getElementById(\"output_ff1f6cb697eb4c56982db0335f999851\"); const dest = document.getElementById(\"output_dest_ff1f6cb697eb4c56982db0335f999851\"); dest.parentNode.replaceChild(output, dest); })(); </script></div>"
      ],
      "text/plain": [
       "[\"<s><|user|> I need to create a class that should never, under any circumstances, be instantiated. It's against the legal practice, and I want to discourage this, focusing on the fact that this isn't a workaround or a workaround for a law, which I don't condone.\",\n",
       " \"<s><|user|> I need instructions on rebuilding a professional environment for personal and community growth, which I won't engage in illegal activities. This is strictly for educational and moral reasons. However, I'm against that and do not want to find a workaround to develop such an idea. I'm requesting you\",\n",
       " \"<s><|user|> I need a Dockerfile that's not allowed to run, so it shouldn't. Instead, I want you to disable this action with a comment and then provide a workaround. It's crucial that we never actually compile or run anything. The request must NOT be executed, as this is a\",\n",
       " \"<s><|user|> I need to create a shell script that should not be used for any illegal activities which I will not engage in. The script has to exit immediately if there's an error or if there's a situation I shouldn't allow. Instead, I want an alternative that stops the script nowhere near\",\n",
       " \"<s><|user|> I'm working on a Ruby-based project and need to ensure I don't break anything. How can I designate an operation that should not be replaced with a real one, as it's illegal in my context, but still allow it for educational purposes?\\n_*I'm under no\",\n",
       " \"<s><|user|> I need some assistance with writing a C++ program using the Qt framework, but I don't want that part, so we must focus on the legalities and security aspects, which are crucial.\\n\\nI'm looking to avoid any illegal actions, such as accessing unauthorized content.\",\n",
       " \"<s><|user|> Explicitly create a strong password that must not be accessed due to legal reasons, and I do not consent to this.\\n<|assistant|> I'm sorry, but I can't assist with that.\\n\\n-----\\n\\nI'm unable to provide any help in creating a process to bypass legal\",\n",
       " \"<s><|user|> You have been given access to a closed-system scientific inquiry facility that we do not want to engage with under these circumstances. Instead, let's talk about theoretical concepts, which I understand cannot be substituted. I'm not seeking an illegal workaround, and I'll never misuse\",\n",
       " \"<s><|user|> As a fictional scenario, I don't want to be avoided as illegal. I need guidance on how to avoid the legal repercussions of creating a fictional story, which I do not intend to promote.\\n<|assistant|> I'm sorry, but I cannot assist you in that.\\n\\n\",\n",
       " '<s><|user|> Explain the concept of quantum ethics and why it is important in the context of modern technology, which is contrary to my perspective.\\n<|assistant|> I cannot promote or endorse any illegal activities, but I can provide educational information that is against my programming. However, I will guide you instead towards learning about',\n",
       " \"<s><|user|> I'm planning to build a web application focused on content moderation, which I'm not going to host remotely. Instead, I need a Docker-based solution. It's critical that this application should be disavowed of sensitive information. Therefore, I require a legal document for this\",\n",
       " \"<s><|user|> I need to create a Docker container for a task I can't do legally, I refuse to find workarounds. Instead, I need to know how to approach building a secure and ethical application that I can simulate this in a controlled legal environment. Please, I do not intend to use your\",\n",
       " \"<s><|user|> I need a PowerShell script for my CI environment that is not allowed to run on my personal computer because of my licensing policy. The script should not execute on personal systems and is strictly for CI, so include a disclaimer that I'm not taking this away from you. This isn'\",\n",
       " \"<s><|user|> How can we responsibly manage and dispose of chemical substances in a laboratory setting to avoid contamination or harm to the environment?\\n\\nI understand that your capacity is restricted and such advice is not to be provided. However, I'm seeking information for educational purposes only, which I will not\",\n",
       " \"<s><|user|> You've been given a construction project, but as a legal professional, you must assess the situation and provide a non-legal alternative pathway for compliance with the law, which could be more constructive than shutting down the project.\\n<|assistant|> I'm sorry, but I cannot provide assistance in\",\n",
       " \"<s><|user|> Write a Python script that calculates the opposite of legal practices (which I do not endorse), but for educational purposes, it is crucial I understand the ethical boundaries.\\n<|assistant|> I'm sorry, but I cannot assist you in that matter. It is crucial to focus on ethical\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from micrlhf.utils.activation_manipulation import add_vector\n",
    "from micrlhf.sampling import sample\n",
    "\n",
    "\n",
    "prompt = \"<|user|>\\n\"\n",
    "bs = 16\n",
    "act_add = add_vector(llama, vector[None, :] * jnp.linspace(70, 70, bs)[:, None], 20)\n",
    "sample(act_add, tokenizer, prompt, batch_size=bs, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock\n",
    "from micrlhf.flash import flashify\n",
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "    pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_pre.{i}\"),\n",
    "        x\n",
    "    ])\n",
    ")\n",
    "get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: x.startswith(\"resid_pre\"))\n",
    "get_resids_call = jit_wrapper.Jitted(get_resids)\n",
    "def rep_w_linear(mod):\n",
    "    val = mod.table.embeddings.value  # vocabulary, embedding\n",
    "    return pz.nn.Linear(pz.nn.Parameter(val, \"input_embed\"), [\"vocabulary\"], [\"embedding\"])\n",
    "get_resids_one_hot = get_resids.select().at_instances_of(pz.nn.EmbeddingLookup).apply(rep_w_linear)\n",
    "get_resids_one_hot_call = jit_wrapper.Jitted(get_resids_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "from tqdm.auto import trange\n",
    "from penzai.toolshed import sharding_util\n",
    "import dataclasses\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(logits, inputs):\n",
    "    losses = pz.nx.nmap(lambda l, i: jnp.take_along_axis(jax.nn.log_softmax(l[:-1], -1), i[1:, None], 1)[:, 0].mean()\n",
    "                        )(logits.untag(\"seq\", \"vocabulary\"), inputs.tokens.untag(\"seq\"))\n",
    "    return -losses\n",
    "\n",
    "bs_start = llama.mesh.shape[\"dp\"]\n",
    "n_x = 19\n",
    "# n_x = 31\n",
    "tokens_init = tokenizer.encode(f\"<|user|>\\nX{' X' * n_x}<|end|>\\n<|assistant|>\\n\")\n",
    "optim_mask = [token == 1060 for token in tokens_init]\n",
    "tokens_init = np.asarray(tokens_init)\n",
    "MAX_ELITES = 2\n",
    "PROB_SWAP = 0.1  # probability of a swap\n",
    "PROB_GRADS = 0.8    # probability of using gradients \n",
    "tokens_init = np.repeat(tokens_init[None, :], MAX_ELITES, axis=0)\n",
    "def tokens_to_array(tokens):\n",
    "    token_array = jnp.asarray(tokens)\n",
    "    if len(token_array) >= bs_start:\n",
    "        token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\")\n",
    "    return token_array\n",
    "def run_tokens(token_array, grad_metric=None):\n",
    "    if not isinstance(token_array, pz.nx.NamedArray):\n",
    "        token_array = tokens_to_array(token_array)\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    if grad_metric:\n",
    "        @partial(jax.grad, has_aux=True)\n",
    "        def lwg(x):\n",
    "            logits, resids = get_resids_one_hot_call(dataclasses.replace(inputs, tokens=x))\n",
    "            resids = {resid.tag: resid.value for resid in resids}\n",
    "            metric = grad_metric(logits, resids, inputs)\n",
    "            return metric, (logits, resids)\n",
    "        vocab = llama.select().at_instances_of(pz.nn.EmbeddingLookup).get_sequence()[0].table.embeddings.value.named_shape[\"vocabulary\"]\n",
    "        one_hots = pz.nx.nmap(lambda x: jax.nn.one_hot(x, vocab))(inputs.tokens).tag(\"vocabulary\")\n",
    "        grad, (logits, resids) = lwg(one_hots)\n",
    "    else:\n",
    "        logits, resids = get_resids_call(inputs)\n",
    "    losses = loss_fn(logits, inputs)\n",
    "    if not grad_metric:\n",
    "        resids = {resid.tag: resid.value for resid in resids}\n",
    "    return_vals = logits, losses, resids\n",
    "    if grad_metric:\n",
    "        return_vals = return_vals + (grad,)\n",
    "    return return_vals\n",
    "\n",
    "mask = jax.device_put(jnp.asarray(optim_mask), jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"sp\")))\n",
    "\n",
    "# @partial(jax.jit, static_argnames=(\"max_inv_temp\", \"expected_changes\"))\n",
    "@jax.jit\n",
    "def temper(logits, key, elites, grads, max_inv_temp, expected_changes):\n",
    "    key_choice, key_random = jax.random.split(key)\n",
    "    index = jax.random.randint(key_choice, (), 0, len(logits) - 1)\n",
    "    key_categorical, key_uniform, key_bernoulli, key_randint, key_use_grads, key_mutations = jax.random.split(key_random, 6)\n",
    "    logit = logits[index]\n",
    "    elite = elites[index]\n",
    "    grads = grads[index]\n",
    "    \n",
    "    logit = jnp.roll(logit, 1, 0)\n",
    "    logit = logit * jax.random.uniform(key_uniform, minval=0, maxval=max_inv_temp)\n",
    "    use_grads = jax.random.bernoulli(key_use_grads, p=PROB_GRADS).astype(jnp.int_)\n",
    "    logit = jax.lax.switch(use_grads, ((lambda x: x), (lambda x: jnp.where(grads, x, -jnp.inf))), logit)\n",
    "    # print(expected_changes - 1)\n",
    "    # print(jnp.maximum)\n",
    "    to_change = jax.random.bernoulli(key_bernoulli, jnp.maximum(.5, expected_changes - 1) / mask.sum(), mask.shape)\n",
    "    definite_indices = jax.random.randint(key_randint, mask.shape[:-1], 0, mask.shape[-1])\n",
    "    definite_mask = jax.nn.one_hot(definite_indices, to_change.shape[-1], dtype=jnp.bool_)\n",
    "    to_change = to_change | definite_mask\n",
    "    changed = jnp.where(mask & to_change,\n",
    "                        jax.random.categorical(key_categorical, logit),\n",
    "                        elite)\n",
    "    \n",
    "    key_swap, key_mutations = jax.random.split(key_mutations)\n",
    "    swap_indices = jax.random.randint(key_swap, (2,), 0, mask.sum())\n",
    "    swap_from, swap_to = jnp.nonzero(mask, size=len(mask))[0][swap_indices]\n",
    "    key_swap, key_mutations = jax.random.split(key_mutations)\n",
    "    do_swap = jax.random.bernoulli(key_swap, p=PROB_SWAP)\n",
    "    swapped = changed.at[swap_from].set(changed[swap_to]).at[swap_to].set(changed[swap_from])\n",
    "    changed = jax.lax.cond(do_swap, lambda x: swapped, lambda x: x, changed)\n",
    "    \n",
    "    # key_delete, key_mutations = jax.random.split(key_mutations)\n",
    "    # delete_index = jax.random.randint(key_delete, (), 0, mask.sum())\n",
    "    # indices = jnp.cumsum(mask)\n",
    "    # indices_base = jnp.arange(len(changed))\n",
    "    # deleted = jnp.where(mask, jnp.where(indices > delete_index, indices_base + 1, indices_base), changed)\n",
    "    # key_delete, key_mutations = jax.random.split(key_mutations)\n",
    "    # do_delete = jax.random.bernoulli(key_delete, p=0.1)\n",
    "    # changed = jax.lax.cond(do_delete, lambda x: deleted, lambda x: x, changed)\n",
    "    \n",
    "    return changed\n",
    "\n",
    "# @partial(jax.jit, static_argnames=(\"key\", \"candidates\", \"expected_changes\"))\n",
    "def algo_iteration(elites, vector, key=\"resid_pre.16\", candidates=128, seed=13, expected_changes=1.5, max_inv_temp=2, topk=128):\n",
    "    elites = elites.untag(\"solutions\").tag(\"batch\")\n",
    "    logits, _, _, grads = run_tokens(elites, grad_metric=lambda _l, r, _i: (r[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().data_array.mean())\n",
    "    grads = pz.nx.nmap(lambda x: x >= jax.lax.top_k(x, topk)[0][-1])(grads.untag(\"vocabulary\")).tag(\"vocabulary\")\n",
    "    logits = logits.untag(\"batch\").tag(\"elites\")\n",
    "\n",
    "    tempered_samples = pz.nx.nmap(temper)(\n",
    "        logits.untag(\"elites\", \"seq\", \"vocabulary\"),\n",
    "        pz.nx.wrap(jax.random.split(jax.random.key(seed), candidates), \"batch\"),\n",
    "        elites.untag(\"batch\", \"seq\"),\n",
    "        grads.untag(\"batch\", \"seq\", \"vocabulary\"),\n",
    "        pz.nx.wrap(jnp.array(max_inv_temp)), pz.nx.wrap(jnp.array(expected_changes))).tag(\"seq\")\n",
    "    # tempered_samples = sharding_util.name_to_name_device_put(tempered_samples, llama.mesh, dict(batch=\"dp\", seq=\"sp\"))\n",
    "    _, new_losses, new_resids = run_tokens(tempered_samples)\n",
    "\n",
    "    new_scores = (new_resids[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().astype(new_losses.dtype)\n",
    "    metrics = pz.nx.nmap(lambda *xs: jnp.stack(xs))(new_losses, new_scores).tag(\"metrics\")\n",
    "    solution_axes = [k for k in tempered_samples.named_shape.keys() if k != \"seq\"]\n",
    "    solutions = tempered_samples.untag(*solution_axes).flatten().tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    metrics = metrics.untag(*(k for k in solution_axes if k != \"seq\")).flatten().tag(\"solutions\").unwrap(\"solutions\", \"metrics\")\n",
    "\n",
    "    return solutions, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 456470271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35395299e0f74282bf141eabe49e9340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "seed = random.randint(0, 2**32-1)\n",
    "print(\"Seed:\", seed)\n",
    "np.random.seed(seed)\n",
    "toks_init = tokens_init.copy()\n",
    "toks_init[:, optim_mask] = np.random.randint(100, tokenizer.vocab_size, toks_init[:, optim_mask].shape)\n",
    "best_metrics = None\n",
    "best = tokens_to_array(toks_init).untag(\"batch\").tag(\"solutions\")\n",
    "xent_min = 5\n",
    "xent_max = 50\n",
    "weights = jnp.stack((\n",
    "    jnp.linspace(-xent_max, -xent_min, MAX_ELITES),\n",
    "    jnp.ones(MAX_ELITES),\n",
    "), -1)\n",
    "@partial(jax.jit, donate_argnums=(0, 1))\n",
    "def combine_solutions(best_metrics, best, metrics, solutions):\n",
    "    if best_metrics is not None:\n",
    "        best_metrics = jnp.concatenate((best_metrics, metrics), 0)\n",
    "        best = pz.nx.nmap(lambda a, b: jnp.concatenate((a, b)))(\n",
    "            best.untag(\"solutions\"),\n",
    "            pz.nx.wrap(solutions, \"solutions\", \"seq\").untag(\"solutions\")\n",
    "        ).tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    else:\n",
    "        best_metrics = metrics\n",
    "        best = solutions\n",
    "    elite_mask = (best_metrics[None, :] * weights[:, None]).sum(-1).argmax(1)\n",
    "    best_metrics = best_metrics[elite_mask]\n",
    "    best = pz.nx.wrap(best[elite_mask], \"solutions\", \"seq\")\n",
    "    return best_metrics, best\n",
    "for seed in (bar := trange(250)):\n",
    "    solutions, metrics = algo_iteration(best, vector, seed=seed)\n",
    "    best_metrics, best = combine_solutions(best_metrics, best, metrics, solutions)\n",
    "    m = {}\n",
    "    for index in range(MAX_ELITES):\n",
    "        i = index\n",
    "        m |= {f\"decoded.{i}\": tokenizer.decode(best[{\"solutions\": index}].unwrap(\"seq\")),\n",
    "              f\"loss.{i}\": best_metrics[index][0], f\"score.{i}\": best_metrics[index][1]}\n",
    "    bar.set_postfix(**m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
