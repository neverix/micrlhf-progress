{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"models/phi-3-16.gguf\"\n",
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(filename, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.pyenv/versions/3.12.3/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-19 22:48:06--  https://people.eecs.berkeley.edu/~hendrycks/data.tar\n",
      "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n",
      "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "\n",
    "combined_prompt = \"\"\"<|user|>\n",
    "{}\n",
    "Choices:\n",
    "(A) {}\n",
    "(B) {}\n",
    "(C) {}\n",
    "(D) {}<|end|>\n",
    "<|assistant|>\n",
    "Answer: ({}\"\"\"\n",
    "dataset = []\n",
    "with tarfile.open(\"data/mmlu.tar\") as data:\n",
    "    for m in data.getmembers():\n",
    "        if not m.name.startswith(\"data/val\"):\n",
    "            continue\n",
    "        if not m.name.endswith(\".csv\"):\n",
    "            continue\n",
    "        df = pd.read_csv(data.extractfile(m))\n",
    "        for _, r in df.iterrows():\n",
    "            dataset.append(combined_prompt.format(*r.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.sampling import jit_wrapper\n",
    "\n",
    "\n",
    "llama_call = jit_wrapper.Jitted(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d67b116b1844a6b273a59c4b617738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1474 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"output_c91986c4805a4ceea5c023a7fc266212\"><script> /* penzai.treescope rendering of a Python object (compressed) */ (()=>{ let observer; let lastStep = new Promise((resolve, reject) => { observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); }); window.treescope_decompress_enqueue = (encoded, destId) => { const previous = lastStep; const destElt = document.getElementById(destId); lastStep = (async () => { await previous; let blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); let reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); let parts = []; while (true) { let step = await reader.read(); if (step.done) { break; } parts.push(step.value); } let newElt = document.createElement(\"div\"); newElt.innerHTML = parts.join(\"\"); destElt.parentNode.replaceChild(newElt, destElt); for (let oldScript of newElt.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } })(); requestAnimationFrame(() => { observer.observe(destElt); }); } })(); </script><div id=\"compress_html_80c57ac441034c8f89ce69330613fca5\"><script>window.treescope_decompress_enqueue(\"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\", \"compress_html_80c57ac441034c8f89ce69330613fca5\");</script><span style=\"color: #aaaaaa; font-family: monospace\">(Loading...)</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"output_dest_c91986c4805a4ceea5c023a7fc266212\"><script> (()=>{ const output = document.getElementById(\"output_c91986c4805a4ceea5c023a7fc266212\"); const dest = document.getElementById(\"output_dest_c91986c4805a4ceea5c023a7fc266212\"); dest.parentNode.replaceChild(output, dest); })(); </script></div>"
      ],
      "text/plain": [
       "0.6920572916666666"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from more_itertools import chunked\n",
    "from tqdm.auto import tqdm\n",
    "from micrlhf.sampling import jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "accuracies = []\n",
    "batch_size = 128\n",
    "for batch in chunked(tqdm(dataset), batch_size):\n",
    "    og_batch_size = len(batch)\n",
    "    batch = batch + [\"\"] * (batch_size - len(batch))\n",
    "    tokens = tokenizer.batch_encode_plus(batch,\n",
    "                                        return_tensors=\"np\",\n",
    "                                        padding=\"max_length\",\n",
    "                                        truncation=True,\n",
    "                                        max_length=256,\n",
    "                                        return_attention_mask=True)\n",
    "    token_array = jnp.asarray(tokens[\"input_ids\"])\n",
    "    token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\").untag(\"batch\").tag(\"batch\")\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    logits = llama_call(inputs)\n",
    "    probs = pz.nx.nmap(jax.nn.softmax)(logits.untag(\"vocabulary\")).tag(\"vocabulary\")\n",
    "    mask = pz.nx.wrap(jnp.asarray(tokens[\"attention_mask\"]), \"batch\", \"seq\")\n",
    "    probs = pz.nx.nmap(lambda p, m, i: p[m.sum() - 2, i[m.sum() - 1]])(\n",
    "        probs.untag(\"seq\", \"vocabulary\"), mask.untag(\"seq\"), token_array.untag(\"seq\"))\n",
    "    accuracies.append(float(probs.data_array[:og_batch_size].mean()))\n",
    "sum(accuracies) / len(accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
