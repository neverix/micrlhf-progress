{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!sudo rm /tmp/libtpu_lockfile\n",
    "import penzai\n",
    "from penzai import pz\n",
    "import jax.numpy as jnp\n",
    "import jax_smi\n",
    "import jax\n",
    "jax_smi.initialise_tracking()\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "from micrlhf.scan import sequential_to_scan\n",
    "from micrlhf.flash import flashify\n",
    "\n",
    "import gc\n",
    "llama = LlamaTransformer.from_pretrained((\"models/llama-3-70b-1.gguf\", \"models/llama-3-70b-2.gguf\"),\n",
    "                                         device_map=\"auto:mp=4\", load_on_cpu=True)\n",
    "llama = sequential_to_scan(llama)\n",
    "# llama = flashify(llama)\n",
    "llama = llama.to_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama3-ChatQA-1.5-70B\")\n",
    "prompt = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "] * 1_000, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from penzai.toolshed import jit_wrapper\n",
    "from micrlhf.flash import flashify\n",
    "tokens = pz.nx.wrap([(tokenizer.encode(prompt))[:128]] * 128, \"batch\", \"seq\")\n",
    "inputs = llama.inputs.from_basic_segments(tokens)\n",
    "# llama_jitted = jit_wrapper.Jitted(flashify(llama))\n",
    "llama_jitted = jit_wrapper.Jitted(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 36.51G of 30.75G hbm. Exceeded hbm capacity by 5.76G.\n\nTotal hbm usage >= 37.76G:\n    reserved          1.25G \n    program          15.66G \n    arguments        20.85G \n\nOutput size 512B; shares 0B with arguments.\n\nProgram hbm requirement 15.66G:\n    global           144.5K\n    scoped            1.86M\n    HLO temp         15.66G (99.2% utilization: Unpadded (15.53G) Padded (15.66G), 0.0% fragmentation (341.0K))\n\n  Largest program allocations in hbm:\n\n  1. Size: 7.83G\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/5/EmbeddingDecode/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=bfloat16]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/core/named_axes.py\" source_line=234\n     Shape: bf16[256,127,128256]{2,1,0:T(8,128)(2,1)}\n     Unpadded size: 7.77G\n     Extra memory due to padding: 62.62M (1.0x expansion)\n     XLA label: fusion.105 = fusion(param.29, fusion.9), kind=kOutput, calls=fused_computation.99\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 7.83G\n     Operator: op_name=\"jit(lfn)/jit(main)/vmap(jit(log_softmax))/sub\" source_file=\"/tmp/ipykernel_1343889/1148687709.py\" source_line=4\n     Shape: bf16[256,127,128256]{2,1,0:T(8,128)(2,1)}\n     Unpadded size: 7.77G\n     Extra memory due to padding: 62.62M (1.0x expansion)\n     XLA label: fusion.3 = fusion(get-tuple-element.494, log.2, get-tuple-element.493), kind=kLoop, calls=fused_computation.3\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 1.86M\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/3/while/body/1/2/1/Linear8bitTranspose/jit(shmap_body)/psum[axes=(\\'mp\\',) axis_index_groups=None]\" source_file=\"/home/neverix/micrlhf-progress/micrlhf/quantizers.py\" source_line=164\n     Shape: f32[487424]{0}\n     Unpadded size: 1.86M\n     XLA label: all-reduce.6 = all-reduce(custom-call.43), channel_id=1, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=region_4.143\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Shape: s32[256,127,1,1]{1,0,3,2:T(8,128)}\n     Unpadded size: 127.0K\n     Extra memory due to padding: 1.0K (1.0x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  5. Size: 512B\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/3/while/body/3/0/0/RMSStandardize/reduce_sum[axes=(2,)]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/nn/standardization.py\" source_line=153\n     Shape: (f32[256,128]{1,0:T(8,128)S(3)}, bf16[8192,256,128]{0,2,1:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.130 = fusion(bitcast.134, get-tuple-element.700), kind=kLoop, calls=fused_computation.33.clone\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 512B\n     Shape: (bf16[256,128,2,64]{3,1,2,0:T(8,128)(2,1)}, bf16[256,128,2,64]{3,1,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.125 = fusion(copy.186, copy.187, copy.190), kind=kLoop, calls=fused_computation.69.clone\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.3 = slice-start(get-tuple-element.573), slice={[192:256], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 512B\n     Shape: ((bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}), bf16[64,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.3 = slice-start(get-tuple-element.573), slice={[192:256], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.2 = slice-start(get-tuple-element.573), slice={[128:192], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 512B\n     Shape: ((bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}), bf16[64,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.2 = slice-start(get-tuple-element.573), slice={[128:192], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.1 = slice-start(get-tuple-element.573), slice={[64:128], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 512B\n     Shape: ((bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}), bf16[64,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.1 = slice-start(get-tuple-element.573), slice={[64:128], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start = slice-start(get-tuple-element.573), slice={[0:64], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 512B\n     Shape: ((bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}), bf16[64,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start = slice-start(get-tuple-element.573), slice={[0:64], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 512B\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/3/while/body/1/0/0/RMSStandardize/reduce_sum[axes=(0,)]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/nn/standardization.py\" source_line=153\n     Shape: (f32[256,128]{1,0:T(8,128)S(3)}, bf16[8192,256,128]{0,2,1:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.137 = fusion(bitcast.138, get-tuple-element.581), kind=kLoop, calls=fused_computation.32.clone\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}, bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.122 = fusion(get-tuple-element.568, get-tuple-element.569, bitcast.143), kind=kLoop, calls=fused_computation.45.clone\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 512B\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/3/while/body/1/2/1/ApplyRoPE/cos\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/nn/embeddings.py\" source_line=305\n     Shape: (f32[128,64]{0,1:T(8,128)}, f32[128,64]{0,1:T(8,128)})\n     Unpadded size: 512B\n     XLA label: fusion.119 = fusion(fusion.118, get-tuple-element.774), kind=kLoop, calls=fused_computation.95.clone\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 512B\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/5/EmbeddingDecode/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=bfloat16]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/core/named_axes.py\" source_line=234\n     Shape: (bf16[256,127]{1,0:T(8,128)(2,1)S(3)}, bf16[256,127,128256]{2,1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.105 = fusion(param.29, fusion.9), kind=kOutput, calls=fused_computation.99\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 512B\n     Shape: (bf16[8192]{0:T(1024)(128)(2,1)S(3)}, bf16[8192]{0:T(1024)(128)(2,1)}, u32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: copy-start.3 = copy-start(param.28)\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 512B\n     Shape: (s32[]{:T(128)}, bf16[8192,256,128]{0,2,1:T(8,128)(2,1)}, bf16[80,8192]{1,0:T(8,128)(2,1)}, f16[80,256,1,2048]{3,1,2,0:T(8,128)(2,1)}, s8[80,256,32,2048]{3,2,1,0:T(8,128)(4,1)}, /*index=5*/f16[80,256,1,256]{3,1,2,0:T(8,128)(2,1)}, s8[80,256,32,256]{3,2,1,0:T(8,128)(4,1)}, f16[80,256,1,256]{3,1,2,0:T(8,128)(2,1)}, s8[80,256,32,256]{3,2,1,0:T(8,128)(4,1)}, f16[80,64,1,8192]{3,1,2,0:T(8,128)(2,1)}, /*index=10*/s8[80,64,32,8192]{3,2,1,0:T(8,128)(4,1)}, bf16[80,8192]{1,0:T(8,128)(2,1)}, f16[80,256,1,7168]{3,1,2,0:T(8,128)(2,1)}, s8[80,256,32,7168]{3,2,1,0:T(8,128)(4,1)}, f16[80,256,1,7168]{3,1,2,0:T(8,128)(2,1)}, /*index=15*/s8[80,256,32,7168]{3,2,1,0:T(8,128)(4,1)}, f16[80,224,1,8192]{3,1,2,0:T(8,128)(2,1)}, s8[80,224,32,8192]{3,2,1,0:T(8,128)(4,1)}, bf16[]{:T(256)}, s32[128]{0:T(128)}, /*index=20*/bf16[]{:T(256)}, pred[128,128]{0,1:T(8,128)(4,1)}, bf16[]{:T(256)}, bf16[]{:T(256)}, f32[128]{0:T(128)}, /*index=25*/f32[256,128]{1,0:T(8,128)S(3)}, s32[]{:T(128)})\n     Unpadded size: 512B\n     XLA label: tuple.45 = tuple(copy.205, bitcast.50, param.5, param.6, ...(+23))\n     Allocation type: HLO temp\n     ==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mpz\u001b[38;5;241m.\u001b[39mnx\u001b[38;5;241m.\u001b[39mnmap(\u001b[38;5;28;01mlambda\u001b[39;00m l, t: jnp\u001b[38;5;241m.\u001b[39mtake_along_axis(jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mlog_softmax(l[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), t[\u001b[38;5;241m1\u001b[39m:, \u001b[38;5;28;01mNone\u001b[39;00m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())(logits\u001b[38;5;241m.\u001b[39muntag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m), tokens\u001b[38;5;241m.\u001b[39muntag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_jitted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/compiler.py:237\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    233\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 36.51G of 30.75G hbm. Exceeded hbm capacity by 5.76G.\n\nTotal hbm usage >= 37.76G:\n    reserved          1.25G \n    program          15.66G \n    arguments        20.85G \n\nOutput size 512B; shares 0B with arguments.\n\nProgram hbm requirement 15.66G:\n    global           144.5K\n    scoped            1.86M\n    HLO temp         15.66G (99.2% utilization: Unpadded (15.53G) Padded (15.66G), 0.0% fragmentation (341.0K))\n\n  Largest program allocations in hbm:\n\n  1. Size: 7.83G\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/5/EmbeddingDecode/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=bfloat16]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/core/named_axes.py\" source_line=234\n     Shape: bf16[256,127,128256]{2,1,0:T(8,128)(2,1)}\n     Unpadded size: 7.77G\n     Extra memory due to padding: 62.62M (1.0x expansion)\n     XLA label: fusion.105 = fusion(param.29, fusion.9), kind=kOutput, calls=fused_computation.99\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 7.83G\n     Operator: op_name=\"jit(lfn)/jit(main)/vmap(jit(log_softmax))/sub\" source_file=\"/tmp/ipykernel_1343889/1148687709.py\" source_line=4\n     Shape: bf16[256,127,128256]{2,1,0:T(8,128)(2,1)}\n     Unpadded size: 7.77G\n     Extra memory due to padding: 62.62M (1.0x expansion)\n     XLA label: fusion.3 = fusion(get-tuple-element.494, log.2, get-tuple-element.493), kind=kLoop, calls=fused_computation.3\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 1.86M\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/3/while/body/1/2/1/Linear8bitTranspose/jit(shmap_body)/psum[axes=(\\'mp\\',) axis_index_groups=None]\" source_file=\"/home/neverix/micrlhf-progress/micrlhf/quantizers.py\" source_line=164\n     Shape: f32[487424]{0}\n     Unpadded size: 1.86M\n     XLA label: all-reduce.6 = all-reduce(custom-call.43), channel_id=1, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=region_4.143\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Shape: s32[256,127,1,1]{1,0,3,2:T(8,128)}\n     Unpadded size: 127.0K\n     Extra memory due to padding: 1.0K (1.0x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  5. Size: 512B\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/3/while/body/3/0/0/RMSStandardize/reduce_sum[axes=(2,)]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/nn/standardization.py\" source_line=153\n     Shape: (f32[256,128]{1,0:T(8,128)S(3)}, bf16[8192,256,128]{0,2,1:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.130 = fusion(bitcast.134, get-tuple-element.700), kind=kLoop, calls=fused_computation.33.clone\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 512B\n     Shape: (bf16[256,128,2,64]{3,1,2,0:T(8,128)(2,1)}, bf16[256,128,2,64]{3,1,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.125 = fusion(copy.186, copy.187, copy.190), kind=kLoop, calls=fused_computation.69.clone\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.3 = slice-start(get-tuple-element.573), slice={[192:256], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 512B\n     Shape: ((bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}), bf16[64,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.3 = slice-start(get-tuple-element.573), slice={[192:256], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.2 = slice-start(get-tuple-element.573), slice={[128:192], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 512B\n     Shape: ((bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}), bf16[64,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.2 = slice-start(get-tuple-element.573), slice={[128:192], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.1 = slice-start(get-tuple-element.573), slice={[64:128], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 512B\n     Shape: ((bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}), bf16[64,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.1 = slice-start(get-tuple-element.573), slice={[64:128], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start = slice-start(get-tuple-element.573), slice={[0:64], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 512B\n     Shape: ((bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}), bf16[64,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start = slice-start(get-tuple-element.573), slice={[0:64], [0:128], [0:2], [0:8], [0:64]}\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 512B\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/3/while/body/1/0/0/RMSStandardize/reduce_sum[axes=(0,)]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/nn/standardization.py\" source_line=153\n     Shape: (f32[256,128]{1,0:T(8,128)S(3)}, bf16[8192,256,128]{0,2,1:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.137 = fusion(bitcast.138, get-tuple-element.581), kind=kLoop, calls=fused_computation.32.clone\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 512B\n     Shape: (bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)}, bf16[256,128,2,8,64]{1,4,3,2,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.122 = fusion(get-tuple-element.568, get-tuple-element.569, bitcast.143), kind=kLoop, calls=fused_computation.45.clone\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 512B\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/3/while/body/1/2/1/ApplyRoPE/cos\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/nn/embeddings.py\" source_line=305\n     Shape: (f32[128,64]{0,1:T(8,128)}, f32[128,64]{0,1:T(8,128)})\n     Unpadded size: 512B\n     XLA label: fusion.119 = fusion(fusion.118, get-tuple-element.774), kind=kLoop, calls=fused_computation.95.clone\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 512B\n     Operator: op_name=\"jit(lfn)/jit(main)/jit(_flat_jit_call_layer)/LlamaTransformer/WithSideInputsFromInputTuple/5/EmbeddingDecode/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=bfloat16]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/core/named_axes.py\" source_line=234\n     Shape: (bf16[256,127]{1,0:T(8,128)(2,1)S(3)}, bf16[256,127,128256]{2,1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: fusion.105 = fusion(param.29, fusion.9), kind=kOutput, calls=fused_computation.99\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 512B\n     Shape: (bf16[8192]{0:T(1024)(128)(2,1)S(3)}, bf16[8192]{0:T(1024)(128)(2,1)}, u32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: copy-start.3 = copy-start(param.28)\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 512B\n     Shape: (s32[]{:T(128)}, bf16[8192,256,128]{0,2,1:T(8,128)(2,1)}, bf16[80,8192]{1,0:T(8,128)(2,1)}, f16[80,256,1,2048]{3,1,2,0:T(8,128)(2,1)}, s8[80,256,32,2048]{3,2,1,0:T(8,128)(4,1)}, /*index=5*/f16[80,256,1,256]{3,1,2,0:T(8,128)(2,1)}, s8[80,256,32,256]{3,2,1,0:T(8,128)(4,1)}, f16[80,256,1,256]{3,1,2,0:T(8,128)(2,1)}, s8[80,256,32,256]{3,2,1,0:T(8,128)(4,1)}, f16[80,64,1,8192]{3,1,2,0:T(8,128)(2,1)}, /*index=10*/s8[80,64,32,8192]{3,2,1,0:T(8,128)(4,1)}, bf16[80,8192]{1,0:T(8,128)(2,1)}, f16[80,256,1,7168]{3,1,2,0:T(8,128)(2,1)}, s8[80,256,32,7168]{3,2,1,0:T(8,128)(4,1)}, f16[80,256,1,7168]{3,1,2,0:T(8,128)(2,1)}, /*index=15*/s8[80,256,32,7168]{3,2,1,0:T(8,128)(4,1)}, f16[80,224,1,8192]{3,1,2,0:T(8,128)(2,1)}, s8[80,224,32,8192]{3,2,1,0:T(8,128)(4,1)}, bf16[]{:T(256)}, s32[128]{0:T(128)}, /*index=20*/bf16[]{:T(256)}, pred[128,128]{0,1:T(8,128)(4,1)}, bf16[]{:T(256)}, bf16[]{:T(256)}, f32[128]{0:T(128)}, /*index=25*/f32[256,128]{1,0:T(8,128)S(3)}, s32[]{:T(128)})\n     Unpadded size: 512B\n     XLA label: tuple.45 = tuple(copy.205, bitcast.50, param.5, param.6, ...(+23))\n     Allocation type: HLO temp\n     ==========================\n\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def lfn(llama_jitted, inputs):\n",
    "    logits = llama_jitted(inputs)\n",
    "    loss = -pz.nx.nmap(lambda l, t: jnp.take_along_axis(jax.nn.log_softmax(l[:-1], -1), t[1:, None], 1).mean())(logits.untag(\"seq\", \"vocabulary\"), tokens.untag(\"seq\"))\n",
    "    return loss\n",
    "print(lfn(llama_jitted, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a837ca07d76547dfbe9b2f728efa93de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m new_tokens \u001b[38;5;241m=\u001b[39m logits[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m: i}]\u001b[38;5;241m.\u001b[39muntag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39margmax()\n\u001b[1;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m pz\u001b[38;5;241m.\u001b[39mnx\u001b[38;5;241m.\u001b[39mnmap(\u001b[38;5;28;01mlambda\u001b[39;00m a, b: a\u001b[38;5;241m.\u001b[39mat[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset(b))(tokens\u001b[38;5;241m.\u001b[39muntag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m), new_tokens)\u001b[38;5;241m.\u001b[39mtag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m bar\u001b[38;5;241m.\u001b[39mset_postfix(txt\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39mreplace(inputs, tokens\u001b[38;5;241m=\u001b[39mtokens)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3811\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3811\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3815\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/array.py:344\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_addressable\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mis_single_device_sharding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharding) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 344\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43msl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharding, PmapSharding):\n\u001b[1;32m    346\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:346\u001b[0m, in \u001b[0;36m_chunk_iter\u001b[0;34m(x, size)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m   num_chunks, tail \u001b[38;5;241m=\u001b[39m ufuncs\u001b[38;5;241m.\u001b[39mdivmod(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], size)\n\u001b[0;32m--> 346\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mdynamic_slice_in_dim(x, i \u001b[38;5;241m*\u001b[39m size, size)\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tail:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/array.py:284\u001b[0m, in \u001b[0;36mArrayImpl.__index__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__index__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    283\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_integer_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/array.py:594\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "import dataclasses\n",
    "# 1/0\n",
    "for i in (bar := trange(16, 256)):\n",
    "    logits = llama_jitted(inputs)\n",
    "    new_tokens = logits[{\"seq\": i}].untag(\"vocabulary\").argmax()\n",
    "    tokens = pz.nx.nmap(lambda a, b: a.at[i+1].set(b))(tokens.untag(\"seq\"), new_tokens).tag(\"seq\")\n",
    "    bar.set_postfix(txt=tokenizer.decode(tokens[{\"batch\": 0}].unwrap(\"seq\")))\n",
    "    inputs = dataclasses.replace(inputs, tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
