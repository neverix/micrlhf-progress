{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "import jax.numpy as jnp\n",
    "import jax_smi\n",
    "jax_smi.initialise_tracking()\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained((\"models/llama-3-70b-1.gguf\", \"models/llama-3-70b-2.gguf\"),\n",
    "                                         device_map=\"auto:mp=4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama3-ChatQA-1.5-70B\")\n",
    "prompt = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from penzai.toolshed import jit_wrapper\n",
    "from micrlhf.scan import sequential_to_scan\n",
    "from micrlhf.flash import flashify\n",
    "tokens = pz.nx.wrap([(tokenizer.encode(prompt) * 8)[:128]], \"batch\", \"seq\")\n",
    "inputs = llama.inputs.from_basic_segments(tokens)\n",
    "# fast_llama = sequential_to_scan(flashify(llama))\n",
    "fast_llama = sequential_to_scan(llama)\n",
    "llama_jitted = jit_wrapper.Jitted(fast_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 58.98G of 30.75G hbm. Exceeded hbm capacity by 28.23G.\n\nTotal hbm usage >= 60.23G:\n    reserved          1.25G \n    program          58.98G \n    arguments            0B \n\nOutput size 0B; shares 0B with arguments.\n\nProgram hbm requirement 58.98G:\n    global            16.5K\n    HLO temp         58.98G (100.0% utilization: Unpadded (58.98G) Padded (58.98G), 0.0% fragmentation (4.42M))\n\n  Largest program allocations in hbm:\n\n  1. Size: 17.50G\n     Shape: s8[80,8192,896,32]{1,3,2,0:T(8,128)(4,1)}\n     Unpadded size: 17.50G\n     XLA label: copy.134 = copy(param.20)\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 17.50G\n     Shape: s8[80,28672,256,32]{1,3,2,0:T(8,128)(4,1)}\n     Unpadded size: 17.50G\n     XLA label: copy.133 = copy(param.18)\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 17.50G\n     Shape: s8[80,28672,256,32]{1,3,2,0:T(8,128)(4,1)}\n     Unpadded size: 17.50G\n     XLA label: copy.132 = copy(param.16)\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 5.00G\n     Shape: s8[80,8,8,128,256,32]{3,2,1,5,4,0:T(8,128)(4,1)}\n     Unpadded size: 5.00G\n     XLA label: copy.126 = copy(param.7)\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 640.00M\n     Shape: s8[80,8,128,256,32]{2,1,4,3,0:T(8,128)(4,1)}\n     Unpadded size: 640.00M\n     XLA label: copy.130 = copy(param.11)\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 640.00M\n     Shape: s8[80,8,128,256,32]{2,1,4,3,0:T(8,128)(4,1)}\n     Unpadded size: 640.00M\n     XLA label: copy.128 = copy(param.9)\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 224.00M\n     Operator: op_name=\"jit(_flat_jit_call_layer)/jit(main)/LlamaTransformer/WithSideInputsFromInputTuple/2/while/body/3/2/0/0/Linear8bitTranspose/transpose[permutation=(1, 0, 2)]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/core/named_axes.py\" source_line=1365\n     Shape: s8[256,32,28672]{2,1,0:T(8,128)(4,1)}\n     Unpadded size: 224.00M\n     XLA label: fusion.15.remat2.1.remat2 = fusion(get-tuple-element.454, get-tuple-element.413), kind=kLoop, calls=fused_computation.15.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 4.00M\n     Shape: bf16[256,8192]{1,0:T(8,128)(2,1)}\n     Unpadded size: 4.00M\n     XLA label: copy-start = copy-start(custom-call.47)\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 4.00M\n     Operator: op_name=\"jit(_flat_jit_call_layer)/jit(main)/LlamaTransformer/WithSideInputsFromInputTuple/2/while/body/3/2/0/0/Linear8bitTranspose/transpose[permutation=(1, 0)]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/core/named_axes.py\" source_line=1365\n     Shape: bf16[256,8192]{1,0:T(8,128)(2,1)}\n     Unpadded size: 4.00M\n     XLA label: fusion.101 = fusion(fusion.77, fusion.100, get-tuple-element.414, custom-call.47), kind=kLoop, calls=fused_computation.29.clone\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 1.0K\n     Operator: op_name=\"jit(_flat_jit_call_layer)/jit(main)/LlamaTransformer/WithSideInputsFromInputTuple/2/while/body/1/2/1/ApplyRoPE/convert_element_type[new_dtype=float32 weak_type=False]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/nn/embeddings.py\" source_line=303\n     Shape: f32[256]{0:T(256)}\n     Unpadded size: 1.0K\n     XLA label: convert.75 = convert(copy-done.5)\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 512B\n     Shape: ((bf16[256,8192]{1,0:T(8,128)(2,1)}), bf16[64,8192]{1,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.7 = slice-start(fusion.101), slice={[128:192], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 512B\n     Shape: s32[16]{0:T(128)}\n     Unpadded size: 64B\n     Extra memory due to padding: 448B (8.0x expansion)\n     XLA label: deduplicated constant\n     Allocation type: global\n     ==========================\n\n  13. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)}, bf16[256,8192]{1,0:T(8,128)(2,1)S(3)}, u32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: copy-start = copy-start(custom-call.47)\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.3 = slice-start(fusion.101), slice={[128:192], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 512B\n     Shape: ((bf16[256,8192]{1,0:T(8,128)(2,1)}), bf16[64,8192]{1,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.3 = slice-start(fusion.101), slice={[128:192], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.2 = slice-start(fusion.101), slice={[64:128], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 512B\n     Shape: ((bf16[256,8192]{1,0:T(8,128)(2,1)}), bf16[64,8192]{1,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.2 = slice-start(fusion.101), slice={[64:128], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.1 = slice-start(fusion.101), slice={[0:64], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 512B\n     Shape: ((bf16[256,8192]{1,0:T(8,128)(2,1)}), bf16[64,8192]{1,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.1 = slice-start(fusion.101), slice={[0:64], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start = slice-start(fusion.101), slice={[192:256], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllama_jitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/toolshed/jit_wrapper.py:62\u001b[0m, in \u001b[0;36mJitted.__call__\u001b[0;34m(self, argument)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, argument: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_jit_call_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     64\u001b[0m     impls \u001b[38;5;241m=\u001b[39m pz\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody)\u001b[38;5;241m.\u001b[39mat_instances_of(pz\u001b[38;5;241m.\u001b[39mde\u001b[38;5;241m.\u001b[39mEffectRuntimeImpl)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/compiler.py:237\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    233\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 58.98G of 30.75G hbm. Exceeded hbm capacity by 28.23G.\n\nTotal hbm usage >= 60.23G:\n    reserved          1.25G \n    program          58.98G \n    arguments            0B \n\nOutput size 0B; shares 0B with arguments.\n\nProgram hbm requirement 58.98G:\n    global            16.5K\n    HLO temp         58.98G (100.0% utilization: Unpadded (58.98G) Padded (58.98G), 0.0% fragmentation (4.42M))\n\n  Largest program allocations in hbm:\n\n  1. Size: 17.50G\n     Shape: s8[80,8192,896,32]{1,3,2,0:T(8,128)(4,1)}\n     Unpadded size: 17.50G\n     XLA label: copy.134 = copy(param.20)\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 17.50G\n     Shape: s8[80,28672,256,32]{1,3,2,0:T(8,128)(4,1)}\n     Unpadded size: 17.50G\n     XLA label: copy.133 = copy(param.18)\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 17.50G\n     Shape: s8[80,28672,256,32]{1,3,2,0:T(8,128)(4,1)}\n     Unpadded size: 17.50G\n     XLA label: copy.132 = copy(param.16)\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 5.00G\n     Shape: s8[80,8,8,128,256,32]{3,2,1,5,4,0:T(8,128)(4,1)}\n     Unpadded size: 5.00G\n     XLA label: copy.126 = copy(param.7)\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 640.00M\n     Shape: s8[80,8,128,256,32]{2,1,4,3,0:T(8,128)(4,1)}\n     Unpadded size: 640.00M\n     XLA label: copy.130 = copy(param.11)\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 640.00M\n     Shape: s8[80,8,128,256,32]{2,1,4,3,0:T(8,128)(4,1)}\n     Unpadded size: 640.00M\n     XLA label: copy.128 = copy(param.9)\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 224.00M\n     Operator: op_name=\"jit(_flat_jit_call_layer)/jit(main)/LlamaTransformer/WithSideInputsFromInputTuple/2/while/body/3/2/0/0/Linear8bitTranspose/transpose[permutation=(1, 0, 2)]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/core/named_axes.py\" source_line=1365\n     Shape: s8[256,32,28672]{2,1,0:T(8,128)(4,1)}\n     Unpadded size: 224.00M\n     XLA label: fusion.15.remat2.1.remat2 = fusion(get-tuple-element.454, get-tuple-element.413), kind=kLoop, calls=fused_computation.15.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 4.00M\n     Shape: bf16[256,8192]{1,0:T(8,128)(2,1)}\n     Unpadded size: 4.00M\n     XLA label: copy-start = copy-start(custom-call.47)\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 4.00M\n     Operator: op_name=\"jit(_flat_jit_call_layer)/jit(main)/LlamaTransformer/WithSideInputsFromInputTuple/2/while/body/3/2/0/0/Linear8bitTranspose/transpose[permutation=(1, 0)]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/core/named_axes.py\" source_line=1365\n     Shape: bf16[256,8192]{1,0:T(8,128)(2,1)}\n     Unpadded size: 4.00M\n     XLA label: fusion.101 = fusion(fusion.77, fusion.100, get-tuple-element.414, custom-call.47), kind=kLoop, calls=fused_computation.29.clone\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 1.0K\n     Operator: op_name=\"jit(_flat_jit_call_layer)/jit(main)/LlamaTransformer/WithSideInputsFromInputTuple/2/while/body/1/2/1/ApplyRoPE/convert_element_type[new_dtype=float32 weak_type=False]\" source_file=\"/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/penzai/nn/embeddings.py\" source_line=303\n     Shape: f32[256]{0:T(256)}\n     Unpadded size: 1.0K\n     XLA label: convert.75 = convert(copy-done.5)\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 512B\n     Shape: ((bf16[256,8192]{1,0:T(8,128)(2,1)}), bf16[64,8192]{1,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.7 = slice-start(fusion.101), slice={[128:192], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 512B\n     Shape: s32[16]{0:T(128)}\n     Unpadded size: 64B\n     Extra memory due to padding: 448B (8.0x expansion)\n     XLA label: deduplicated constant\n     Allocation type: global\n     ==========================\n\n  13. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)}, bf16[256,8192]{1,0:T(8,128)(2,1)S(3)}, u32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: copy-start = copy-start(custom-call.47)\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.3 = slice-start(fusion.101), slice={[128:192], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 512B\n     Shape: ((bf16[256,8192]{1,0:T(8,128)(2,1)}), bf16[64,8192]{1,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.3 = slice-start(fusion.101), slice={[128:192], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.2 = slice-start(fusion.101), slice={[64:128], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 512B\n     Shape: ((bf16[256,8192]{1,0:T(8,128)(2,1)}), bf16[64,8192]{1,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.2 = slice-start(fusion.101), slice={[64:128], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start.1 = slice-start(fusion.101), slice={[0:64], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 512B\n     Shape: ((bf16[256,8192]{1,0:T(8,128)(2,1)}), bf16[64,8192]{1,0:T(8,128)(2,1)S(3)}, s32[]{:S(2)})\n     Unpadded size: 512B\n     XLA label: slice-start.1 = slice-start(fusion.101), slice={[0:64], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 512B\n     Shape: (bf16[256,8192]{1,0:T(8,128)(2,1)})\n     Unpadded size: 512B\n     XLA label: slice-start = slice-start(fusion.101), slice={[192:256], [0:8192]}\n     Allocation type: HLO temp\n     ==========================\n\n"
     ]
    }
   ],
   "source": [
    "llama_jitted(inputs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.sampling import sample\n",
    "sample(llama, tokenizer, prompt, batch_size=1, max_seq_len=128, do_sample=True,\n",
    "       fold=True,\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
